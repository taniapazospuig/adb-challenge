{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9fab94a",
   "metadata": {},
   "source": [
    "# Dataset Generation Pipeline\n",
    "\n",
    "This notebook processes and cleans multiple data sources to create a unified dataset for analysis. The pipeline handles:\n",
    "\n",
    "1. **Water consumption data** (`consum_telelectura_v2.parquet`) - Daily water consumption readings\n",
    "2. **Leak incident data** (`consum_avisos_missatges_v2.parquet`) - Water leak reports and messages\n",
    "3. **Weather data** (`Dades_meteorològiques_diàries_de_la_XEMA`) - Daily meteorological measurements\n",
    "4. **Socioeconomic index** (`ist_per_seccio_censal.csv`) - Territorial socioeconomic index by census section\n",
    "\n",
    "All data is filtered to Barcelona (census sections starting with 8019/08019) and processed in chunks to handle large file sizes efficiently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07932c0",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Import required libraries and establish database connection. We use DuckDB for efficient data loading and pandas for data manipulation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c95ddb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "424648fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "con = duckdb.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d907f6b",
   "metadata": {},
   "source": [
    "## 2. Data Loading\n",
    "\n",
    "Load and inspect all raw data sources. We use DuckDB to efficiently read large parquet files without loading everything into memory at once. This allows us to check data sizes and sample records before processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee814e9",
   "metadata": {},
   "source": [
    "### 2.1 Water Consumption Data\n",
    "\n",
    "The consumption dataset contains daily water meter readings. We check the total number of records and inspect a sample to understand the data structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0902aa93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking consum data...\n",
      "Total rows in consum: 17,112,709\n",
      "\n",
      "Sample data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POLIZA_SUMINISTRO</th>\n",
       "      <th>FECHA</th>\n",
       "      <th>CONSUMO_REAL</th>\n",
       "      <th>SECCIO_CENSAL</th>\n",
       "      <th>US_AIGUA_GEST</th>\n",
       "      <th>NUM_MUN_SGAB</th>\n",
       "      <th>NUM_DTE_MUNI</th>\n",
       "      <th>NUM_COMPLET</th>\n",
       "      <th>DATA_INST_COMP</th>\n",
       "      <th>MARCA_COMP</th>\n",
       "      <th>CODI_MODEL</th>\n",
       "      <th>DIAM_COMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VECWAVDUULZDSBOP</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>1758</td>\n",
       "      <td>801903025</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>D15TD031058M</td>\n",
       "      <td>2016-04-25</td>\n",
       "      <td>ITR</td>\n",
       "      <td>23</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VECWAVDUULZDSBOP</td>\n",
       "      <td>2021-01-02</td>\n",
       "      <td>1854</td>\n",
       "      <td>801903025</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>D15TD031058M</td>\n",
       "      <td>2016-04-25</td>\n",
       "      <td>ITR</td>\n",
       "      <td>23</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VECWAVDUULZDSBOP</td>\n",
       "      <td>2021-01-03</td>\n",
       "      <td>1885</td>\n",
       "      <td>801903025</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>D15TD031058M</td>\n",
       "      <td>2016-04-25</td>\n",
       "      <td>ITR</td>\n",
       "      <td>23</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VECWAVDUULZDSBOP</td>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>5676</td>\n",
       "      <td>801903025</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>D15TD031058M</td>\n",
       "      <td>2016-04-25</td>\n",
       "      <td>ITR</td>\n",
       "      <td>23</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VECWAVDUULZDSBOP</td>\n",
       "      <td>2021-01-05</td>\n",
       "      <td>4456</td>\n",
       "      <td>801903025</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>D15TD031058M</td>\n",
       "      <td>2016-04-25</td>\n",
       "      <td>ITR</td>\n",
       "      <td>23</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  POLIZA_SUMINISTRO       FECHA  CONSUMO_REAL  SECCIO_CENSAL US_AIGUA_GEST  \\\n",
       "0  VECWAVDUULZDSBOP  2021-01-01          1758      801903025             C   \n",
       "1  VECWAVDUULZDSBOP  2021-01-02          1854      801903025             C   \n",
       "2  VECWAVDUULZDSBOP  2021-01-03          1885      801903025             C   \n",
       "3  VECWAVDUULZDSBOP  2021-01-04          5676      801903025             C   \n",
       "4  VECWAVDUULZDSBOP  2021-01-05          4456      801903025             C   \n",
       "\n",
       "   NUM_MUN_SGAB  NUM_DTE_MUNI   NUM_COMPLET DATA_INST_COMP MARCA_COMP  \\\n",
       "0             0             3  D15TD031058M     2016-04-25        ITR   \n",
       "1             0             3  D15TD031058M     2016-04-25        ITR   \n",
       "2             0             3  D15TD031058M     2016-04-25        ITR   \n",
       "3             0             3  D15TD031058M     2016-04-25        ITR   \n",
       "4             0             3  D15TD031058M     2016-04-25        ITR   \n",
       "\n",
       "   CODI_MODEL  DIAM_COMP  \n",
       "0          23         30  \n",
       "1          23         30  \n",
       "2          23         30  \n",
       "3          23         30  \n",
       "4          23         30  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check consumption data size and sample\n",
    "# We'll load data only when needed for pandas operations\n",
    "print(\"Checking consum data...\")\n",
    "row_count = con.execute(\"SELECT COUNT(*) as count FROM 'data/consum_telelectura_v2.parquet'\").df()['count'].iloc[0]\n",
    "print(f\"Total rows in consum: {row_count:,}\")\n",
    "\n",
    "# Load a sample for inspection\n",
    "df_consum_sample = con.execute(\"SELECT * FROM 'data/consum_telelectura_v2.parquet' LIMIT 5\").df()\n",
    "print(\"\\nSample data:\")\n",
    "df_consum_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de912b40",
   "metadata": {},
   "source": [
    "### 2.2 Leak Incidents Data\n",
    "\n",
    "The leak incidents dataset contains water leak reports and messages. This is a very large dataset (over 76 million rows), so we'll process it in chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adedddfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking fuites data...\n",
      "Total rows in fuites: 76,372,248\n",
      "\n",
      "Sample data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POLISSA_SUBM</th>\n",
       "      <th>DATA_INI_FACT</th>\n",
       "      <th>DATA_FIN_FACT</th>\n",
       "      <th>CREATED_MENSAJE</th>\n",
       "      <th>CODIGO_MENSAJE</th>\n",
       "      <th>TIPO_MENSAJE</th>\n",
       "      <th>US_AIGUA_SUBM</th>\n",
       "      <th>SECCIO_CENSAL</th>\n",
       "      <th>NUMEROSERIECONTADOR</th>\n",
       "      <th>CONSUMO_REAL</th>\n",
       "      <th>FECHA_HORA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RGYFWIZ4ZRRZKX2K</td>\n",
       "      <td>2023-09-13 00:00:00</td>\n",
       "      <td>2023-11-14 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>DOMÈSTIC</td>\n",
       "      <td>0801907090</td>\n",
       "      <td>IBAJ44VHSIRRTASA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HHB4U5HUQKW7IOGD</td>\n",
       "      <td>2023-08-13 00:00:00</td>\n",
       "      <td>2023-10-16 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>DOMÈSTIC</td>\n",
       "      <td>0801909040</td>\n",
       "      <td>L2CLPPJRIPAEESV7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EU6AT3IKPUKCZTBU</td>\n",
       "      <td>2024-01-24 00:00:00</td>\n",
       "      <td>2024-03-26 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>DOMÈSTIC</td>\n",
       "      <td>0801902046</td>\n",
       "      <td>45TBDJQN4LA37ZIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EU6AT3IKPUKCZTBU</td>\n",
       "      <td>2023-11-27 00:00:00</td>\n",
       "      <td>2024-01-24 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>DOMÈSTIC</td>\n",
       "      <td>0801902046</td>\n",
       "      <td>45TBDJQN4LA37ZIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EWNDTPECBVEGW6AU</td>\n",
       "      <td>2023-09-29 00:00:00</td>\n",
       "      <td>2023-11-27 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>DOMÈSTIC</td>\n",
       "      <td>0801902046</td>\n",
       "      <td>VTRAI3L24SWKVC5H</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       POLISSA_SUBM        DATA_INI_FACT        DATA_FIN_FACT CREATED_MENSAJE  \\\n",
       "0  RGYFWIZ4ZRRZKX2K  2023-09-13 00:00:00  2023-11-14 00:00:00             NaT   \n",
       "1  HHB4U5HUQKW7IOGD  2023-08-13 00:00:00  2023-10-16 00:00:00             NaT   \n",
       "2  EU6AT3IKPUKCZTBU  2024-01-24 00:00:00  2024-03-26 00:00:00             NaT   \n",
       "3  EU6AT3IKPUKCZTBU  2023-11-27 00:00:00  2024-01-24 00:00:00             NaT   \n",
       "4  EWNDTPECBVEGW6AU  2023-09-29 00:00:00  2023-11-27 00:00:00             NaT   \n",
       "\n",
       "  CODIGO_MENSAJE TIPO_MENSAJE US_AIGUA_SUBM SECCIO_CENSAL NUMEROSERIECONTADOR  \\\n",
       "0           None         None      DOMÈSTIC    0801907090    IBAJ44VHSIRRTASA   \n",
       "1           None         None      DOMÈSTIC    0801909040    L2CLPPJRIPAEESV7   \n",
       "2           None         None      DOMÈSTIC    0801902046    45TBDJQN4LA37ZIN   \n",
       "3           None         None      DOMÈSTIC    0801902046    45TBDJQN4LA37ZIN   \n",
       "4           None         None      DOMÈSTIC    0801902046    VTRAI3L24SWKVC5H   \n",
       "\n",
       "   CONSUMO_REAL FECHA_HORA  \n",
       "0           NaN 2024-01-01  \n",
       "1           NaN 2024-01-01  \n",
       "2           NaN 2024-01-01  \n",
       "3           NaN 2024-01-01  \n",
       "4           NaN 2024-01-01  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check leak incidents data size and sample\n",
    "print(\"Checking fuites data...\")\n",
    "row_count = con.execute(\"SELECT COUNT(*) as count FROM 'data/consum_avisos_missatges_v2.parquet'\").df()['count'].iloc[0]\n",
    "print(f\"Total rows in fuites: {row_count:,}\")\n",
    "\n",
    "# Load a sample for inspection\n",
    "df_fuites_sample = con.execute(\"SELECT * FROM 'data/consum_avisos_missatges_v2.parquet' LIMIT 5\").df()\n",
    "print(\"\\nSample data:\")\n",
    "df_fuites_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed52e8fc",
   "metadata": {},
   "source": [
    "### 2.3 Socioeconomic Index Data\n",
    "\n",
    "The socioeconomic index provides territorial indicators at the census section level, which will be used for joining with consumption and leak data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "212caf97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ist_per_seccio_censal.csv with DuckDB...\n",
      "Loaded 5,092 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>any</th>\n",
       "      <th>secció censal</th>\n",
       "      <th>concepte</th>\n",
       "      <th>estat</th>\n",
       "      <th>valor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022</td>\n",
       "      <td>25001901001</td>\n",
       "      <td>Índex socioeconòmic territorial</td>\n",
       "      <td>None</td>\n",
       "      <td>97.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022</td>\n",
       "      <td>08001801001</td>\n",
       "      <td>Índex socioeconòmic territorial</td>\n",
       "      <td>None</td>\n",
       "      <td>105.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022</td>\n",
       "      <td>08001801002</td>\n",
       "      <td>Índex socioeconòmic territorial</td>\n",
       "      <td>None</td>\n",
       "      <td>102.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022</td>\n",
       "      <td>08001801003</td>\n",
       "      <td>Índex socioeconòmic territorial</td>\n",
       "      <td>None</td>\n",
       "      <td>118.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022</td>\n",
       "      <td>08001801004</td>\n",
       "      <td>Índex socioeconòmic territorial</td>\n",
       "      <td>None</td>\n",
       "      <td>118.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    any secció censal                         concepte estat  valor\n",
       "0  2022   25001901001  Índex socioeconòmic territorial  None   97.8\n",
       "1  2022   08001801001  Índex socioeconòmic territorial  None  105.1\n",
       "2  2022   08001801002  Índex socioeconòmic territorial  None  102.9\n",
       "3  2022   08001801003  Índex socioeconòmic territorial  None  118.5\n",
       "4  2022   08001801004  Índex socioeconòmic territorial  None  118.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset Índex Socioeconòmic Territorial per secció censal\n",
    "print(\"Loading ist_per_seccio_censal.csv with DuckDB...\")\n",
    "df_socio = con.execute(\"\"\"\n",
    "    SELECT * FROM read_csv_auto('data/ist_per_seccio_censal.csv', \n",
    "                                 delim=';', \n",
    "                                 decimal_separator=',',\n",
    "                                 header=true)\n",
    "\"\"\").df()\n",
    "print(f\"Loaded {len(df_socio):,} rows\")\n",
    "df_socio.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089ee9dd",
   "metadata": {},
   "source": [
    "### 2.4 Weather Data\n",
    "\n",
    "Weather data from XEMA (Catalan Meteorological Service) contains daily measurements from multiple weather stations. This data will be used to analyze correlations between weather patterns and water consumption/leaks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7256cd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dades meteorloògiques diàries with DuckDB...\n",
      "Loaded 129,301 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CODI_ESTACIO</th>\n",
       "      <th>NOM_ESTACIO</th>\n",
       "      <th>DATA_LECTURA</th>\n",
       "      <th>CODI_VARIABLE</th>\n",
       "      <th>NOM_VARIABLE</th>\n",
       "      <th>VALOR</th>\n",
       "      <th>UNITAT</th>\n",
       "      <th>HORA _TU</th>\n",
       "      <th>Estat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D51000012101</td>\n",
       "      <td>D5</td>\n",
       "      <td>Barcelona - Observatori Fabra</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>1.000</td>\n",
       "      <td>Temperatura mitjana diària</td>\n",
       "      <td>5,3</td>\n",
       "      <td>°C</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Representatiu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D51001012101</td>\n",
       "      <td>D5</td>\n",
       "      <td>Barcelona - Observatori Fabra</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>1.001</td>\n",
       "      <td>Temperatura màxima diària + hora</td>\n",
       "      <td>6,5</td>\n",
       "      <td>°C</td>\n",
       "      <td>13:06:00</td>\n",
       "      <td>Representatiu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D51002012101</td>\n",
       "      <td>D5</td>\n",
       "      <td>Barcelona - Observatori Fabra</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>1.002</td>\n",
       "      <td>Temperatura mínima diària + hora</td>\n",
       "      <td>3,1</td>\n",
       "      <td>°C</td>\n",
       "      <td>23:56:00</td>\n",
       "      <td>Representatiu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D51003012101</td>\n",
       "      <td>D5</td>\n",
       "      <td>Barcelona - Observatori Fabra</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>1.003</td>\n",
       "      <td>Temperatura mitjana diària clàssica</td>\n",
       "      <td>4,8</td>\n",
       "      <td>°C</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Representatiu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D51004012101</td>\n",
       "      <td>D5</td>\n",
       "      <td>Barcelona - Observatori Fabra</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>1.004</td>\n",
       "      <td>Amplitud tèrmica diària</td>\n",
       "      <td>3,4</td>\n",
       "      <td>°C</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Representatiu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID CODI_ESTACIO                    NOM_ESTACIO DATA_LECTURA  \\\n",
       "0  D51000012101           D5  Barcelona - Observatori Fabra   2021-01-01   \n",
       "1  D51001012101           D5  Barcelona - Observatori Fabra   2021-01-01   \n",
       "2  D51002012101           D5  Barcelona - Observatori Fabra   2021-01-01   \n",
       "3  D51003012101           D5  Barcelona - Observatori Fabra   2021-01-01   \n",
       "4  D51004012101           D5  Barcelona - Observatori Fabra   2021-01-01   \n",
       "\n",
       "   CODI_VARIABLE                         NOM_VARIABLE VALOR UNITAT  HORA _TU  \\\n",
       "0          1.000           Temperatura mitjana diària   5,3     °C      <NA>   \n",
       "1          1.001     Temperatura màxima diària + hora   6,5     °C  13:06:00   \n",
       "2          1.002     Temperatura mínima diària + hora   3,1     °C  23:56:00   \n",
       "3          1.003  Temperatura mitjana diària clàssica   4,8     °C      <NA>   \n",
       "4          1.004              Amplitud tèrmica diària   3,4     °C      <NA>   \n",
       "\n",
       "           Estat  \n",
       "0  Representatiu  \n",
       "1  Representatiu  \n",
       "2  Representatiu  \n",
       "3  Representatiu  \n",
       "4  Representatiu  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load weather data using DuckDB\n",
    "print(\"Loading dades meteorloògiques diàries with DuckDB...\")\n",
    "df_weather = con.execute(\"SELECT * FROM read_csv_auto('data/Dades_meteorològiques_diàries_de_la_XEMA_20251119.csv', header=true)\").df()\n",
    "print(f\"Loaded {len(df_weather):,} rows\")\n",
    "\n",
    "df_weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539d5db1",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning and Filtering\n",
    "\n",
    "Process the raw data files to extract Barcelona-specific data and clean weather data. Since the datasets are very large, we process them in chunks (split files) to avoid memory issues. For consumption and leak data, we:\n",
    "\n",
    "1. Filter to Barcelona census sections (8019/08019)\n",
    "2. Transform census section codes to the standard 11-digit format\n",
    "3. Keep only relevant columns\n",
    "4. Save cleaned data:\n",
    "   - Consumption: Split parquet files in `clean/split_consum_bcn/` (kept as splits due to size)\n",
    "   - Leaks: Single parquet file `clean/fuites_clean_bcn.parquet` (one leak per row)\n",
    "\n",
    "For weather data, we clean missing values and ensure consistent formatting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ad77a3",
   "metadata": {},
   "source": [
    "### 3.1 Clean Consumption Data\n",
    "\n",
    "Process consumption data split files:\n",
    "- Filter to Barcelona (SECCIO_CENSAL starting with 8019)\n",
    "- Transform census section codes: add '3' after 8019, then prepend '0' to get 11-digit format\n",
    "- Keep relevant columns: POLIZA_SUMINISTRO, FECHA, CONSUMO_REAL, SECCIO_CENSAL, US_AIGUA_GEST, DATA_INST_COMP\n",
    "- Convert date columns to datetime format\n",
    "- Save cleaned Barcelona data as split parquet files: `clean/split_consum_bcn/consum_clean_bcn_part_*.parquet`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8111fc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pre-split consumption files found. Splitting the raw file now (one-time operation)...\n",
      "Reading parquet file: data/consum_telelectura_v2.parquet\n",
      "Chunk size: 1,000,000 rows\n",
      "Output directory: data/split_consum\n",
      "------------------------------------------------------------\n",
      "Total rows in file: 17,112,709\n",
      "Number of row groups: 17\n",
      "Will create approximately 18 output files\n",
      "------------------------------------------------------------\n",
      "Reading row group 1/17... loaded 1,048,576 rows\n",
      "  ✓ Chunk 1: Saved 1,000,000 rows to consum_telelectura_v2_part_001.parquet\n",
      "Reading row group 2/17... loaded 1,048,576 rows\n",
      "  ✓ Chunk 2: Saved 1,000,000 rows to consum_telelectura_v2_part_002.parquet\n",
      "Reading row group 3/17... loaded 1,048,576 rows\n",
      "  ✓ Chunk 3: Saved 1,000,000 rows to consum_telelectura_v2_part_003.parquet\n",
      "Reading row group 4/17... loaded 1,048,576 rows\n",
      "  ✓ Chunk 4: Saved 1,000,000 rows to consum_telelectura_v2_part_004.parquet\n",
      "Reading row group 5/17... loaded 1,048,576 rows\n",
      "  ✓ Chunk 5: Saved 1,000,000 rows to consum_telelectura_v2_part_005.parquet\n",
      "Reading row group 6/17... loaded 1,048,576 rows\n",
      "  ✓ Chunk 6: Saved 1,000,000 rows to consum_telelectura_v2_part_006.parquet\n",
      "Reading row group 7/17... loaded 1,048,576 rows\n",
      "  ✓ Chunk 7: Saved 1,000,000 rows to consum_telelectura_v2_part_007.parquet\n",
      "Reading row group 8/17... loaded 1,048,576 rows\n",
      "  ✓ Chunk 8: Saved 1,000,000 rows to consum_telelectura_v2_part_008.parquet\n",
      "Reading row group 9/17... loaded 1,048,576 rows\n",
      "  ✓ Chunk 9: Saved 1,000,000 rows to consum_telelectura_v2_part_009.parquet\n",
      "Reading row group 10/17... loaded 1,048,576 rows\n",
      "  ✓ Chunk 10: Saved 1,000,000 rows to consum_telelectura_v2_part_010.parquet\n",
      "Reading row group 11/17... loaded 1,048,576 rows\n",
      "  ✓ Chunk 11: Saved 1,000,000 rows to consum_telelectura_v2_part_011.parquet\n",
      "Reading row group 12/17... loaded 1,048,576 rows\n",
      "  ✓ Chunk 12: Saved 1,000,000 rows to consum_telelectura_v2_part_012.parquet\n",
      "Reading row group 13/17... loaded 1,048,576 rows\n",
      "  ✓ Chunk 13: Saved 1,000,000 rows to consum_telelectura_v2_part_013.parquet\n",
      "Reading row group 14/17... loaded 1,048,576 rows\n",
      "  ✓ Chunk 14: Saved 1,000,000 rows to consum_telelectura_v2_part_014.parquet\n",
      "Reading row group 15/17... loaded 1,048,576 rows\n",
      "  ✓ Chunk 15: Saved 1,000,000 rows to consum_telelectura_v2_part_015.parquet\n",
      "Reading row group 16/17... loaded 1,048,576 rows\n",
      "  ✓ Chunk 16: Saved 1,000,000 rows to consum_telelectura_v2_part_016.parquet\n",
      "Reading row group 17/17... loaded 335,493 rows\n",
      "  ✓ Chunk 17: Saved 1,000,000 rows to consum_telelectura_v2_part_017.parquet\n",
      "  ✓ Chunk 18: Saved 112,709 rows to consum_telelectura_v2_part_018.parquet\n",
      "------------------------------------------------------------\n",
      "Split complete! Created 18 files in data/split_consum\n",
      "Found 18 split files to process\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Processing file 1/18: consum_telelectura_v2_part_001.parquet\n",
      "Processed 1,000,000 rows → 24,726 Barcelona rows\n",
      "Saved to consum_clean_bcn_part_001.parquet\n",
      "\n",
      "Processing file 2/18: consum_telelectura_v2_part_002.parquet\n",
      "Processed 1,000,000 rows → 142,434 Barcelona rows\n",
      "Saved to consum_clean_bcn_part_002.parquet\n",
      "\n",
      "Processing file 3/18: consum_telelectura_v2_part_003.parquet\n",
      "Processed 1,000,000 rows → 127,915 Barcelona rows\n",
      "Saved to consum_clean_bcn_part_003.parquet\n",
      "\n",
      "Processing file 4/18: consum_telelectura_v2_part_004.parquet\n",
      "Processed 1,000,000 rows → 68,312 Barcelona rows\n",
      "Saved to consum_clean_bcn_part_004.parquet\n",
      "\n",
      "Processing file 5/18: consum_telelectura_v2_part_005.parquet\n",
      "Processed 1,000,000 rows → 391,385 Barcelona rows\n",
      "Saved to consum_clean_bcn_part_005.parquet\n",
      "\n",
      "Processing file 6/18: consum_telelectura_v2_part_006.parquet\n",
      "Processed 1,000,000 rows → 1,000,000 Barcelona rows\n",
      "Saved to consum_clean_bcn_part_006.parquet\n",
      "\n",
      "Processing file 7/18: consum_telelectura_v2_part_007.parquet\n",
      "Processed 1,000,000 rows → 903,223 Barcelona rows\n",
      "Saved to consum_clean_bcn_part_007.parquet\n",
      "\n",
      "Processing file 8/18: consum_telelectura_v2_part_008.parquet\n",
      "Processed 1,000,000 rows → 776,716 Barcelona rows\n",
      "Saved to consum_clean_bcn_part_008.parquet\n",
      "\n",
      "Processing file 9/18: consum_telelectura_v2_part_009.parquet\n",
      "Processed 1,000,000 rows → 735,317 Barcelona rows\n",
      "Saved to consum_clean_bcn_part_009.parquet\n",
      "\n",
      "Processing file 10/18: consum_telelectura_v2_part_010.parquet\n",
      "Processed 1,000,000 rows → 813,702 Barcelona rows\n",
      "Saved to consum_clean_bcn_part_010.parquet\n",
      "\n",
      "Processing file 11/18: consum_telelectura_v2_part_011.parquet\n",
      "Processed 1,000,000 rows → 786,354 Barcelona rows\n",
      "Saved to consum_clean_bcn_part_011.parquet\n",
      "\n",
      "Processing file 12/18: consum_telelectura_v2_part_012.parquet\n",
      "Processed 1,000,000 rows → 796,221 Barcelona rows\n",
      "Saved to consum_clean_bcn_part_012.parquet\n",
      "\n",
      "Processing file 13/18: consum_telelectura_v2_part_013.parquet\n",
      "Processed 1,000,000 rows → 732,441 Barcelona rows\n",
      "Saved to consum_clean_bcn_part_013.parquet\n",
      "\n",
      "Processing file 14/18: consum_telelectura_v2_part_014.parquet\n",
      "Processed 1,000,000 rows → 790,307 Barcelona rows\n",
      "Saved to consum_clean_bcn_part_014.parquet\n",
      "\n",
      "Processing file 15/18: consum_telelectura_v2_part_015.parquet\n",
      "Processed 1,000,000 rows → 596,488 Barcelona rows\n",
      "Saved to consum_clean_bcn_part_015.parquet\n",
      "\n",
      "Processing file 16/18: consum_telelectura_v2_part_016.parquet\n",
      "Processed 1,000,000 rows → 629,567 Barcelona rows\n",
      "Saved to consum_clean_bcn_part_016.parquet\n",
      "\n",
      "Processing file 17/18: consum_telelectura_v2_part_017.parquet\n",
      "Processed 1,000,000 rows → 677,368 Barcelona rows\n",
      "Saved to consum_clean_bcn_part_017.parquet\n",
      "\n",
      "Processing file 18/18: consum_telelectura_v2_part_018.parquet\n",
      "Processed 112,709 rows → 75,907 Barcelona rows\n",
      "Saved to consum_clean_bcn_part_018.parquet\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Total rows processed: 17,112,709\n",
      "Total Barcelona rows (after filtering): 10,068,383\n",
      "Cleaned files saved to: clean/split_consum_bcn/\n",
      "\n",
      "First rows sample from last processed file:\n",
      "     POLIZA_SUMINISTRO      FECHA  CONSUMO_REAL SECCIO_CENSAL US_AIGUA_GEST  \\\n",
      "1819  TMLN34TUQW5A4WUW 2021-01-14           165   08019301041             D   \n",
      "1820  TMLN34TUQW5A4WUW 2021-01-15           152   08019301041             D   \n",
      "1821  TMLN34TUQW5A4WUW 2021-01-16           149   08019301041             D   \n",
      "1822  TMLN34TUQW5A4WUW 2021-01-17           162   08019301041             D   \n",
      "1823  TMLN34TUQW5A4WUW 2021-01-18            96   08019301041             D   \n",
      "\n",
      "     DATA_INST_COMP  \n",
      "1819     2018-02-26  \n",
      "1820     2018-02-26  \n",
      "1821     2018-02-26  \n",
      "1822     2018-02-26  \n",
      "1823     2018-02-26  \n"
     ]
    }
   ],
   "source": [
    "# df_consum\n",
    "# Clean and filter df_consum data from split files\n",
    "\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Locate (or create) the directory that should contain the split chunks\n",
    "split_dir = Path(\"data/split_consum\")\n",
    "split_dir.mkdir(parents=True, exist_ok=True)\n",
    "split_pattern = \"consum_telelectura_v2_part_*.parquet\"\n",
    "split_files = sorted(glob.glob(str(split_dir / split_pattern)))\n",
    "\n",
    "if not split_files:\n",
    "    raw_consum_file = Path(\"data/consum_telelectura_v2.parquet\")\n",
    "    if raw_consum_file.exists():\n",
    "        print(\"No pre-split consumption files found. Splitting the raw file now (one-time operation)...\")\n",
    "        from split_parquet import split_parquet_efficient\n",
    "\n",
    "        split_chunk_size = 1_000_000\n",
    "        split_parquet_efficient(str(raw_consum_file), str(split_dir), chunk_size=split_chunk_size)\n",
    "        split_files = sorted(glob.glob(str(split_dir / split_pattern)))\n",
    "    else:\n",
    "        print(\"⚠ No split files found and raw file 'data/consum_telelectura_v2.parquet' is missing.\")\n",
    "        split_files = []\n",
    "\n",
    "if not split_files:\n",
    "    raise FileNotFoundError(\"Unable to locate consumption split files even after attempting to split the raw file.\")\n",
    "\n",
    "print(f\"Found {len(split_files)} split files to process\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Create output directory for cleaned files\n",
    "output_dir = Path(\"clean/split_consum_bcn\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Keep only relevant columns\n",
    "cols_keep_consum = [\n",
    "    'POLIZA_SUMINISTRO', 'FECHA', 'CONSUMO_REAL', 'SECCIO_CENSAL', 'US_AIGUA_GEST', 'DATA_INST_COMP'\n",
    "]\n",
    "\n",
    "# Track statistics\n",
    "total_rows_processed = 0\n",
    "total_rows_bcn = 0\n",
    "\n",
    "# Process each split file\n",
    "for i, file_path in enumerate(split_files, 1):\n",
    "    print(f\"\\nProcessing file {i}/{len(split_files)}: {Path(file_path).name}\")\n",
    "    \n",
    "    # Load one split file\n",
    "    df_chunk = pd.read_parquet(file_path)\n",
    "    total_rows_processed += len(df_chunk)\n",
    "    \n",
    "    # Convert FECHA and DATA_INST_COMP to datetime\n",
    "    for date_col in [\"FECHA\", \"DATA_INST_COMP\"]:\n",
    "        if date_col in df_chunk.columns:\n",
    "            df_chunk[date_col] = pd.to_datetime(df_chunk[date_col], errors=\"coerce\")\n",
    "    \n",
    "    # Keep only relevant columns\n",
    "    df_chunk_relevant = df_chunk[cols_keep_consum].copy()\n",
    "    \n",
    "    # Create df_consum_bcn: First filter to keep only rows where SECCIO_CENSAL starts with 8019\n",
    "    # Convert SECCIO_CENSAL to string to check if it starts with 8019\n",
    "    df_chunk_relevant['SECCIO_CENSAL'] = df_chunk_relevant['SECCIO_CENSAL'].astype(str)\n",
    "    mask_bcn = df_chunk_relevant['SECCIO_CENSAL'].str.startswith('8019', na=False)\n",
    "    df_chunk_bcn = df_chunk_relevant[mask_bcn].copy()\n",
    "    \n",
    "    # Then transform SECCIO_CENSAL: add 3 after 8019, then add 0 before all digits to get 11-digit strings\n",
    "    df_chunk_bcn['SECCIO_CENSAL'] = df_chunk_bcn['SECCIO_CENSAL'].str.replace('^8019', '80193', regex=True)\n",
    "    df_chunk_bcn['SECCIO_CENSAL'] = '0' + df_chunk_bcn['SECCIO_CENSAL']\n",
    "    \n",
    "    # Save cleaned split file\n",
    "    output_file = output_dir / f\"consum_clean_bcn_part_{i:03d}.parquet\"\n",
    "    df_chunk_bcn.to_parquet(output_file, index=False)\n",
    "    \n",
    "    total_rows_bcn += len(df_chunk_bcn)\n",
    "    print(f\"Processed {len(df_chunk):,} rows → {len(df_chunk_bcn):,} Barcelona rows\")\n",
    "    print(f\"Saved to {output_file.name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total rows processed: {total_rows_processed:,}\")\n",
    "print(f\"Total Barcelona rows (after filtering): {total_rows_bcn:,}\")\n",
    "print(f\"Cleaned files saved to: {output_dir}/\")\n",
    "print(f\"\\nFirst rows sample from last processed file:\")\n",
    "print(df_chunk_bcn.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3d4e27",
   "metadata": {},
   "source": [
    "### 3.2 Clean Leak Incidents Data\n",
    "\n",
    "Process leak incidents data split files:\n",
    "- Drop rows where both CODIGO_MENSAJE and CREATED_MENSAJE are missing (invalid records)\n",
    "- Filter to keep only CODIGO_MENSAJE = \"FUITA\" (remove \"REITERACIÓ DE FUITA\" and others)\n",
    "- Convert CREATED_MENSAJE to date only (year-month-day, no time component)\n",
    "- Remove duplicates based on POLISSA_SUBM and CREATED_MENSAJE (ensures one leak per row)\n",
    "- Filter to Barcelona (SECCIO_CENSAL starting with 08019)\n",
    "- Transform census section codes: add '3' after 08019 to get standard format\n",
    "- Keep relevant columns: POLISSA_SUBM, CREATED_MENSAJE, CODIGO_MENSAJE, US_AIGUA_SUBM, SECCIO_CENSAL\n",
    "- Save cleaned Barcelona data as a single parquet file: `clean/fuites_clean_bcn.parquet`\n",
    "\n",
    "**Result**: One leak per row (one leak per POLISSA_SUBM per day)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc2f5b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pre-split leak files found. Splitting the raw avisos file now (one-time operation)...\n",
      "Reading parquet file: data/consum_avisos_missatges_v2.parquet\n",
      "Chunk size: 1,000,000 rows\n",
      "Output directory: data/split_avisos\n",
      "------------------------------------------------------------\n",
      "Total rows in file: 76,372,248\n",
      "Number of row groups: 73\n",
      "Will create approximately 77 output files\n",
      "------------------------------------------------------------\n",
      "Reading row group 1/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 1: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_001.parquet\n",
      "Reading row group 2/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 2: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_002.parquet\n",
      "Reading row group 3/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 3: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_003.parquet\n",
      "Reading row group 4/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 4: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_004.parquet\n",
      "Reading row group 5/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 5: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_005.parquet\n",
      "Reading row group 6/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 6: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_006.parquet\n",
      "Reading row group 7/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 7: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_007.parquet\n",
      "Reading row group 8/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 8: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_008.parquet\n",
      "Reading row group 9/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 9: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_009.parquet\n",
      "Reading row group 10/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 10: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_010.parquet\n",
      "Reading row group 11/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 11: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_011.parquet\n",
      "Reading row group 12/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 12: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_012.parquet\n",
      "Reading row group 13/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 13: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_013.parquet\n",
      "Reading row group 14/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 14: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_014.parquet\n",
      "Reading row group 15/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 15: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_015.parquet\n",
      "Reading row group 16/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 16: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_016.parquet\n",
      "Reading row group 17/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 17: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_017.parquet\n",
      "Reading row group 18/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 18: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_018.parquet\n",
      "Reading row group 19/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 19: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_019.parquet\n",
      "Reading row group 20/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 20: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_020.parquet\n",
      "Reading row group 21/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 21: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_021.parquet\n",
      "  ✓ Chunk 22: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_022.parquet\n",
      "Reading row group 22/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 23: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_023.parquet\n",
      "Reading row group 23/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 24: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_024.parquet\n",
      "Reading row group 24/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 25: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_025.parquet\n",
      "Reading row group 25/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 26: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_026.parquet\n",
      "Reading row group 26/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 27: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_027.parquet\n",
      "Reading row group 27/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 28: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_028.parquet\n",
      "Reading row group 28/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 29: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_029.parquet\n",
      "Reading row group 29/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 30: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_030.parquet\n",
      "Reading row group 30/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 31: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_031.parquet\n",
      "Reading row group 31/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 32: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_032.parquet\n",
      "Reading row group 32/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 33: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_033.parquet\n",
      "Reading row group 33/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 34: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_034.parquet\n",
      "Reading row group 34/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 35: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_035.parquet\n",
      "Reading row group 35/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 36: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_036.parquet\n",
      "Reading row group 36/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 37: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_037.parquet\n",
      "Reading row group 37/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 38: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_038.parquet\n",
      "Reading row group 38/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 39: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_039.parquet\n",
      "Reading row group 39/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 40: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_040.parquet\n",
      "Reading row group 40/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 41: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_041.parquet\n",
      "Reading row group 41/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 42: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_042.parquet\n",
      "Reading row group 42/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 43: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_043.parquet\n",
      "  ✓ Chunk 44: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_044.parquet\n",
      "Reading row group 43/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 45: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_045.parquet\n",
      "Reading row group 44/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 46: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_046.parquet\n",
      "Reading row group 45/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 47: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_047.parquet\n",
      "Reading row group 46/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 48: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_048.parquet\n",
      "Reading row group 47/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 49: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_049.parquet\n",
      "Reading row group 48/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 50: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_050.parquet\n",
      "Reading row group 49/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 51: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_051.parquet\n",
      "Reading row group 50/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 52: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_052.parquet\n",
      "Reading row group 51/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 53: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_053.parquet\n",
      "Reading row group 52/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 54: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_054.parquet\n",
      "Reading row group 53/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 55: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_055.parquet\n",
      "Reading row group 54/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 56: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_056.parquet\n",
      "Reading row group 55/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 57: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_057.parquet\n",
      "Reading row group 56/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 58: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_058.parquet\n",
      "Reading row group 57/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 59: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_059.parquet\n",
      "Reading row group 58/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 60: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_060.parquet\n",
      "Reading row group 59/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 61: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_061.parquet\n",
      "Reading row group 60/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 62: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_062.parquet\n",
      "Reading row group 61/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 63: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_063.parquet\n",
      "Reading row group 62/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 64: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_064.parquet\n",
      "  ✓ Chunk 65: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_065.parquet\n",
      "Reading row group 63/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 66: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_066.parquet\n",
      "Reading row group 64/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 67: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_067.parquet\n",
      "Reading row group 65/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 68: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_068.parquet\n",
      "Reading row group 66/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 69: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_069.parquet\n",
      "Reading row group 67/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 70: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_070.parquet\n",
      "Reading row group 68/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 71: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_071.parquet\n",
      "Reading row group 69/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 72: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_072.parquet\n",
      "Reading row group 70/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 73: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_073.parquet\n",
      "Reading row group 71/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 74: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_074.parquet\n",
      "Reading row group 72/73... loaded 1,048,576 rows\n",
      "  ✓ Chunk 75: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_075.parquet\n",
      "Reading row group 73/73... loaded 874,776 rows\n",
      "  ✓ Chunk 76: Saved 1,000,000 rows to consum_avisos_missatges_v2_part_076.parquet\n",
      "  ✓ Chunk 77: Saved 372,248 rows to consum_avisos_missatges_v2_part_077.parquet\n",
      "------------------------------------------------------------\n",
      "Split complete! Created 77 files in data/split_avisos\n",
      "Found 77 split files to process\n",
      "================================================================================\n",
      "\n",
      "Processing file 1/77: consum_avisos_missatges_v2_part_001.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 260,788 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 186,526 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 552,231 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 439 rows (one leak per row)\n",
      "\n",
      "Processing file 2/77: consum_avisos_missatges_v2_part_002.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 262,029 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 186,274 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 551,240 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 441 rows (one leak per row)\n",
      "\n",
      "Processing file 3/77: consum_avisos_missatges_v2_part_003.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 262,824 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 185,496 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 551,221 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 443 rows (one leak per row)\n",
      "\n",
      "Processing file 4/77: consum_avisos_missatges_v2_part_004.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 265,324 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 184,673 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 549,544 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 442 rows (one leak per row)\n",
      "\n",
      "Processing file 5/77: consum_avisos_missatges_v2_part_005.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 265,319 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 184,750 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 549,471 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 443 rows (one leak per row)\n",
      "\n",
      "Processing file 6/77: consum_avisos_missatges_v2_part_006.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 266,677 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 184,461 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 548,401 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 444 rows (one leak per row)\n",
      "\n",
      "Processing file 7/77: consum_avisos_missatges_v2_part_007.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 265,298 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 185,249 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 548,992 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 444 rows (one leak per row)\n",
      "\n",
      "Processing file 8/77: consum_avisos_missatges_v2_part_008.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 264,800 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 184,383 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 550,350 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 450 rows (one leak per row)\n",
      "\n",
      "Processing file 9/77: consum_avisos_missatges_v2_part_009.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 262,664 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 183,684 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 553,178 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 457 rows (one leak per row)\n",
      "\n",
      "Processing file 10/77: consum_avisos_missatges_v2_part_010.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 263,593 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 183,354 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 552,578 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 458 rows (one leak per row)\n",
      "\n",
      "Processing file 11/77: consum_avisos_missatges_v2_part_011.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 267,356 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 184,872 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 547,297 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 458 rows (one leak per row)\n",
      "\n",
      "Processing file 12/77: consum_avisos_missatges_v2_part_012.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 269,731 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 185,426 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 544,373 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 453 rows (one leak per row)\n",
      "\n",
      "Processing file 13/77: consum_avisos_missatges_v2_part_013.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 270,622 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 185,362 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 543,543 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 456 rows (one leak per row)\n",
      "\n",
      "Processing file 14/77: consum_avisos_missatges_v2_part_014.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 269,582 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 186,504 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 543,439 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 458 rows (one leak per row)\n",
      "\n",
      "Processing file 15/77: consum_avisos_missatges_v2_part_015.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 269,028 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 186,504 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 543,993 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 458 rows (one leak per row)\n",
      "\n",
      "Processing file 16/77: consum_avisos_missatges_v2_part_016.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 270,113 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 186,014 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 543,400 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 456 rows (one leak per row)\n",
      "\n",
      "Processing file 17/77: consum_avisos_missatges_v2_part_017.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 271,529 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 185,724 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 542,275 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 455 rows (one leak per row)\n",
      "\n",
      "Processing file 18/77: consum_avisos_missatges_v2_part_018.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 271,543 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 186,141 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 541,845 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 453 rows (one leak per row)\n",
      "\n",
      "Processing file 19/77: consum_avisos_missatges_v2_part_019.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 271,719 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 186,488 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 541,324 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 451 rows (one leak per row)\n",
      "\n",
      "Processing file 20/77: consum_avisos_missatges_v2_part_020.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 270,862 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 186,741 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 541,929 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 450 rows (one leak per row)\n",
      "\n",
      "Processing file 21/77: consum_avisos_missatges_v2_part_021.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 271,158 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 186,591 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 541,783 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 450 rows (one leak per row)\n",
      "\n",
      "Processing file 22/77: consum_avisos_missatges_v2_part_022.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 270,844 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 186,522 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 542,166 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 450 rows (one leak per row)\n",
      "\n",
      "Processing file 23/77: consum_avisos_missatges_v2_part_023.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 271,423 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 186,399 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 541,709 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 451 rows (one leak per row)\n",
      "\n",
      "Processing file 24/77: consum_avisos_missatges_v2_part_024.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 270,554 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 186,572 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 542,405 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 451 rows (one leak per row)\n",
      "\n",
      "Processing file 25/77: consum_avisos_missatges_v2_part_025.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 270,878 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 186,841 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 541,812 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 451 rows (one leak per row)\n",
      "\n",
      "Processing file 26/77: consum_avisos_missatges_v2_part_026.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 271,737 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 187,121 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 540,676 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 448 rows (one leak per row)\n",
      "\n",
      "Processing file 27/77: consum_avisos_missatges_v2_part_027.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 273,278 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 186,841 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 539,415 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 448 rows (one leak per row)\n",
      "\n",
      "Processing file 28/77: consum_avisos_missatges_v2_part_028.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 275,469 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 185,973 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 537,694 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 833 rows (one leak per row)\n",
      "\n",
      "Processing file 29/77: consum_avisos_missatges_v2_part_029.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 314,929 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 168,917 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 515,751 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 390 rows (one leak per row)\n",
      "\n",
      "Processing file 30/77: consum_avisos_missatges_v2_part_030.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 316,044 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 169,586 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 513,969 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 388 rows (one leak per row)\n",
      "\n",
      "Processing file 31/77: consum_avisos_missatges_v2_part_031.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 314,246 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 169,065 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 516,282 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 394 rows (one leak per row)\n",
      "\n",
      "Processing file 32/77: consum_avisos_missatges_v2_part_032.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 315,007 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 168,672 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 515,910 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 398 rows (one leak per row)\n",
      "\n",
      "Processing file 33/77: consum_avisos_missatges_v2_part_033.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 314,211 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 168,569 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 516,809 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 398 rows (one leak per row)\n",
      "\n",
      "Processing file 34/77: consum_avisos_missatges_v2_part_034.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 316,025 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 168,383 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 515,182 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 397 rows (one leak per row)\n",
      "\n",
      "Processing file 35/77: consum_avisos_missatges_v2_part_035.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 317,730 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 168,188 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 513,672 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 397 rows (one leak per row)\n",
      "\n",
      "Processing file 36/77: consum_avisos_missatges_v2_part_036.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 317,574 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 168,324 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 513,691 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 398 rows (one leak per row)\n",
      "\n",
      "Processing file 37/77: consum_avisos_missatges_v2_part_037.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 316,557 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 167,914 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 515,114 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 402 rows (one leak per row)\n",
      "\n",
      "Processing file 38/77: consum_avisos_missatges_v2_part_038.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 319,214 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 167,298 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 513,073 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 402 rows (one leak per row)\n",
      "\n",
      "Processing file 39/77: consum_avisos_missatges_v2_part_039.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 320,107 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 167,443 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 512,034 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 403 rows (one leak per row)\n",
      "\n",
      "Processing file 40/77: consum_avisos_missatges_v2_part_040.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 318,022 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 168,814 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 512,750 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 401 rows (one leak per row)\n",
      "\n",
      "Processing file 41/77: consum_avisos_missatges_v2_part_041.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 317,287 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 169,913 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 512,384 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 403 rows (one leak per row)\n",
      "\n",
      "Processing file 42/77: consum_avisos_missatges_v2_part_042.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 316,393 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 170,211 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 512,978 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 405 rows (one leak per row)\n",
      "\n",
      "Processing file 43/77: consum_avisos_missatges_v2_part_043.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 315,363 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 170,050 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 514,169 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 405 rows (one leak per row)\n",
      "\n",
      "Processing file 44/77: consum_avisos_missatges_v2_part_044.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 315,379 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 169,856 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 514,347 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 405 rows (one leak per row)\n",
      "\n",
      "Processing file 45/77: consum_avisos_missatges_v2_part_045.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 314,667 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 170,380 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 514,532 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 408 rows (one leak per row)\n",
      "\n",
      "Processing file 46/77: consum_avisos_missatges_v2_part_046.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 313,695 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 171,949 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 513,935 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 408 rows (one leak per row)\n",
      "\n",
      "Processing file 47/77: consum_avisos_missatges_v2_part_047.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 314,022 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 171,937 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 513,622 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 406 rows (one leak per row)\n",
      "\n",
      "Processing file 48/77: consum_avisos_missatges_v2_part_048.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 314,760 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 172,046 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 512,777 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 404 rows (one leak per row)\n",
      "\n",
      "Processing file 49/77: consum_avisos_missatges_v2_part_049.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 316,420 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 171,749 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 511,414 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 404 rows (one leak per row)\n",
      "\n",
      "Processing file 50/77: consum_avisos_missatges_v2_part_050.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 317,088 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 171,489 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 511,004 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 406 rows (one leak per row)\n",
      "\n",
      "Processing file 51/77: consum_avisos_missatges_v2_part_051.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 317,093 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 171,345 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 511,144 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 405 rows (one leak per row)\n",
      "\n",
      "Processing file 52/77: consum_avisos_missatges_v2_part_052.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 318,790 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 157,616 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 522,779 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 799 rows (one leak per row)\n",
      "\n",
      "Processing file 53/77: consum_avisos_missatges_v2_part_053.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 321,381 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 154,372 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 523,845 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 399 rows (one leak per row)\n",
      "\n",
      "Processing file 54/77: consum_avisos_missatges_v2_part_054.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 320,688 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 153,314 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 525,589 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 406 rows (one leak per row)\n",
      "\n",
      "Processing file 55/77: consum_avisos_missatges_v2_part_055.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 318,915 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 152,615 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 528,061 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 406 rows (one leak per row)\n",
      "\n",
      "Processing file 56/77: consum_avisos_missatges_v2_part_056.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 321,517 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 151,494 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 526,578 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 408 rows (one leak per row)\n",
      "\n",
      "Processing file 57/77: consum_avisos_missatges_v2_part_057.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 323,258 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 150,384 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 525,951 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 404 rows (one leak per row)\n",
      "\n",
      "Processing file 58/77: consum_avisos_missatges_v2_part_058.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 325,058 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 150,230 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 524,309 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 400 rows (one leak per row)\n",
      "\n",
      "Processing file 59/77: consum_avisos_missatges_v2_part_059.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 327,163 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 149,456 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 522,977 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 401 rows (one leak per row)\n",
      "\n",
      "Processing file 60/77: consum_avisos_missatges_v2_part_060.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 329,432 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 148,452 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 521,711 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 402 rows (one leak per row)\n",
      "\n",
      "Processing file 61/77: consum_avisos_missatges_v2_part_061.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 329,372 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 148,293 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 521,926 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 406 rows (one leak per row)\n",
      "\n",
      "Processing file 62/77: consum_avisos_missatges_v2_part_062.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 328,493 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 147,739 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 523,360 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 405 rows (one leak per row)\n",
      "\n",
      "Processing file 63/77: consum_avisos_missatges_v2_part_063.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 330,543 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 147,492 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 521,557 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 405 rows (one leak per row)\n",
      "\n",
      "Processing file 64/77: consum_avisos_missatges_v2_part_064.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 331,623 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 147,410 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 520,557 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 407 rows (one leak per row)\n",
      "\n",
      "Processing file 65/77: consum_avisos_missatges_v2_part_065.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 330,038 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 148,508 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 521,043 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 408 rows (one leak per row)\n",
      "\n",
      "Processing file 66/77: consum_avisos_missatges_v2_part_066.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 330,581 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 148,455 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 520,554 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 407 rows (one leak per row)\n",
      "\n",
      "Processing file 67/77: consum_avisos_missatges_v2_part_067.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 333,640 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 147,260 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 518,691 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 406 rows (one leak per row)\n",
      "\n",
      "Processing file 68/77: consum_avisos_missatges_v2_part_068.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 334,427 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 146,942 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 518,223 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 405 rows (one leak per row)\n",
      "\n",
      "Processing file 69/77: consum_avisos_missatges_v2_part_069.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 335,406 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 146,914 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 517,272 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 405 rows (one leak per row)\n",
      "\n",
      "Processing file 70/77: consum_avisos_missatges_v2_part_070.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 335,709 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 147,361 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 516,521 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 406 rows (one leak per row)\n",
      "\n",
      "Processing file 71/77: consum_avisos_missatges_v2_part_071.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 336,000 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 147,739 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 515,850 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 408 rows (one leak per row)\n",
      "\n",
      "Processing file 72/77: consum_avisos_missatges_v2_part_072.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 338,345 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 147,266 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 513,978 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 408 rows (one leak per row)\n",
      "\n",
      "Processing file 73/77: consum_avisos_missatges_v2_part_073.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 345,120 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 144,563 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 509,905 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 409 rows (one leak per row)\n",
      "\n",
      "Processing file 74/77: consum_avisos_missatges_v2_part_074.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 343,338 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 146,471 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 509,779 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 409 rows (one leak per row)\n",
      "\n",
      "Processing file 75/77: consum_avisos_missatges_v2_part_075.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 387,028 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 132,530 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 479,995 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 444 rows (one leak per row)\n",
      "\n",
      "Processing file 76/77: consum_avisos_missatges_v2_part_076.parquet\n",
      "  ✓ Processed 1,000,000 rows\n",
      "  ✓ Dropped 402,553 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 130,656 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 466,753 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 38 rows (one leak per row)\n",
      "\n",
      "Processing file 77/77: consum_avisos_missatges_v2_part_077.parquet\n",
      "  ✓ Processed 372,248 rows\n",
      "  ✓ Dropped 151,980 rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\n",
      "  ✓ Dropped 48,304 rows (CODIGO_MENSAJE != 'FUITA')\n",
      "  ✓ Dropped 0 rows (invalid CREATED_MENSAJE date)\n",
      "  ✓ Dropped 171,927 duplicate rows (same POLISSA_SUBM + date)\n",
      "  ✓ After filtering Barcelona: 37 rows (one leak per row)\n",
      "\n",
      "================================================================================\n",
      "COMBINING ALL CLEANED DATA\n",
      "================================================================================\n",
      "Combining all cleaned data...\n",
      "\n",
      "Saving to clean/fuites_clean_bcn.parquet...\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Total rows processed: 76,372,248\n",
      "Total rows dropped (both CODIGO_MENSAJE and CREATED_MENSAJE missing): 23,318,973\n",
      "Total rows dropped (CODIGO_MENSAJE != 'FUITA'): 12,845,420\n",
      "Total rows dropped (duplicates: same POLISSA_SUBM + date): 40,174,518\n",
      "Total rows dropped (final deduplication across files): 31,097\n",
      "Total Barcelona rows (after all filtering, one leak per row): 1,359\n",
      "Cleaned file saved to: clean/fuites_clean_bcn.parquet\n",
      "\n",
      "First rows sample:\n",
      "       POLISSA_SUBM CREATED_MENSAJE CODIGO_MENSAJE US_AIGUA_SUBM SECCIO_CENSAL\n",
      "0  KWHZ5UG2ZKENUFC2      2023-12-03          FUITA      DOMÈSTIC   08019305059\n",
      "1  GVXPU34GVXQUIWFK      2023-08-10          FUITA      DOMÈSTIC   08019310139\n",
      "2  GVXPU34GVXQUIWFK      2023-06-10          FUITA      DOMÈSTIC   08019310139\n",
      "3  I7GGTJ6C6FMR5ARW      2024-09-06          FUITA      DOMÈSTIC   08019302087\n",
      "4  I7GGTJ6C6FMR5ARW      2024-11-13          FUITA      DOMÈSTIC   08019302087\n",
      "\n",
      "Dataset shape: (1359, 5)\n",
      "Columns: ['POLISSA_SUBM', 'CREATED_MENSAJE', 'CODIGO_MENSAJE', 'US_AIGUA_SUBM', 'SECCIO_CENSAL']\n"
     ]
    }
   ],
   "source": [
    "# df_fuites - Process split files one at a time to avoid memory issues\n",
    "# Clean and filter df_fuites data from split files\n",
    "\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Locate (or create) the directory with the avisos split files\n",
    "split_dir = Path(\"data/split_avisos\")\n",
    "split_dir.mkdir(parents=True, exist_ok=True)\n",
    "split_pattern = \"consum_avisos_missatges_v2_part_*.parquet\"\n",
    "split_files = sorted(glob.glob(str(split_dir / split_pattern)))\n",
    "\n",
    "if not split_files:\n",
    "    raw_avisos_file = Path(\"data/consum_avisos_missatges_v2.parquet\")\n",
    "    if raw_avisos_file.exists():\n",
    "        print(\"No pre-split leak files found. Splitting the raw avisos file now (one-time operation)...\")\n",
    "        from split_parquet import split_parquet_efficient\n",
    "\n",
    "        split_chunk_size = 1_000_000\n",
    "        split_parquet_efficient(str(raw_avisos_file), str(split_dir), chunk_size=split_chunk_size)\n",
    "        split_files = sorted(glob.glob(str(split_dir / split_pattern)))\n",
    "    else:\n",
    "        print(\"⚠ No split files found and raw file 'data/consum_avisos_missatges_v2.parquet' is missing.\")\n",
    "        split_files = []\n",
    "\n",
    "if not split_files:\n",
    "    raise FileNotFoundError(\"Unable to locate leak split files even after attempting to split the raw file.\")\n",
    "\n",
    "print(f\"Found {len(split_files)} split files to process\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create output directory for cleaned files\n",
    "output_dir = Path(\"clean\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Keep only relevant columns\n",
    "cols_keep_fuites = [\n",
    "    'POLISSA_SUBM', 'CREATED_MENSAJE', 'CODIGO_MENSAJE',\n",
    "    'US_AIGUA_SUBM', 'SECCIO_CENSAL'\n",
    "]\n",
    "\n",
    "# Track statistics\n",
    "total_rows_processed = 0\n",
    "total_rows_dropped_both_missing = 0\n",
    "total_rows_dropped_fuita = 0\n",
    "total_rows_dropped_duplicates = 0\n",
    "total_rows_bcn = 0\n",
    "\n",
    "# List to collect all cleaned data\n",
    "fuites_clean_list = []\n",
    "\n",
    "# Process each split file\n",
    "for i, file_path in enumerate(split_files, 1):\n",
    "    print(f\"\\nProcessing file {i}/{len(split_files)}: {Path(file_path).name}\")\n",
    "    \n",
    "    # Load one split file\n",
    "    df_chunk = pd.read_parquet(file_path)\n",
    "    total_rows_processed += len(df_chunk)\n",
    "    \n",
    "    # Keep only relevant columns\n",
    "    df_chunk_relevant = df_chunk[cols_keep_fuites].copy()\n",
    "    \n",
    "    # 1. Drop rows where both CODIGO_MENSAJE and CREATED_MENSAJE are missing\n",
    "    mask_both_missing = df_chunk_relevant['CODIGO_MENSAJE'].isna() & df_chunk_relevant['CREATED_MENSAJE'].isna()\n",
    "    rows_dropped = mask_both_missing.sum()\n",
    "    total_rows_dropped_both_missing += rows_dropped\n",
    "    df_chunk_relevant = df_chunk_relevant[~mask_both_missing].copy()\n",
    "    \n",
    "    # 2. Filter to keep only CODIGO_MENSAJE = \"FUITA\" (remove \"REITERACIÓ DE FUITA\" and others)\n",
    "    mask_fuita = df_chunk_relevant['CODIGO_MENSAJE'] == 'FUITA'\n",
    "    rows_before_fuita_filter = len(df_chunk_relevant)\n",
    "    df_chunk_relevant = df_chunk_relevant[mask_fuita].copy()\n",
    "    rows_dropped_fuita = rows_before_fuita_filter - len(df_chunk_relevant)\n",
    "    \n",
    "    # 3. Convert CREATED_MENSAJE to datetime and extract date only (year-month-day, no time)\n",
    "    df_chunk_relevant['CREATED_MENSAJE'] = pd.to_datetime(df_chunk_relevant['CREATED_MENSAJE'], errors='coerce')\n",
    "    # Drop rows where CREATED_MENSAJE is NaT (invalid dates)\n",
    "    rows_before_date_filter = len(df_chunk_relevant)\n",
    "    df_chunk_relevant = df_chunk_relevant[df_chunk_relevant['CREATED_MENSAJE'].notna()].copy()\n",
    "    rows_dropped_invalid_date = rows_before_date_filter - len(df_chunk_relevant)\n",
    "    # Extract date only (year-month-day, no time component)\n",
    "    df_chunk_relevant['CREATED_MENSAJE'] = df_chunk_relevant['CREATED_MENSAJE'].dt.date\n",
    "    \n",
    "    # 4. Remove duplicates based on POLISSA_SUBM and CREATED_MENSAJE (date only)\n",
    "    # This ensures one leak per row (one leak per POLISSA_SUBM per day)\n",
    "    rows_before_dedup = len(df_chunk_relevant)\n",
    "    df_chunk_relevant = df_chunk_relevant.drop_duplicates(subset=['POLISSA_SUBM', 'CREATED_MENSAJE'], keep='first')\n",
    "    rows_dropped_duplicates = rows_before_dedup - len(df_chunk_relevant)\n",
    "    \n",
    "    # 5. Filter to keep only SECCIO_CENSAL starting with '08019' which corresponds to Barcelona\n",
    "    df_chunk_relevant['SECCIO_CENSAL'] = df_chunk_relevant['SECCIO_CENSAL'].astype(str)\n",
    "    mask_bcn_fuites = df_chunk_relevant['SECCIO_CENSAL'].str.startswith('08019', na=False)\n",
    "    df_chunk_bcn = df_chunk_relevant[mask_bcn_fuites].copy()\n",
    "    \n",
    "    # 6. Transform SECCIO_CENSAL: add 3 after 08019 (no need to add 0, it's already included)\n",
    "    df_chunk_bcn['SECCIO_CENSAL'] = df_chunk_bcn['SECCIO_CENSAL'].str.replace('^08019', '080193', regex=True)\n",
    "    \n",
    "    # Collect cleaned data\n",
    "    fuites_clean_list.append(df_chunk_bcn)\n",
    "    total_rows_bcn += len(df_chunk_bcn)\n",
    "    total_rows_dropped_fuita += rows_dropped_fuita\n",
    "    total_rows_dropped_duplicates += rows_dropped_duplicates\n",
    "    print(f\"  ✓ Processed {len(df_chunk):,} rows\")\n",
    "    print(f\"  ✓ Dropped {rows_dropped:,} rows (both CODIGO_MENSAJE and CREATED_MENSAJE missing)\")\n",
    "    print(f\"  ✓ Dropped {rows_dropped_fuita:,} rows (CODIGO_MENSAJE != 'FUITA')\")\n",
    "    print(f\"  ✓ Dropped {rows_dropped_invalid_date:,} rows (invalid CREATED_MENSAJE date)\")\n",
    "    print(f\"  ✓ Dropped {rows_dropped_duplicates:,} duplicate rows (same POLISSA_SUBM + date)\")\n",
    "    print(f\"  ✓ After filtering Barcelona: {len(df_chunk_bcn):,} rows (one leak per row)\")\n",
    "\n",
    "# Combine all cleaned data\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMBINING ALL CLEANED DATA\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Combining all cleaned data...\")\n",
    "df_fuites_clean = pd.concat(fuites_clean_list, ignore_index=True)\n",
    "\n",
    "# Remove any duplicates that might exist across files (same POLISSA_SUBM + CREATED_MENSAJE)\n",
    "rows_before_final_dedup = len(df_fuites_clean)\n",
    "df_fuites_clean = df_fuites_clean.drop_duplicates(subset=['POLISSA_SUBM', 'CREATED_MENSAJE'], keep='first')\n",
    "rows_dropped_final_duplicates = rows_before_final_dedup - len(df_fuites_clean)\n",
    "\n",
    "# Save as single parquet file\n",
    "output_file = output_dir / \"fuites_clean_bcn.parquet\"\n",
    "print(f\"\\nSaving to {output_file}...\")\n",
    "df_fuites_clean.to_parquet(output_file, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total rows processed: {total_rows_processed:,}\")\n",
    "print(f\"Total rows dropped (both CODIGO_MENSAJE and CREATED_MENSAJE missing): {total_rows_dropped_both_missing:,}\")\n",
    "print(f\"Total rows dropped (CODIGO_MENSAJE != 'FUITA'): {total_rows_dropped_fuita:,}\")\n",
    "print(f\"Total rows dropped (duplicates: same POLISSA_SUBM + date): {total_rows_dropped_duplicates:,}\")\n",
    "print(f\"Total rows dropped (final deduplication across files): {rows_dropped_final_duplicates:,}\")\n",
    "print(f\"Total Barcelona rows (after all filtering, one leak per row): {len(df_fuites_clean):,}\")\n",
    "print(f\"Cleaned file saved to: {output_file}\")\n",
    "print(f\"\\nFirst rows sample:\")\n",
    "print(df_fuites_clean.head())\n",
    "print(f\"\\nDataset shape: {df_fuites_clean.shape}\")\n",
    "print(f\"Columns: {list(df_fuites_clean.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac04087",
   "metadata": {},
   "source": [
    "### 3.3 Clean Weather Data\n",
    "\n",
    "Process weather data:\n",
    "- Replace null values in HORA_TU with \"NA\" (only extreme values have time information)\n",
    "- This cleaning step ensures consistent data format for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccf0869",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import time\n",
    "\n",
    "\n",
    "def normalize_hour_column(series, col_name):\n",
    "    \"\"\"Convert datetime.time objects to strings and normalize missing values.\"\"\"\n",
    "    # Treat blank strings or placeholder values as missing\n",
    "    series = series.replace(r'^\\s*$', pd.NA, regex=True)\n",
    "    series = series.replace(['NA', 'NaN', 'nan'], pd.NA)\n",
    "    null_count = series.isna().sum()\n",
    "    print(f\"Detected {null_count:,} missing values in '{col_name}' before fill\")\n",
    "\n",
    "    def convert_time_to_str(val):\n",
    "        if pd.isna(val):\n",
    "            return pd.NA\n",
    "        if isinstance(val, time):\n",
    "            return val.strftime('%H:%M:%S')\n",
    "        return str(val)\n",
    "\n",
    "    series = series.apply(convert_time_to_str)\n",
    "    series = series.fillna('NA')\n",
    "    return series, null_count\n",
    "\n",
    "\n",
    "if 'HORA _TU' in df_weather.columns:\n",
    "    df_weather['HORA _TU'], null_count = normalize_hour_column(df_weather['HORA _TU'], 'HORA _TU')\n",
    "    print(f\"Replaced missing values in 'HORA _TU' with 'NA' and converted times to strings\")\n",
    "elif 'HORA_TU' in df_weather.columns:\n",
    "    df_weather['HORA_TU'], null_count = normalize_hour_column(df_weather['HORA_TU'], 'HORA_TU')\n",
    "    print(f\"Replaced missing values in 'HORA_TU' with 'NA' and converted times to strings\")\n",
    "else:\n",
    "    print(\"⚠ Column 'HORA _TU' or 'HORA_TU' not found in weather data\")\n",
    "\n",
    "# Convert VALOR column (comma decimal) to numeric\n",
    "if 'VALOR' in df_weather.columns:\n",
    "    df_weather['VALOR_NUM'] = (\n",
    "        df_weather['VALOR']\n",
    "        .astype(str)\n",
    "        .str.replace('.', '', regex=False)  # remove thousand separators if any\n",
    "        .str.replace(',', '.', regex=False)\n",
    "    )\n",
    "    df_weather['VALOR_NUM'] = pd.to_numeric(df_weather['VALOR_NUM'], errors='coerce')\n",
    "else:\n",
    "    print(\"⚠ Column 'VALOR' not found in weather data\")\n",
    "\n",
    "print(\"\\nSample of cleaned weather data:\")\n",
    "print(df_weather.head())\n",
    "\n",
    "# Save cleaned weather data\n",
    "output_dir = Path(\"clean\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_file = output_dir / \"weather_clean.parquet\"\n",
    "df_weather.to_parquet(output_file, index=False)\n",
    "print(f\"\\n✓ Saved cleaned weather data to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9d3a3c",
   "metadata": {},
   "source": [
    "### 3.4 Clean Socioeconomic Data\n",
    "\n",
    "Process socioeconomic index data:\n",
    "- Rename column to match other datasets (\"secció censal\" → \"SECCIO_CENSAL\")\n",
    "- Filter to Barcelona (SECCIO_CENSAL starting with \"080193\")\n",
    "- Remove \"estat\" column (100% null values)\n",
    "- Save cleaned data to clean directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a827ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Filter df_socio for Barcelona and remove \"estat\" column\n",
    "# First, rename \"secció censal\" to \"SECCIO_CENSAL\" to match other datasets\n",
    "if 'secció censal' in df_socio.columns:\n",
    "    df_socio = df_socio.rename(columns={'secció censal': 'SECCIO_CENSAL'})\n",
    "\n",
    "# Filter for Barcelona: SECCIO_CENSAL starting with \"080193\"\n",
    "mask_bcn = df_socio['SECCIO_CENSAL'].str.startswith('080193', na=False)\n",
    "df_socio_bcn = df_socio[mask_bcn].copy()\n",
    "\n",
    "print(f\"Filtering df_socio for Barcelona (SECCIO_CENSAL starting with '080193'):\")\n",
    "print(f\"  Original rows: {len(df_socio):,}\")\n",
    "print(f\"  Barcelona rows: {len(df_socio_bcn):,}\")\n",
    "print(f\"  Rows filtered out: {len(df_socio) - len(df_socio_bcn):,}\")\n",
    "\n",
    "# Remove \"estat\" column (100% null)\n",
    "if 'estat' in df_socio_bcn.columns:\n",
    "    df_socio_bcn = df_socio_bcn.drop(columns=['estat'])\n",
    "    print(f\"\\n✓ Removed 'estat' column\")\n",
    "\n",
    "print(f\"\\nFinal df_socio_bcn shape: {df_socio_bcn.shape}\")\n",
    "print(f\"Columns: {list(df_socio_bcn.columns)}\")\n",
    "print(f\"\\nFirst rows:\")\n",
    "df_socio_bcn.head()\n",
    "\n",
    "# Save cleaned socioeconomic data\n",
    "output_dir = Path(\"clean\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_file = output_dir / \"socio_clean.parquet\"\n",
    "df_socio_bcn.to_parquet(output_file, index=False)\n",
    "print(f\"\\n✓ Saved cleaned socioeconomic data to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c052a1d",
   "metadata": {},
   "source": [
    "## 4. Temporal and Spatial Coverage Analysis\n",
    "\n",
    "After cleaning the data, we analyze both temporal and spatial coverage of each dataset to understand:\n",
    "- **Temporal coverage**: Date ranges for each dataset and overlaps between datasets (important for joining data)\n",
    "- **Spatial coverage**: Number of unique census sections in each dataset and overlaps between datasets (important for merging data)\n",
    "\n",
    "This analysis helps identify the common time period where all three datasets (consumption, leaks, and weather) have data available.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc6c0e1",
   "metadata": {},
   "source": [
    "### 4.1 Weather Station Time Coverage\n",
    "\n",
    "Check the temporal coverage for each weather station to understand data availability. This helps identify which stations have complete data and which periods are covered by all stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92751ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check time coverage for each weather station\n",
    "# Show earliest and latest DATA_LECTURA for each unique station\n",
    "print(\"Time coverage per weather station:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Convert DATA_LECTURA to datetime if it's not already\n",
    "df_weather['DATA_LECTURA'] = pd.to_datetime(df_weather['DATA_LECTURA'], format='%d/%m/%Y', errors='coerce')\n",
    "\n",
    "# Group by station and get min/max dates\n",
    "station_coverage = df_weather.groupby(['CODI_ESTACIO']).agg({\n",
    "    'DATA_LECTURA': ['min', 'max', 'count']\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten column names\n",
    "station_coverage.columns = ['CODI_ESTACIO', 'Earliest_Date', 'Latest_Date', 'Total_Records']\n",
    "\n",
    "# Calculate date range in days\n",
    "station_coverage['Date_Range_Days'] = (station_coverage['Latest_Date'] - station_coverage['Earliest_Date']).dt.days\n",
    "\n",
    "# Store datetime versions for calculations\n",
    "earliest_dt = station_coverage['Earliest_Date'].min()\n",
    "latest_dt = station_coverage['Latest_Date'].max()\n",
    "\n",
    "# Format dates for display\n",
    "station_coverage['Earliest_Date'] = station_coverage['Earliest_Date'].dt.strftime('%Y-%m-%d')\n",
    "station_coverage['Latest_Date'] = station_coverage['Latest_Date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Sort by earliest date\n",
    "station_coverage = station_coverage.sort_values('Earliest_Date')\n",
    "\n",
    "print(f\"\\nTotal stations: {len(station_coverage)}\")\n",
    "print(f\"\\nTime coverage summary:\")\n",
    "print(station_coverage.to_string(index=False))\n",
    "\n",
    "print(f\"\\n\\nOverall date range:\")\n",
    "print(f\"  Earliest: {earliest_dt.strftime('%Y-%m-%d')}\")\n",
    "print(f\"  Latest: {latest_dt.strftime('%Y-%m-%d')}\")\n",
    "print(f\"  Total days: {(latest_dt - earliest_dt).days}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d61f540",
   "metadata": {},
   "source": [
    "### 4.2 Temporal Coverage Check for Consumption and Leak Data\n",
    "\n",
    "Check the date ranges for cleaned consumption and leak data, and compute overlaps between all three datasets (consumption, leaks, and weather). This information is crucial for determining the analysis period where all data sources are available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46227df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal check for cleaned consum data\n",
    "# Check time coverage for cleaned consumption data\n",
    "print(\"=\" * 80)\n",
    "print(\"TEMPORAL CHECK: Cleaned Consumption Data (consum)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Get all cleaned consum split files\n",
    "consum_dir = Path(\"clean/split_consum_bcn\")\n",
    "consum_files = sorted(glob.glob(str(consum_dir / \"consum_clean_bcn_part_*.parquet\")))\n",
    "\n",
    "print(f\"\\nFound {len(consum_files)} cleaned consum files to analyze\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Load and process each file to get date ranges (memory-efficient: track min/max only)\n",
    "total_records_consum = 0\n",
    "earliest_consum = None\n",
    "latest_consum = None\n",
    "\n",
    "for file_path in consum_files:\n",
    "    df_chunk = pd.read_parquet(file_path)\n",
    "    total_records_consum += len(df_chunk)\n",
    "    \n",
    "    # Convert FECHA to datetime if not already\n",
    "    if 'FECHA' in df_chunk.columns:\n",
    "        df_chunk['FECHA'] = pd.to_datetime(df_chunk['FECHA'], errors='coerce')\n",
    "        # Get valid dates (not null)\n",
    "        valid_dates = df_chunk['FECHA'].dropna()\n",
    "        if len(valid_dates) > 0:\n",
    "            file_min = valid_dates.min()\n",
    "            file_max = valid_dates.max()\n",
    "            if earliest_consum is None or file_min < earliest_consum:\n",
    "                earliest_consum = file_min\n",
    "            if latest_consum is None or file_max > latest_consum:\n",
    "                latest_consum = file_max\n",
    "\n",
    "if earliest_consum is not None and latest_consum is not None:\n",
    "    date_range_days_consum = (latest_consum - earliest_consum).days\n",
    "    \n",
    "    print(f\"\\nOverall date range for cleaned consum:\")\n",
    "    print(f\"  Earliest date: {earliest_consum.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"  Latest date: {latest_consum.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"  Total days: {date_range_days_consum:,}\")\n",
    "    print(f\"  Total records: {total_records_consum:,}\")\n",
    "else:\n",
    "    print(\"\\n⚠ No valid dates found in cleaned consum data\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEMPORAL CHECK: Cleaned Leak Incidents Data (fuites/avisos)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load single cleaned fuites file\n",
    "fuites_file = Path(\"clean/fuites_clean_bcn.parquet\")\n",
    "\n",
    "if fuites_file.exists():\n",
    "    print(f\"\\nLoading cleaned fuites data from: {fuites_file}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    df_fuites_temp = pd.read_parquet(fuites_file)\n",
    "    total_records_fuites = len(df_fuites_temp)\n",
    "    \n",
    "    # Convert CREATED_MENSAJE to datetime if not already\n",
    "    if 'CREATED_MENSAJE' in df_fuites_temp.columns:\n",
    "        df_fuites_temp['CREATED_MENSAJE'] = pd.to_datetime(df_fuites_temp['CREATED_MENSAJE'], errors='coerce')\n",
    "        # Get valid dates (not null)\n",
    "        valid_dates = df_fuites_temp['CREATED_MENSAJE'].dropna()\n",
    "        if len(valid_dates) > 0:\n",
    "            earliest_fuites = valid_dates.min()\n",
    "            latest_fuites = valid_dates.max()\n",
    "        else:\n",
    "            earliest_fuites = None\n",
    "            latest_fuites = None\n",
    "    else:\n",
    "        earliest_fuites = None\n",
    "        latest_fuites = None\n",
    "else:\n",
    "    print(f\"\\n⚠ File not found: {fuites_file}\")\n",
    "    total_records_fuites = 0\n",
    "    earliest_fuites = None\n",
    "    latest_fuites = None\n",
    "\n",
    "if earliest_fuites is not None and latest_fuites is not None:\n",
    "    date_range_days_fuites = (latest_fuites - earliest_fuites).days\n",
    "    \n",
    "    print(f\"\\nOverall date range for cleaned fuites:\")\n",
    "    print(f\"  Earliest date: {earliest_fuites.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"  Latest date: {latest_fuites.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"  Total days: {date_range_days_fuites:,}\")\n",
    "    print(f\"  Total records: {total_records_fuites:,}\")\n",
    "else:\n",
    "    print(\"\\n⚠ No valid dates found in cleaned fuites data\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARISON WITH WEATHER DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Compute weather data range dynamically\n",
    "if 'df_weather' in globals() and 'DATA_LECTURA' in df_weather.columns:\n",
    "    df_weather_temp = df_weather.copy()\n",
    "    df_weather_temp['DATA_LECTURA'] = pd.to_datetime(df_weather_temp['DATA_LECTURA'], format='%d/%m/%Y', errors='coerce')\n",
    "    valid_dates = df_weather_temp['DATA_LECTURA'].dropna()\n",
    "    if len(valid_dates) > 0:\n",
    "        earliest_weather = valid_dates.min()\n",
    "        latest_weather = valid_dates.max()\n",
    "        date_range_days_weather = (latest_weather - earliest_weather).days\n",
    "    else:\n",
    "        earliest_weather = None\n",
    "        latest_weather = None\n",
    "        date_range_days_weather = None\n",
    "else:\n",
    "    earliest_weather = None\n",
    "    latest_weather = None\n",
    "    date_range_days_weather = None\n",
    "\n",
    "if earliest_consum is not None and latest_consum is not None and earliest_fuites is not None and latest_fuites is not None:\n",
    "    if earliest_weather is not None and latest_weather is not None:\n",
    "        print(f\"\\nWeather data range: {earliest_weather.strftime('%Y-%m-%d')} to {latest_weather.strftime('%Y-%m-%d')} ({date_range_days_weather} days)\")\n",
    "    else:\n",
    "        print(f\"\\nWeather data range: Unable to compute (df_weather not available)\")\n",
    "    print(f\"Consum data range: {earliest_consum.strftime('%Y-%m-%d')} to {latest_consum.strftime('%Y-%m-%d')} ({date_range_days_consum} days)\")\n",
    "    print(f\"Fuites data range: {earliest_fuites.strftime('%Y-%m-%d')} to {latest_fuites.strftime('%Y-%m-%d')} ({date_range_days_fuites} days)\")\n",
    "    \n",
    "    # Calculate overlaps between all three datasets\n",
    "    if earliest_weather is not None and latest_weather is not None:\n",
    "        # Overlap between consum and fuites\n",
    "        overlap_consum_fuites_start = max(earliest_consum, earliest_fuites)\n",
    "        overlap_consum_fuites_end = min(latest_consum, latest_fuites)\n",
    "        if overlap_consum_fuites_start <= overlap_consum_fuites_end:\n",
    "            overlap_consum_fuites_days = (overlap_consum_fuites_end - overlap_consum_fuites_start).days\n",
    "            print(f\"\\nOverlap between consum and fuites:\")\n",
    "            print(f\"  From: {overlap_consum_fuites_start.strftime('%Y-%m-%d')}\")\n",
    "            print(f\"  To: {overlap_consum_fuites_end.strftime('%Y-%m-%d')}\")\n",
    "            print(f\"  Days: {overlap_consum_fuites_days:,}\")\n",
    "        \n",
    "        # Overlap between consum and weather\n",
    "        overlap_consum_weather_start = max(earliest_consum, earliest_weather)\n",
    "        overlap_consum_weather_end = min(latest_consum, latest_weather)\n",
    "        if overlap_consum_weather_start <= overlap_consum_weather_end:\n",
    "            overlap_consum_weather_days = (overlap_consum_weather_end - overlap_consum_weather_start).days\n",
    "            print(f\"\\nOverlap between consum and weather:\")\n",
    "            print(f\"  From: {overlap_consum_weather_start.strftime('%Y-%m-%d')}\")\n",
    "            print(f\"  To: {overlap_consum_weather_end.strftime('%Y-%m-%d')}\")\n",
    "            print(f\"  Days: {overlap_consum_weather_days:,}\")\n",
    "        \n",
    "        # Overlap between fuites and weather\n",
    "        overlap_fuites_weather_start = max(earliest_fuites, earliest_weather)\n",
    "        overlap_fuites_weather_end = min(latest_fuites, latest_weather)\n",
    "        if overlap_fuites_weather_start <= overlap_fuites_weather_end:\n",
    "            overlap_fuites_weather_days = (overlap_fuites_weather_end - overlap_fuites_weather_start).days\n",
    "            print(f\"\\nOverlap between fuites and weather:\")\n",
    "            print(f\"  From: {overlap_fuites_weather_start.strftime('%Y-%m-%d')}\")\n",
    "            print(f\"  To: {overlap_fuites_weather_end.strftime('%Y-%m-%d')}\")\n",
    "            print(f\"  Days: {overlap_fuites_weather_days:,}\")\n",
    "        \n",
    "        # Overlap between all three (consum, fuites, and weather)\n",
    "        overlap_all_start = max(earliest_consum, earliest_fuites, earliest_weather)\n",
    "        overlap_all_end = min(latest_consum, latest_fuites, latest_weather)\n",
    "        if overlap_all_start <= overlap_all_end:\n",
    "            overlap_all_days = (overlap_all_end - overlap_all_start).days\n",
    "            print(f\"\\nOverlap between consum, fuites, and weather:\")\n",
    "            print(f\"  From: {overlap_all_start.strftime('%Y-%m-%d')}\")\n",
    "            print(f\"  To: {overlap_all_end.strftime('%Y-%m-%d')}\")\n",
    "            print(f\"  Days: {overlap_all_days:,}\")\n",
    "    else:\n",
    "        # Only calculate overlap between consum and fuites if weather is not available\n",
    "        overlap_start = max(earliest_consum, earliest_fuites)\n",
    "        overlap_end = min(latest_consum, latest_fuites)\n",
    "        if overlap_start <= overlap_end:\n",
    "            overlap_days = (overlap_end - overlap_start).days\n",
    "            print(f\"\\nOverlap between consum and fuites:\")\n",
    "            print(f\"  From: {overlap_start.strftime('%Y-%m-%d')}\")\n",
    "            print(f\"  To: {overlap_end.strftime('%Y-%m-%d')}\")\n",
    "            print(f\"  Days: {overlap_days:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dfa8e5",
   "metadata": {},
   "source": [
    "### 4.3 Spatial Coverage Analysis\n",
    "\n",
    "Analyze the spatial coverage (number of unique census sections) across all datasets (except weather) to understand data availability and assess the feasibility of merging datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ad6a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial Coverage Analysis\n",
    "# Analyze unique census sections across all datasets (except weather)\n",
    "\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Load socioeconomic data\n",
    "socio_file = Path(\"clean/socio_clean.parquet\")\n",
    "if socio_file.exists():\n",
    "    df_socio = pd.read_parquet(socio_file)\n",
    "    sections_socio = set(df_socio['SECCIO_CENSAL'].unique())\n",
    "else:\n",
    "    sections_socio = set()\n",
    "\n",
    "# Load consumption data (from split files)\n",
    "consum_dir = Path(\"clean/split_consum_bcn\")\n",
    "consum_files = sorted(glob.glob(str(consum_dir / \"consum_clean_bcn_part_*.parquet\")))\n",
    "sections_consum = set()\n",
    "if consum_files:\n",
    "    for file_path in consum_files:\n",
    "        df_chunk = pd.read_parquet(file_path)\n",
    "        sections_consum.update(df_chunk['SECCIO_CENSAL'].unique())\n",
    "\n",
    "# Load leak incidents data (from single file)\n",
    "fuites_file = Path(\"clean/fuites_clean_bcn.parquet\")\n",
    "sections_fuites = set()\n",
    "if fuites_file.exists():\n",
    "    df_fuites_temp = pd.read_parquet(fuites_file)\n",
    "    sections_fuites.update(df_fuites_temp['SECCIO_CENSAL'].unique())\n",
    "\n",
    "# Number of census sections in each dataset\n",
    "print(\"Number of census sections per dataset:\")\n",
    "print(f\"  Socioeconomic: {len(sections_socio):,}\")\n",
    "print(f\"  Consumption: {len(sections_consum):,}\")\n",
    "print(f\"  Leaks: {len(sections_fuites):,}\")\n",
    "\n",
    "# Overlap analysis\n",
    "if sections_socio and sections_consum and sections_fuites:\n",
    "    total_sections = len(sections_socio)\n",
    "    \n",
    "    # Calculate overlaps\n",
    "    sections_all_three = sections_socio & sections_consum & sections_fuites\n",
    "    sections_socio_consum_only = (sections_socio & sections_consum) - sections_fuites\n",
    "    sections_socio_fuites_only = (sections_socio & sections_fuites) - sections_consum\n",
    "    sections_consum_fuites_only = (sections_consum & sections_fuites) - sections_socio\n",
    "    sections_socio_only = sections_socio - sections_consum - sections_fuites\n",
    "    sections_consum_only = sections_consum - sections_socio - sections_fuites\n",
    "    sections_fuites_only = sections_fuites - sections_socio - sections_consum\n",
    "    sections_all = sections_socio | sections_consum | sections_fuites\n",
    "    \n",
    "    # Calculate percentages\n",
    "    pct_consum = (len(sections_consum) / total_sections * 100) if total_sections > 0 else 0\n",
    "    pct_fuites = (len(sections_fuites) / total_sections * 100) if total_sections > 0 else 0\n",
    "    pct_all_three = (len(sections_all_three) / total_sections * 100) if total_sections > 0 else 0\n",
    "    pct_socio_consum = (len(sections_socio_consum_only) / total_sections * 100) if total_sections > 0 else 0\n",
    "    pct_socio_fuites = (len(sections_socio_fuites_only) / total_sections * 100) if total_sections > 0 else 0\n",
    "    pct_consum_fuites = (len(sections_consum_fuites_only) / total_sections * 100) if total_sections > 0 else 0\n",
    "    pct_socio_only = (len(sections_socio_only) / total_sections * 100) if total_sections > 0 else 0\n",
    "    pct_consum_only = (len(sections_consum_only) / total_sections * 100) if total_sections > 0 else 0\n",
    "    pct_fuites_only = (len(sections_fuites_only) / total_sections * 100) if total_sections > 0 else 0\n",
    "    pct_total = (len(sections_all) / total_sections * 100) if total_sections > 0 else 0\n",
    "    \n",
    "    print(\"\\nOverlap analysis:\")\n",
    "    print(f\"  All three datasets: {len(sections_all_three):,} ({pct_all_three:.1f}%)\")\n",
    "    print(f\"  Socioeconomic + Consumption only: {len(sections_socio_consum_only):,} ({pct_socio_consum:.1f}%)\")\n",
    "    print(f\"  Socioeconomic + Leaks only: {len(sections_socio_fuites_only):,} ({pct_socio_fuites:.1f}%)\")\n",
    "    print(f\"  Consumption + Leaks only: {len(sections_consum_fuites_only):,} ({pct_consum_fuites:.1f}%)\")\n",
    "    print(f\"  Socioeconomic only: {len(sections_socio_only):,} ({pct_socio_only:.1f}%)\")\n",
    "    print(f\"  Consumption only: {len(sections_consum_only):,} ({pct_consum_only:.1f}%)\")\n",
    "    print(f\"  Leaks only: {len(sections_fuites_only):,} ({pct_fuites_only:.1f}%)\")\n",
    "    print(f\"  Total unique sections: {len(sections_all):,} ({pct_total:.1f}%)\")\n",
    "    \n",
    "    # Summary table\n",
    "    coverage_summary = pd.DataFrame({\n",
    "        'Dataset': ['Socioeconomic (Total)', 'Consumption', 'Leaks', 'All Three', 'Total Unique'],\n",
    "        'Unique Sections': [\n",
    "            len(sections_socio),\n",
    "            len(sections_consum),\n",
    "            len(sections_fuites),\n",
    "            len(sections_all_three),\n",
    "            len(sections_all)\n",
    "        ],\n",
    "        'Coverage %': [100.0, pct_consum, pct_fuites, pct_all_three, pct_total]\n",
    "    })\n",
    "    \n",
    "    print(\"\\nSummary:\")\n",
    "    print(coverage_summary.to_string(index=False))\n",
    "    \n",
    "elif sections_socio and sections_consum:\n",
    "    total_sections = len(sections_socio)\n",
    "    sections_both = sections_socio & sections_consum\n",
    "    sections_socio_only = sections_socio - sections_consum\n",
    "    sections_consum_only = sections_consum - sections_socio\n",
    "    \n",
    "    pct_both = (len(sections_both) / total_sections * 100) if total_sections > 0 else 0\n",
    "    pct_socio_only = (len(sections_socio_only) / total_sections * 100) if total_sections > 0 else 0\n",
    "    pct_consum_only = (len(sections_consum_only) / total_sections * 100) if total_sections > 0 else 0\n",
    "    pct_total = (len(sections_socio | sections_consum) / total_sections * 100) if total_sections > 0 else 0\n",
    "    pct_consum = (len(sections_consum) / total_sections * 100) if total_sections > 0 else 0\n",
    "    \n",
    "    print(\"\\nOverlap analysis:\")\n",
    "    print(f\"  Both datasets: {len(sections_both):,} ({pct_both:.1f}%)\")\n",
    "    print(f\"  Socioeconomic only: {len(sections_socio_only):,} ({pct_socio_only:.1f}%)\")\n",
    "    print(f\"  Consumption only: {len(sections_consum_only):,} ({pct_consum_only:.1f}%)\")\n",
    "    print(f\"  Total unique sections: {len(sections_socio | sections_consum):,} ({pct_total:.1f}%)\")\n",
    "    \n",
    "    coverage_summary = pd.DataFrame({\n",
    "        'Dataset': ['Socioeconomic (Total)', 'Consumption', 'Both', 'Total Unique'],\n",
    "        'Unique Sections': [\n",
    "            len(sections_socio),\n",
    "            len(sections_consum),\n",
    "            len(sections_both),\n",
    "            len(sections_socio | sections_consum)\n",
    "        ],\n",
    "        'Coverage %': [100.0, pct_consum, pct_both, pct_total]\n",
    "    })\n",
    "    \n",
    "    print(\"\\nSummary:\")\n",
    "    print(coverage_summary.to_string(index=False))\n",
    "else:\n",
    "    print(\"⚠ Need at least two datasets to perform overlap analysis\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c061ef81",
   "metadata": {},
   "source": [
    "**Why data merging is not recommended:**\n",
    "\n",
    "Based on the spatial coverage analysis above, merging the datasets would result in significant data loss:\n",
    "\n",
    "- **Only 29.8% of census sections** (318 out of 1,068) have data in all three datasets\n",
    "- **41.9% of sections** (447) would have missing consumption data\n",
    "- **59.2% of sections** (632) would have missing leak data\n",
    "- **30.8% of sections** (329) would have missing data in both consumption and leaks\n",
    "\n",
    "If we merge the datasets using socioeconomic data as the base (left join), 70.2% of the rows would have `NaN` values for one or more variables. This would:\n",
    "- Reduce the usable sample size for any analysis requiring all three variables\n",
    "- Potentially introduce bias (sections with complete data may not be representative)\n",
    "- Require imputation or exclusion strategies that could distort results\n",
    "\n",
    "**Recommendation:** Keep datasets separate and join on-demand for specific analyses that can accept the reduced sample size (318 sections with all three variables) or work with incomplete data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59e93f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview cleaned datasets\n",
    "# Display sample rows from each cleaned dataset to verify data structure\n",
    "\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PREVIEW OF CLEANED DATASETS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Consumption data (first split file)\n",
    "print(\"\\n1. CONSUMPTION DATA (first split file):\")\n",
    "print(\"-\" * 80)\n",
    "consum_dir = Path(\"clean/split_consum_bcn\")\n",
    "consum_files = sorted(glob.glob(str(consum_dir / \"consum_clean_bcn_part_*.parquet\")))\n",
    "if consum_files:\n",
    "    df_consum_sample = pd.read_parquet(consum_files[0])\n",
    "    print(f\"File: {Path(consum_files[0]).name}\")\n",
    "    print(f\"Shape: {df_consum_sample.shape}\")\n",
    "    print(f\"Columns: {list(df_consum_sample.columns)}\")\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df_consum_sample.head())\n",
    "else:\n",
    "    print(\"⚠ No consumption split files found\")\n",
    "\n",
    "# 2. Leak incidents data (first split file)\n",
    "print(\"\\n\\n2. LEAK INCIDENTS DATA:\")\n",
    "print(\"-\" * 80)\n",
    "fuites_file = Path(\"clean/fuites_clean_bcn.parquet\")\n",
    "if fuites_file.exists():\n",
    "    df_fuites_sample = pd.read_parquet(fuites_file)\n",
    "    print(f\"File: {fuites_file.name}\")\n",
    "    print(f\"Shape: {df_fuites_sample.shape}\")\n",
    "    print(f\"Columns: {list(df_fuites_sample.columns)}\")\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df_fuites_sample.head())\n",
    "else:\n",
    "    print(\"⚠ Leak incidents file not found\")\n",
    "\n",
    "# 3. Weather data\n",
    "print(\"\\n\\n3. WEATHER DATA:\")\n",
    "print(\"-\" * 80)\n",
    "weather_file = Path(\"clean/weather_clean.parquet\")\n",
    "if weather_file.exists():\n",
    "    df_weather_sample = pd.read_parquet(weather_file)\n",
    "    print(f\"File: {weather_file.name}\")\n",
    "    print(f\"Shape: {df_weather_sample.shape}\")\n",
    "    print(f\"Columns: {list(df_weather_sample.columns)}\")\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df_weather_sample.head())\n",
    "else:\n",
    "    print(\"⚠ Weather cleaned file not found\")\n",
    "\n",
    "# 4. Socioeconomic data\n",
    "print(\"\\n\\n4. SOCIOECONOMIC DATA:\")\n",
    "print(\"-\" * 80)\n",
    "socio_file = Path(\"clean/socio_clean.parquet\")\n",
    "if socio_file.exists():\n",
    "    df_socio_sample = pd.read_parquet(socio_file)\n",
    "    print(f\"File: {socio_file.name}\")\n",
    "    print(f\"Shape: {df_socio_sample.shape}\")\n",
    "    print(f\"Columns: {list(df_socio_sample.columns)}\")\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df_socio_sample.head())\n",
    "else:\n",
    "    print(\"⚠ Socioeconomic cleaned file not found\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab251ca7",
   "metadata": {},
   "source": [
    "## 5. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Perform detailed exploratory data analysis on the cleaned datasets to understand patterns, distributions, and relationships in the data. We work directly with the cleaned data files, performing any necessary aggregations inline for specific visualizations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e9da87",
   "metadata": {},
   "source": [
    "We will work directly with the cleaned data files:\n",
    "- **Consumption data**: Split parquet files in `clean/split_consum_bcn/`\n",
    "- **Leak incidents data**: Single parquet file `clean/fuites_clean_bcn.parquet`\n",
    "- **Weather data**: `clean/weather_clean.parquet`\n",
    "- **Socioeconomic data**: `clean/socio_clean.parquet`\n",
    "\n",
    "Any aggregations needed for specific visualizations will be performed inline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a38e629",
   "metadata": {},
   "source": [
    "### 5.1 Socioeconomic Analysis\n",
    "\n",
    "Analyze the distribution of IST (Índex Socioeconòmic Territorial) across Barcelona census sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5ba1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load socioeconomic data if not already loaded\n",
    "if 'df_socio_bcn' not in globals():\n",
    "    df_socio_bcn = pd.read_parquet(\"clean/socio_clean.parquet\")\n",
    "\n",
    "# Ensure 'valor' column is numeric\n",
    "if df_socio_bcn['valor'].dtype == object:\n",
    "    df_socio_bcn['valor'] = pd.to_numeric(df_socio_bcn['valor'], errors='coerce')\n",
    "\n",
    "print(\"Available columns in socioeconomic data:\")\n",
    "print(df_socio_bcn.columns.tolist())\n",
    "print(f\"\\nData shape: {df_socio_bcn.shape}\")\n",
    "print(f\"\\nSample data:\")\n",
    "print(df_socio_bcn.head())\n",
    "\n",
    "print(\"\\nDescriptive statistics for socio-economic index (valor):\")\n",
    "print(df_socio_bcn['valor'].describe())\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(df_socio_bcn['valor'], bins=30, color='teal', kde=True)\n",
    "plt.title(\"Distribution of Socioeconomic Index\")\n",
    "plt.xlabel(\"Index value\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Box plot for better understanding of distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(y=df_socio_bcn['valor'], color='teal')\n",
    "plt.title(\"Box Plot of Socioeconomic Index\")\n",
    "plt.ylabel(\"Index value\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Top/bottom census sections by socio-economic index\n",
    "top_sections = df_socio_bcn.sort_values('valor', ascending=False).head(10)\n",
    "bottom_sections = df_socio_bcn.sort_values('valor').head(10)\n",
    "\n",
    "print(\"\\nTop 10 census sections by socio-economic index:\")\n",
    "print(top_sections[['any', 'SECCIO_CENSAL', 'valor']])\n",
    "\n",
    "print(\"\\nBottom 10 census sections by socio-economic index:\")\n",
    "print(bottom_sections[['any', 'SECCIO_CENSAL', 'valor']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9131f44",
   "metadata": {},
   "source": [
    "### 5.2 Consumption Data Analysis\n",
    "\n",
    "Analyze the distribution of daily water consumption across census sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5ba741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and aggregate consumption data from cleaned split files\n",
    "# Aggregate by SECCIO_CENSAL and FECHA (sum CONSUMO_REAL per section per day)\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"CONSUMPTION DATA ANALYSIS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "consum_dir = Path(\"clean/split_consum_bcn\")\n",
    "consum_files = sorted(glob.glob(str(consum_dir / \"consum_clean_bcn_part_*.parquet\")))\n",
    "\n",
    "print(f\"Found {len(consum_files)} cleaned consumption files\")\n",
    "\n",
    "# Process all files and aggregate\n",
    "consum_agg_list = []\n",
    "total_rows = 0\n",
    "\n",
    "for i, file_path in enumerate(consum_files, 1):\n",
    "    if i % 5 == 0 or i == len(consum_files):\n",
    "        print(f\"Processing file {i}/{len(consum_files)}: {Path(file_path).name}\")\n",
    "    \n",
    "    df_chunk = pd.read_parquet(file_path)\n",
    "    df_chunk['FECHA'] = pd.to_datetime(df_chunk['FECHA'], errors='coerce')\n",
    "    total_rows += len(df_chunk)\n",
    "    \n",
    "    # Aggregate: sum CONSUMO_REAL by SECCIO_CENSAL and FECHA\n",
    "    df_chunk_agg = (\n",
    "        df_chunk\n",
    "        .groupby(['SECCIO_CENSAL', 'FECHA'])\n",
    "        .agg({'CONSUMO_REAL': 'sum'})\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    consum_agg_list.append(df_chunk_agg)\n",
    "\n",
    "# Combine all aggregated chunks\n",
    "df_consum_agg = pd.concat(consum_agg_list, ignore_index=True)\n",
    "\n",
    "# Final aggregation (in case same SECCIO_CENSAL + FECHA appears in multiple files)\n",
    "df_consum_agg = (\n",
    "    df_consum_agg\n",
    "    .groupby(['SECCIO_CENSAL', 'FECHA'])\n",
    "    .agg({'CONSUMO_REAL': 'sum'})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Processed {total_rows:,} rows from {len(consum_files)} files\")\n",
    "print(f\"✓ Aggregated to {len(df_consum_agg):,} section-date combinations\")\n",
    "print(f\"Date range: {df_consum_agg['FECHA'].min()} to {df_consum_agg['FECHA'].max()}\")\n",
    "print(f\"Unique census sections: {df_consum_agg['SECCIO_CENSAL'].nunique():,}\")\n",
    "print(f\"Unique dates: {df_consum_agg['FECHA'].nunique():,}\")\n",
    "\n",
    "print(f\"\\nConsumption statistics (daily per census section):\")\n",
    "print(df_consum_agg['CONSUMO_REAL'].describe())\n",
    "\n",
    "# 1. Distribution plot: Histogram of average consumption per census section\n",
    "avg_consumption_per_section = (\n",
    "    df_consum_agg\n",
    "    .groupby('SECCIO_CENSAL')['CONSUMO_REAL']\n",
    "    .mean()\n",
    "    .reset_index(name='avg_consumption')\n",
    ")\n",
    "\n",
    "# Get data range for axis limits\n",
    "data_min = avg_consumption_per_section['avg_consumption'].min()\n",
    "data_max = avg_consumption_per_section['avg_consumption'].max()\n",
    "data_range = data_max - data_min\n",
    "# Add 5% padding on each side\n",
    "x_min = data_min - (data_range * 0.05)\n",
    "x_max = data_max + (data_range * 0.05)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(avg_consumption_per_section['avg_consumption'], bins=50, color='skyblue', kde=True)\n",
    "plt.title(\"Distribution of Average Daily Water Consumption per Census Section\")\n",
    "plt.xlabel(\"Average Daily Consumption per Census Section (liters/day)\")\n",
    "plt.ylabel(\"Number of Census Sections\")\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.locator_params(axis='x', nbins=15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAverage consumption per section statistics:\")\n",
    "print(avg_consumption_per_section['avg_consumption'].describe())\n",
    "\n",
    "# 2. Box plot: Summary statistics\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(y=df_consum_agg['CONSUMO_REAL'], color='lightblue')\n",
    "plt.yscale('log')\n",
    "plt.title(\"Box Plot: Summary Statistics of Daily Water Consumption per Census Section\")\n",
    "plt.ylabel(\"Daily Consumption (liters/day)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Consumption by month\n",
    "df_consum_agg['month'] = df_consum_agg['FECHA'].dt.month\n",
    "df_consum_agg['month_name'] = df_consum_agg['FECHA'].dt.strftime('%B')\n",
    "\n",
    "consumption_by_month = (\n",
    "    df_consum_agg\n",
    "    .groupby('month')['CONSUMO_REAL']\n",
    "    .agg(['sum', 'mean', 'count'])\n",
    "    .reset_index()\n",
    ")\n",
    "consumption_by_month['month_name'] = pd.to_datetime(consumption_by_month['month'], format='%m').dt.strftime('%B')\n",
    "consumption_by_month = consumption_by_month.sort_values('month')\n",
    "\n",
    "print(\"\\nTotal consumption by month:\")\n",
    "print(consumption_by_month[['month_name', 'sum', 'mean', 'count']])\n",
    "\n",
    "# Plot total consumption by month\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=consumption_by_month, x='month_name', y='sum', color='skyblue', order=consumption_by_month['month_name'])\n",
    "plt.title(\"Total Water Consumption by Month\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Total Consumption (liters)\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Consumption over time (daily aggregate)\n",
    "consumption_daily = (\n",
    "    df_consum_agg\n",
    "    .groupby('FECHA')['CONSUMO_REAL']\n",
    "    .agg(['sum', 'mean'])\n",
    "    .reset_index()\n",
    "    .sort_values('FECHA')\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(consumption_daily['FECHA'], consumption_daily['sum'], color='steelblue', linewidth=1, alpha=0.7)\n",
    "plt.title(\"Total Daily Water Consumption Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Total Consumption (liters/day)\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Average consumption per census section over time (top 10 sections by total consumption)\n",
    "section_totals = (\n",
    "    df_consum_agg\n",
    "    .groupby('SECCIO_CENSAL')['CONSUMO_REAL']\n",
    "    .sum()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "top_sections = section_totals.index.tolist()\n",
    "df_top_sections = df_consum_agg[df_consum_agg['SECCIO_CENSAL'].isin(top_sections)]\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "for section in top_sections:\n",
    "    section_data = df_top_sections[df_top_sections['SECCIO_CENSAL'] == section].sort_values('FECHA')\n",
    "    plt.plot(section_data['FECHA'], section_data['CONSUMO_REAL'], label=section, linewidth=1, alpha=0.7)\n",
    "\n",
    "plt.title(\"Daily Water Consumption Over Time (Top 10 Census Sections by Total Consumption)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Daily Consumption (liters/day)\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6290db0",
   "metadata": {},
   "source": [
    "### 5.3 Leak Incidents Analysis\n",
    "\n",
    "Analyze temporal patterns in leak incidents across Barcelona. Since the cleaned data has one leak per row (one leak per POLISSA_SUBM per day), we can analyze the data directly without additional aggregation steps. The analysis includes:\n",
    "\n",
    "- Distribution of leaks per census section-date\n",
    "- Temporal trends (daily, monthly)\n",
    "- Analysis by use type (US_AIGUA_SUBM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65c578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load leak incidents data from cleaned single parquet file\n",
    "# Since we have one leak per row (one leak per POLISSA_SUBM per day), we can work directly with the data\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LEAK INCIDENTS ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load single cleaned parquet file\n",
    "fuites_file = Path(\"clean/fuites_clean_bcn.parquet\")\n",
    "\n",
    "if fuites_file.exists():\n",
    "    print(f\"\\nLoading cleaned leak data from: {fuites_file}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    df_fuites = pd.read_parquet(fuites_file)\n",
    "    # CREATED_MENSAJE is already a date (from cleaning step), convert to datetime for analysis\n",
    "    df_fuites['FECHA'] = pd.to_datetime(df_fuites['CREATED_MENSAJE'])\n",
    "else:\n",
    "    print(f\"\\n⚠ Error: File not found: {fuites_file}\")\n",
    "    print(\"Please run the cleaning step first (Section 3.2)\")\n",
    "    df_fuites = pd.DataFrame()\n",
    "\n",
    "if len(df_fuites) > 0:\n",
    "    print(f\"\\n✓ Loaded {len(df_fuites):,} leaks from single file\")\n",
    "    print(f\"Date range: {df_fuites['FECHA'].min()} to {df_fuites['FECHA'].max()}\")\n",
    "    print(f\"Unique census sections: {df_fuites['SECCIO_CENSAL'].nunique():,}\")\n",
    "    print(f\"Unique dates: {df_fuites['FECHA'].nunique():,}\")\n",
    "    print(f\"Unique POLISSA_SUBM: {df_fuites['POLISSA_SUBM'].nunique():,}\")\n",
    "else:\n",
    "    print(f\"\\n⚠ No data loaded - cannot continue analysis\")\n",
    "\n",
    "# Aggregate by section-date for analysis\n",
    "df_fuites_agg = (\n",
    "    df_fuites\n",
    "    .groupby(['SECCIO_CENSAL', 'FECHA'])\n",
    "    .size()\n",
    "    .reset_index(name='num_fuites')\n",
    ")\n",
    "\n",
    "print(f\"\\nLeak statistics (number of leaks per section-date):\")\n",
    "print(df_fuites_agg['num_fuites'].describe())\n",
    "\n",
    "# 1. Distribution of leaks per POLISSA_SUBM (how many leaks does each policy have?)\n",
    "leaks_per_polissa = (\n",
    "    df_fuites\n",
    "    .groupby('POLISSA_SUBM')\n",
    "    .size()\n",
    "    .reset_index(name='num_leaks')\n",
    "    .sort_values('num_leaks', ascending=False)\n",
    ")\n",
    "\n",
    "print(f\"\\nLeak statistics per POLISSA_SUBM:\")\n",
    "print(leaks_per_polissa['num_leaks'].describe())\n",
    "\n",
    "# Count how many policies have each number of leaks\n",
    "leak_counts_polissa = leaks_per_polissa['num_leaks'].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "leak_counts_polissa.plot(kind='bar', color='coral', width=0.8)\n",
    "plt.title(\"Distribution of Leak Frequency per Policy (POLISSA_SUBM)\")\n",
    "plt.xlabel(\"Number of Leaks per Policy\")\n",
    "plt.ylabel(\"Number of Policies\")\n",
    "plt.xticks(rotation=0, ha='center')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Distribution of leaks per SECCIO_CENSAL (how many leaks per census section?)\n",
    "leaks_per_section = (\n",
    "    df_fuites\n",
    "    .groupby('SECCIO_CENSAL')\n",
    "    .size()\n",
    "    .reset_index(name='num_leaks')\n",
    "    .sort_values('num_leaks', ascending=False)\n",
    ")\n",
    "\n",
    "print(f\"\\nLeak statistics per SECCIO_CENSAL:\")\n",
    "print(leaks_per_section['num_leaks'].describe())\n",
    "\n",
    "# Count how many sections have each number of leaks\n",
    "leak_counts_section = leaks_per_section['num_leaks'].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "leak_counts_section.plot(kind='bar', color='coral', width=0.8)\n",
    "plt.title(\"Distribution of Leak Frequency per Census Section (SECCIO_CENSAL)\")\n",
    "plt.xlabel(\"Number of Leaks per Census Section\")\n",
    "plt.ylabel(\"Number of Census Sections\")\n",
    "plt.xticks(rotation=0, ha='center')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional: Top sections and policies by leak count\n",
    "print(f\"\\nTop 10 census sections by number of leaks:\")\n",
    "print(leaks_per_section.head(10))\n",
    "\n",
    "print(f\"\\nTop 10 policies by number of leaks:\")\n",
    "print(leaks_per_polissa.head(10))\n",
    "\n",
    "# 3. Leaks over time (daily aggregate)\n",
    "leaks_daily = (\n",
    "    df_fuites\n",
    "    .groupby('FECHA')\n",
    "    .size()\n",
    "    .reset_index(name='num_fuites')\n",
    "    .sort_values('FECHA')\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(leaks_daily['FECHA'], leaks_daily['num_fuites'], color='teal', linewidth=1, alpha=0.7)\n",
    "plt.title(\"Total Daily Leaks in Barcelona Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Total Number of Leaks per Day\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Leaks by month (only full years: 2023 and 2024)\n",
    "# Filter to only include full years to avoid partial year bias\n",
    "df_fuites['year'] = df_fuites['FECHA'].dt.year\n",
    "df_fuites_full_years = df_fuites[df_fuites['year'].isin([2023, 2024])].copy()\n",
    "\n",
    "print(f\"\\nFiltering to full years only (2023 and 2024):\")\n",
    "print(f\"  Total leaks in dataset: {len(df_fuites):,}\")\n",
    "print(f\"  Leaks in 2023-2024: {len(df_fuites_full_years):,}\")\n",
    "print(f\"  Leaks excluded (partial years): {len(df_fuites) - len(df_fuites_full_years):,}\")\n",
    "\n",
    "df_fuites_full_years['month'] = df_fuites_full_years['FECHA'].dt.month\n",
    "df_fuites_full_years['month_name'] = df_fuites_full_years['FECHA'].dt.strftime('%B')\n",
    "\n",
    "leaks_by_month = (\n",
    "    df_fuites_full_years\n",
    "    .groupby('month')\n",
    "    .size()\n",
    "    .reset_index(name='total_leaks')\n",
    ")\n",
    "leaks_by_month['month_name'] = pd.to_datetime(leaks_by_month['month'], format='%m').dt.strftime('%B')\n",
    "leaks_by_month = leaks_by_month.sort_values('month')\n",
    "\n",
    "# Also calculate mean leaks per section-date by month (using full years only)\n",
    "df_fuites_agg_full_years = df_fuites_agg[df_fuites_agg['FECHA'].dt.year.isin([2023, 2024])].copy()\n",
    "leaks_by_month_agg = (\n",
    "    df_fuites_agg_full_years\n",
    "    .assign(month=df_fuites_agg_full_years['FECHA'].dt.month)\n",
    "    .groupby('month')['num_fuites']\n",
    "    .agg(['mean', 'count'])\n",
    "    .reset_index()\n",
    ")\n",
    "leaks_by_month = leaks_by_month.merge(leaks_by_month_agg, on='month', how='left')\n",
    "leaks_by_month = leaks_by_month.sort_values('month')\n",
    "\n",
    "print(\"\\nTotal leaks by month (2023-2024 only, full years):\")\n",
    "print(leaks_by_month[['month_name', 'total_leaks', 'mean', 'count']])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=leaks_by_month, x='month_name', y='total_leaks', color='coral', order=leaks_by_month['month_name'])\n",
    "plt.title(\"Total Leaks by Month (2023-2024, Full Years Only)\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Total Number of Leaks\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Leaks by use type (US_AIGUA_SUBM)\n",
    "if 'US_AIGUA_SUBM' in df_fuites.columns:\n",
    "    leaks_by_use = (\n",
    "        df_fuites\n",
    "        .groupby('US_AIGUA_SUBM')\n",
    "        .size()\n",
    "        .reset_index(name='num_leaks')\n",
    "        .sort_values('num_leaks', ascending=False)\n",
    "    )\n",
    "    \n",
    "    print(\"\\nLeaks by use type:\")\n",
    "    print(leaks_by_use)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=leaks_by_use, x='US_AIGUA_SUBM', y='num_leaks', color='coral')\n",
    "    plt.title(\"Total Leaks by Use Type\")\n",
    "    plt.xlabel(\"Use Type (US_AIGUA_SUBM)\")\n",
    "    plt.ylabel(\"Total Number of Leaks\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Leaks by use type over time\n",
    "    leaks_by_use_time = (\n",
    "        df_fuites\n",
    "        .groupby(['FECHA', 'US_AIGUA_SUBM'])\n",
    "        .size()\n",
    "        .reset_index(name='num_leaks')\n",
    "        .sort_values('FECHA')\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(14, 6))\n",
    "    for use_type in leaks_by_use_time['US_AIGUA_SUBM'].unique():\n",
    "        use_data = leaks_by_use_time[leaks_by_use_time['US_AIGUA_SUBM'] == use_type]\n",
    "        plt.plot(use_data['FECHA'], use_data['num_leaks'], label=use_type, linewidth=1, alpha=0.7)\n",
    "    plt.title(\"Daily Leaks by Use Type Over Time\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Number of Leaks per Day\")\n",
    "    plt.legend(title='Use Type')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\n⚠ US_AIGUA_SUBM column not found in data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4581980",
   "metadata": {},
   "source": [
    "### 5.4 Consumption Patterns: Weekday vs. Weekend by Type of Use\n",
    "\n",
    "Analyze differences in consumption patterns between weekdays and weekends, broken down by water use type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf6358b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this analysis, we need the detailed consumption data (not aggregated)\n",
    "# Load all files to analyze weekday/weekend patterns\n",
    "print(\"Loading all consumption data for weekday/weekend analysis...\")\n",
    "\n",
    "consum_dir = Path(\"clean/split_consum_bcn\")\n",
    "consum_files = sorted(glob.glob(str(consum_dir / \"consum_clean_bcn_part_*.parquet\")))\n",
    "\n",
    "print(f\"Found {len(consum_files)} consumption files to load\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Load all files\n",
    "consum_list = []\n",
    "for i, file_path in enumerate(consum_files, 1):\n",
    "    print(f\"Loading file {i}/{len(consum_files)}: {Path(file_path).name}\")\n",
    "    df_chunk = pd.read_parquet(file_path)\n",
    "    consum_list.append(df_chunk)\n",
    "\n",
    "# Combine all data\n",
    "df_consum_sample = pd.concat(consum_list, ignore_index=True)\n",
    "\n",
    "print(f\"\\n✓ All data loaded: {len(df_consum_sample):,} rows\")\n",
    "print(f\"Date range: {df_consum_sample['FECHA'].min()} to {df_consum_sample['FECHA'].max()}\")\n",
    "\n",
    "# Ensure date is parsed\n",
    "df_consum_sample['FECHA'] = pd.to_datetime(df_consum_sample['FECHA'], errors='coerce')\n",
    "\n",
    "# Extract weekday info\n",
    "df_consum_sample['weekday'] = df_consum_sample['FECHA'].dt.day_name()\n",
    "df_consum_sample['is_weekend'] = df_consum_sample['FECHA'].dt.dayofweek >= 5  # Saturday=5, Sunday=6\n",
    "\n",
    "# Check if US_AIGUA_GEST column exists\n",
    "if 'US_AIGUA_GEST' in df_consum_sample.columns:\n",
    "    # Plot: Consumption by weekend and by type of use\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(\n",
    "        data=df_consum_sample,\n",
    "        x='is_weekend',\n",
    "        y='CONSUMO_REAL',\n",
    "        hue='US_AIGUA_GEST',\n",
    "        showfliers=False,\n",
    "        palette='Set2'\n",
    "    )\n",
    "    plt.yscale('log')  # Because consumption values can vary a lot\n",
    "    plt.title(\"Water Consumption: Weekday vs Weekend by Type of Use\")\n",
    "    plt.xlabel(\"Weekend (True = Saturday/Sunday)\")\n",
    "    plt.ylabel(\"Consumption (liters/day)\")\n",
    "    plt.legend(title=\"Use Type\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nSummary statistics by weekend and use type:\")\n",
    "    summary = df_consum_sample.groupby(['is_weekend', 'US_AIGUA_GEST'])['CONSUMO_REAL'].describe()\n",
    "    print(summary)\n",
    "else:\n",
    "    print(\"⚠ Column 'US_AIGUA_GEST' not found in consumption data\")\n",
    "    print(\"Available columns:\", df_consum_sample.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2c814d",
   "metadata": {},
   "source": [
    "### 5.5 Weather Data Analysis\n",
    "\n",
    "Analyze weather patterns and their relationship with water consumption and leaks. We manually cleaned the column NOM_VARIABLE of the dataset to remove corrupted characters due to accents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b76c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned weather data\n",
    "df_weather_clean = pd.read_parquet(\"clean/weather_clean.parquet\")\n",
    "\n",
    "# Convert DATA_LECTURA to datetime\n",
    "df_weather_clean['DATA_LECTURA'] = pd.to_datetime(df_weather_clean['DATA_LECTURA'], format=\"%d/%m/%Y\", errors='coerce')\n",
    "\n",
    "# Identify rows with incorrect NOM_VARIABLE values\n",
    "print(\"=\" * 80)\n",
    "print(\"ROWS WITH INCORRECT NOM_VARIABLE VALUES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# List of correct NOM_VARIABLE values\n",
    "correct_vars = [\n",
    "    'Temperatura mitjana diària',\n",
    "    'Temperatura màxima diària + hora',\n",
    "    'Temperatura mínima diària + hora',\n",
    "    'Temperatura mitjana diària clàssica',\n",
    "    'Amplitud tèrmica diària',\n",
    "    'Humitat relativa mitjana diària',\n",
    "    'Humitat relativa màxima diària + data',\n",
    "    'Humitat relativa mínima diària + data',\n",
    "    'Pressió atmosfèrica mitjana diària',\n",
    "    'Pressió atmosfèrica màxima diària + hora',\n",
    "    'Pressió atmosfèrica mínima diària + hora',\n",
    "    'Precipitació acumulada diària',\n",
    "    'Precipitació acumulada diària (8-8 h)',\n",
    "    'Precipitació màxima en 1 min (diària) + hora',\n",
    "    'Precipitació màxima en 1 h (diària) + hora',\n",
    "    'Precipitació màxima en 30 min (diària)+ hora',\n",
    "    'Irradiació solar global diària',\n",
    "    'Velocitat mitjana diària del vent 10 m (esc.)',\n",
    "    'Direcció mitjana diària del vent 10 m (m. 1)',\n",
    "    'Ratxa màxima diària del vent 10 m + hora',\n",
    "    'Direcció de la ratxa màx. diària de vent 10 m',\n",
    "    'Evapotranspiració de referència',\n",
    "    'Velocitat mitjana diària del vent 10 m (vec.)',\n",
    "    'Direcció mitjana diària del vent 10 m (m. u)'\n",
    "]\n",
    "\n",
    "# Filter rows where NOM_VARIABLE is NOT in the correct list\n",
    "if 'NOM_VARIABLE' in df_weather_clean.columns:\n",
    "    incorrect_mask = ~df_weather_clean['NOM_VARIABLE'].isin(correct_vars)\n",
    "    df_incorrect = df_weather_clean[incorrect_mask].copy()\n",
    "    \n",
    "    print(f\"\\nTotal rows with incorrect NOM_VARIABLE: {len(df_incorrect):,}\")\n",
    "    print(f\"Total rows in dataset: {len(df_weather_clean):,}\")\n",
    "    print(f\"Percentage incorrect: {len(df_incorrect)/len(df_weather_clean)*100:.4f}%\")\n",
    "    \n",
    "    if len(df_incorrect) > 0:\n",
    "        print(f\"\\nIncorrect variable names found:\")\n",
    "        incorrect_counts = df_incorrect['NOM_VARIABLE'].value_counts().sort_index()\n",
    "        for var, count in incorrect_counts.items():\n",
    "            print(f\"  - '{var}': {count} records\")\n",
    "        \n",
    "        print(f\"\\nAll rows with incorrect NOM_VARIABLE:\")\n",
    "        print(df_incorrect.to_string())\n",
    "        \n",
    "        print(f\"\\nSummary by incorrect variable:\")\n",
    "        summary = df_incorrect.groupby('NOM_VARIABLE').size().reset_index(name='count').sort_values('NOM_VARIABLE')\n",
    "        print(summary.to_string(index=False))\n",
    "    else:\n",
    "        print(\"\\n✓ All rows have correct NOM_VARIABLE values\")\n",
    "else:\n",
    "    print(\"⚠ Column 'NOM_VARIABLE' not found in weather data\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Ensure numeric 'VALOR_NUM' column is present\n",
    "if 'VALOR_NUM' not in df_weather_clean.columns and 'VALOR' in df_weather_clean.columns:\n",
    "    df_weather_clean['VALOR_NUM'] = (\n",
    "        df_weather_clean['VALOR']\n",
    "        .astype(str)\n",
    "        .str.replace('.', '', regex=False)\n",
    "        .str.replace(',', '.', regex=False)\n",
    "    )\n",
    "    df_weather_clean['VALOR_NUM'] = pd.to_numeric(df_weather_clean['VALOR_NUM'], errors='coerce')\n",
    "\n",
    "print(f\"Weather data shape: {df_weather_clean.shape}\")\n",
    "print(f\"Date range: {df_weather_clean['DATA_LECTURA'].min()} to {df_weather_clean['DATA_LECTURA'].max()}\")\n",
    "print(f\"Available columns: {df_weather_clean.columns.tolist()}\")\n",
    "\n",
    "# Focus on key temperature variables\n",
    "variable_map = {\n",
    "    'Temperatura màxima diària + hora': 'temp_max',\n",
    "    'Temperatura mínima diària + hora': 'temp_min',\n",
    "    'Temperatura mitjana diària': 'temp_avg'\n",
    "}\n",
    "\n",
    "# Create comprehensive weather analysis with multiple variables\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPREHENSIVE WEATHER DATA ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Pivot all weather variables to wide format for analysis\n",
    "weather_pivot = (\n",
    "    df_weather_clean\n",
    "    .pivot_table(\n",
    "        index='DATA_LECTURA', \n",
    "        columns='NOM_VARIABLE', \n",
    "        values='VALOR_NUM', \n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values('DATA_LECTURA')\n",
    ")\n",
    "\n",
    "print(f\"\\nPivoted weather data (all variables): {weather_pivot.shape}\")\n",
    "print(f\"Variables: {list(weather_pivot.columns[1:])}\")\n",
    "\n",
    "# 1. Temperature, Humidity, and Pressure over time (multi-panel)\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 12), sharex=True)\n",
    "\n",
    "# Temperature\n",
    "temp_vars = ['Temperatura màxima diària + hora', 'Temperatura mínima diària + hora', 'Temperatura mitjana diària']\n",
    "for var in temp_vars:\n",
    "    if var in weather_pivot.columns:\n",
    "        axes[0].plot(weather_pivot['DATA_LECTURA'], weather_pivot[var], label=var, linewidth=0.8, alpha=0.7)\n",
    "axes[0].set_ylabel('Temperature (°C)')\n",
    "axes[0].set_title('Temperature Trends Over Time')\n",
    "axes[0].legend(fontsize=8)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Humidity\n",
    "humidity_vars = ['Humitat relativa màxima diària + data', 'Humitat relativa mínima diària + data', 'Humitat relativa mitjana diària']\n",
    "for var in humidity_vars:\n",
    "    if var in weather_pivot.columns:\n",
    "        axes[1].plot(weather_pivot['DATA_LECTURA'], weather_pivot[var], label=var, linewidth=0.8, alpha=0.7)\n",
    "axes[1].set_ylabel('Humidity (%)')\n",
    "axes[1].set_title('Humidity Trends Over Time')\n",
    "axes[1].legend(fontsize=8)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Pressure\n",
    "pressure_vars = ['Pressió atmosfèrica màxima diària + hora', 'Pressió atmosfèrica mínima diària + hora', 'Pressió atmosfèrica mitjana diària']\n",
    "for var in pressure_vars:\n",
    "    if var in weather_pivot.columns:\n",
    "        axes[2].plot(weather_pivot['DATA_LECTURA'], weather_pivot[var], label=var, linewidth=0.8, alpha=0.7)\n",
    "axes[2].set_ylabel('Pressure (hPa)')\n",
    "axes[2].set_xlabel('Date')\n",
    "axes[2].set_title('Atmospheric Pressure Trends Over Time')\n",
    "axes[2].legend(fontsize=8)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Seasonal patterns - Monthly averages for key variables\n",
    "weather_pivot['year'] = weather_pivot['DATA_LECTURA'].dt.year\n",
    "weather_pivot['month'] = weather_pivot['DATA_LECTURA'].dt.month\n",
    "weather_pivot['month_name'] = weather_pivot['DATA_LECTURA'].dt.strftime('%B')\n",
    "\n",
    "# Monthly averages for temperature\n",
    "monthly_temp = weather_pivot.groupby('month').agg({\n",
    "    'Temperatura màxima diària + hora': 'mean',\n",
    "    'Temperatura mínima diària + hora': 'mean',\n",
    "    'Temperatura mitjana diària': 'mean'\n",
    "}).reset_index()\n",
    "monthly_temp['month_name'] = pd.to_datetime(monthly_temp['month'], format='%m').dt.strftime('%B')\n",
    "monthly_temp = monthly_temp.sort_values('month')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = range(len(monthly_temp))\n",
    "width = 0.25\n",
    "if 'Temperatura màxima diària + hora' in monthly_temp.columns:\n",
    "    ax.bar([i - width for i in x], monthly_temp['Temperatura màxima diària + hora'], width, label='Max', color='tomato', alpha=0.8)\n",
    "if 'Temperatura mitjana diària' in monthly_temp.columns:\n",
    "    ax.bar(x, monthly_temp['Temperatura mitjana diària'], width, label='Average', color='seagreen', alpha=0.8)\n",
    "if 'Temperatura mínima diària + hora' in monthly_temp.columns:\n",
    "    ax.bar([i + width for i in x], monthly_temp['Temperatura mínima diària + hora'], width, label='Min', color='royalblue', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_ylabel('Temperature (°C)')\n",
    "ax.set_title('Average Monthly Temperature Patterns')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(monthly_temp['month_name'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Precipitation analysis\n",
    "if 'Precipitació acumulada diària' in weather_pivot.columns:\n",
    "    precip_data = weather_pivot[['DATA_LECTURA', 'Precipitació acumulada diària']].dropna()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
    "    \n",
    "    # Daily precipitation\n",
    "    axes[0].plot(precip_data['DATA_LECTURA'], precip_data['Precipitació acumulada diària'], \n",
    "                 color='steelblue', linewidth=0.5, alpha=0.6)\n",
    "    axes[0].set_ylabel('Daily Precipitation (mm)')\n",
    "    axes[0].set_title('Daily Precipitation Over Time')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Monthly total precipitation\n",
    "    precip_data['year'] = precip_data['DATA_LECTURA'].dt.year\n",
    "    precip_data['month'] = precip_data['DATA_LECTURA'].dt.month\n",
    "    monthly_precip = precip_data.groupby(['year', 'month'])['Precipitació acumulada diària'].sum().reset_index()\n",
    "    monthly_precip['date'] = pd.to_datetime(monthly_precip[['year', 'month']].assign(day=1))\n",
    "    monthly_precip = monthly_precip.sort_values('date')\n",
    "    \n",
    "    axes[1].bar(monthly_precip['date'], monthly_precip['Precipitació acumulada diària'], \n",
    "                color='steelblue', alpha=0.7, width=20)\n",
    "    axes[1].set_ylabel('Monthly Total Precipitation (mm)')\n",
    "    axes[1].set_xlabel('Date')\n",
    "    axes[1].set_title('Monthly Total Precipitation')\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nPrecipitation statistics:\")\n",
    "    print(precip_data['Precipitació acumulada diària'].describe())\n",
    "\n",
    "# 4. Wind analysis\n",
    "wind_vars = ['Velocitat mitjana diària del vent 10 m (esc.)', 'Ratxa màxima diària del vent 10 m + hora']\n",
    "wind_data = weather_pivot[['DATA_LECTURA'] + [v for v in wind_vars if v in weather_pivot.columns]].dropna()\n",
    "\n",
    "if len(wind_data.columns) > 1:\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
    "    \n",
    "    for i, var in enumerate([v for v in wind_vars if v in weather_pivot.columns]):\n",
    "        axes[i].plot(wind_data['DATA_LECTURA'], wind_data[var], linewidth=0.8, alpha=0.7, color='darkgreen')\n",
    "        axes[i].set_ylabel('Wind Speed (m/s)')\n",
    "        axes[i].set_title(var)\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[-1].set_xlabel('Date')\n",
    "    axes[-1].tick_params(axis='x', rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 5. Distribution of key variables\n",
    "key_vars_for_dist = [\n",
    "    'Temperatura mitjana diària',\n",
    "    'Humitat relativa mitjana diària',\n",
    "    'Pressió atmosfèrica mitjana diària',\n",
    "    'Precipitació acumulada diària'\n",
    "]\n",
    "\n",
    "dist_data = weather_pivot[['DATA_LECTURA'] + [v for v in key_vars_for_dist if v in weather_pivot.columns]].dropna()\n",
    "\n",
    "if len(dist_data.columns) > 1:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, var in enumerate([v for v in key_vars_for_dist if v in weather_pivot.columns]):\n",
    "        if i < len(axes):\n",
    "            dist_data[var].hist(bins=50, ax=axes[i], color='coral', alpha=0.7, edgecolor='black')\n",
    "            axes[i].set_xlabel(var)\n",
    "            axes[i].set_ylabel('Frequency')\n",
    "            axes[i].set_title(f'Distribution of {var}')\n",
    "            axes[i].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(len([v for v in key_vars_for_dist if v in weather_pivot.columns]), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 6. Correlation heatmap between weather variables\n",
    "correlation_vars = [\n",
    "    'Temperatura mitjana diària',\n",
    "    'Temperatura màxima diària + hora',\n",
    "    'Temperatura mínima diària + hora',\n",
    "    'Humitat relativa mitjana diària',\n",
    "    'Pressió atmosfèrica mitjana diària',\n",
    "    'Precipitació acumulada diària',\n",
    "    'Irradiació solar global diària',\n",
    "    'Velocitat mitjana diària del vent 10 m (esc.)'\n",
    "]\n",
    "\n",
    "corr_data = weather_pivot[[v for v in correlation_vars if v in weather_pivot.columns]].dropna()\n",
    "\n",
    "if len(corr_data.columns) > 1:\n",
    "    corr_matrix = corr_data.corr()\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0, \n",
    "                square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "    plt.title('Correlation Matrix of Weather Variables')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 7. Box plots by month for temperature\n",
    "if 'Temperatura mitjana diària' in weather_pivot.columns:\n",
    "    temp_monthly = weather_pivot[['DATA_LECTURA', 'Temperatura mitjana diària', 'month', 'month_name']].dropna()\n",
    "    temp_monthly = temp_monthly.sort_values('month')\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(data=temp_monthly, x='month_name', y='Temperatura mitjana diària', \n",
    "                order=temp_monthly.sort_values('month')['month_name'].unique(), color='coral')\n",
    "    plt.title('Distribution of Average Daily Temperature by Month')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Temperature (°C)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddf2b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adb",
   "language": "python",
   "name": "adb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
