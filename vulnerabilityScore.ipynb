{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f7d08c6",
   "metadata": {},
   "source": [
    "# Vulnerability Score Analysis\n",
    "\n",
    "This notebook calculates **rainfall** and **heatwave** vulnerability scores per census section per day using:\n",
    "\n",
    "1. **Data-driven weather variables** identified in `correlationAnalysis.ipynb`\n",
    "2. **Socioeconomic factors** (IST index)\n",
    "3. **Infrastructure factors** (leak frequency, density, consumption intensity)\n",
    "\n",
    "The weather vulnerability components use recommended variables and weights from correlation analysis, ensuring evidence-based vulnerability assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8e0aa7",
   "metadata": {},
   "source": [
    "## 10. Save Results\n",
    "\n",
    "Save the final dataset with vulnerability scores for use in mapping and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3a53b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5b97ac",
   "metadata": {},
   "source": [
    "## 1. Load Census Sections Data\n",
    "\n",
    "Load the Barcelona census sections from CSV. The file contains polygon geometries in WKT format (WGS84 coordinate system).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0025ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gy/5nns71_n7nx_v_2mkns5_k340000gn/T/ipykernel_857/2562052214.py:9: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  gdf[\"centroid\"] = gdf.geometry.centroid\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>codi_districte</th>\n",
       "      <th>nom_districte</th>\n",
       "      <th>codi_barri</th>\n",
       "      <th>nom_barri</th>\n",
       "      <th>codi_aeb</th>\n",
       "      <th>codi_seccio_censal</th>\n",
       "      <th>geometria_etrs89</th>\n",
       "      <th>geometry</th>\n",
       "      <th>centroid</th>\n",
       "      <th>centroid_lat</th>\n",
       "      <th>centroid_lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01</td>\n",
       "      <td>Ciutat Vella</td>\n",
       "      <td>01</td>\n",
       "      <td>el Raval</td>\n",
       "      <td>001</td>\n",
       "      <td>001</td>\n",
       "      <td>POLYGON ((431076.9025 4581077.3095, 431058.164...</td>\n",
       "      <td>POLYGON ((2.17575 41.37827, 2.17552 41.37865, ...</td>\n",
       "      <td>POINT (2.17722 41.37432)</td>\n",
       "      <td>41.374324</td>\n",
       "      <td>2.177219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01</td>\n",
       "      <td>Ciutat Vella</td>\n",
       "      <td>01</td>\n",
       "      <td>el Raval</td>\n",
       "      <td>001</td>\n",
       "      <td>002</td>\n",
       "      <td>POLYGON ((431023.5455 4581164.3265, 430990.550...</td>\n",
       "      <td>POLYGON ((2.1751 41.37905, 2.1747 41.37951, 2....</td>\n",
       "      <td>POINT (2.17391 41.37793)</td>\n",
       "      <td>41.377927</td>\n",
       "      <td>2.173914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01</td>\n",
       "      <td>Ciutat Vella</td>\n",
       "      <td>01</td>\n",
       "      <td>el Raval</td>\n",
       "      <td>002</td>\n",
       "      <td>003</td>\n",
       "      <td>POLYGON ((430778.3455 4580930.5395, 430766.851...</td>\n",
       "      <td>POLYGON ((2.1722 41.37692, 2.17206 41.37696, 2...</td>\n",
       "      <td>POINT (2.17199 41.37576)</td>\n",
       "      <td>41.375757</td>\n",
       "      <td>2.171985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01</td>\n",
       "      <td>Ciutat Vella</td>\n",
       "      <td>01</td>\n",
       "      <td>el Raval</td>\n",
       "      <td>002</td>\n",
       "      <td>004</td>\n",
       "      <td>POLYGON ((430564.2645 4581104.2995, 430496.863...</td>\n",
       "      <td>POLYGON ((2.16962 41.37847, 2.16882 41.37784, ...</td>\n",
       "      <td>POINT (2.16924 41.37642)</td>\n",
       "      <td>41.376416</td>\n",
       "      <td>2.169238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01</td>\n",
       "      <td>Ciutat Vella</td>\n",
       "      <td>01</td>\n",
       "      <td>el Raval</td>\n",
       "      <td>003</td>\n",
       "      <td>005</td>\n",
       "      <td>POLYGON ((430905.0315 4581350.0725, 430874.963...</td>\n",
       "      <td>POLYGON ((2.17366 41.38071, 2.1733 41.38113, 2...</td>\n",
       "      <td>POINT (2.17277 41.37884)</td>\n",
       "      <td>41.378836</td>\n",
       "      <td>2.172773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  codi_districte nom_districte codi_barri nom_barri codi_aeb  \\\n",
       "0             01  Ciutat Vella         01  el Raval      001   \n",
       "1             01  Ciutat Vella         01  el Raval      001   \n",
       "2             01  Ciutat Vella         01  el Raval      002   \n",
       "3             01  Ciutat Vella         01  el Raval      002   \n",
       "4             01  Ciutat Vella         01  el Raval      003   \n",
       "\n",
       "  codi_seccio_censal                                   geometria_etrs89  \\\n",
       "0                001  POLYGON ((431076.9025 4581077.3095, 431058.164...   \n",
       "1                002  POLYGON ((431023.5455 4581164.3265, 430990.550...   \n",
       "2                003  POLYGON ((430778.3455 4580930.5395, 430766.851...   \n",
       "3                004  POLYGON ((430564.2645 4581104.2995, 430496.863...   \n",
       "4                005  POLYGON ((430905.0315 4581350.0725, 430874.963...   \n",
       "\n",
       "                                            geometry  \\\n",
       "0  POLYGON ((2.17575 41.37827, 2.17552 41.37865, ...   \n",
       "1  POLYGON ((2.1751 41.37905, 2.1747 41.37951, 2....   \n",
       "2  POLYGON ((2.1722 41.37692, 2.17206 41.37696, 2...   \n",
       "3  POLYGON ((2.16962 41.37847, 2.16882 41.37784, ...   \n",
       "4  POLYGON ((2.17366 41.38071, 2.1733 41.38113, 2...   \n",
       "\n",
       "                   centroid  centroid_lat  centroid_lon  \n",
       "0  POINT (2.17722 41.37432)     41.374324      2.177219  \n",
       "1  POINT (2.17391 41.37793)     41.377927      2.173914  \n",
       "2  POINT (2.17199 41.37576)     41.375757      2.171985  \n",
       "3  POINT (2.16924 41.37642)     41.376416      2.169238  \n",
       "4  POINT (2.17277 41.37884)     41.378836      2.172773  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load CSV with WKT geometry\n",
    "df = gpd.read_file(\"data/BarcelonaCiutat_SeccionsCensals.csv\", GEOM_POSSIBLE_NAMES=\"geometria_wgs84\", KEEP_GEOM_COLUMNS=\"NO\")\n",
    "\n",
    "# Create GeoDataFrame with geometry\n",
    "gdf = gpd.GeoDataFrame(df, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "\n",
    "# Calculate centroids (for distance calculations later)\n",
    "# Note: For accurate centroid calculations, we'll reproject to UTM in a later step\n",
    "gdf[\"centroid\"] = gdf.geometry.centroid\n",
    "\n",
    "# Extract latitude and longitude from centroids\n",
    "gdf[\"centroid_lat\"] = gdf[\"centroid\"].y\n",
    "gdf[\"centroid_lon\"] = gdf[\"centroid\"].x\n",
    "\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4160af",
   "metadata": {},
   "source": [
    "## 2. Create SECCIO_CENSAL Identifier\n",
    "\n",
    "Create a unique identifier for each census section by concatenating:\n",
    "- Prefix: `080193` (Barcelona municipality code)\n",
    "- District code: `codi_districte`\n",
    "- Section code: `codi_seccio_censal`\n",
    "\n",
    "This creates a standardized identifier format: `080193 + codi_districte + codi_seccio_censal`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "79c4ef79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>codi_districte</th>\n",
       "      <th>codi_seccio_censal</th>\n",
       "      <th>SECCIO_CENSAL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01</td>\n",
       "      <td>001</td>\n",
       "      <td>08019301001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01</td>\n",
       "      <td>002</td>\n",
       "      <td>08019301002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01</td>\n",
       "      <td>003</td>\n",
       "      <td>08019301003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01</td>\n",
       "      <td>004</td>\n",
       "      <td>08019301004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01</td>\n",
       "      <td>005</td>\n",
       "      <td>08019301005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  codi_districte codi_seccio_censal SECCIO_CENSAL\n",
       "0             01                001   08019301001\n",
       "1             01                002   08019301002\n",
       "2             01                003   08019301003\n",
       "3             01                004   08019301004\n",
       "4             01                005   08019301005"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create SECCIO_CENSAL field: prefix 080193 + codi_districte + codi_seccio_censal\n",
    "gdf[\"SECCIO_CENSAL\"] = \"080193\" + gdf[\"codi_districte\"].astype(str) + gdf[\"codi_seccio_censal\"].astype(str)\n",
    "\n",
    "# Verify the identifier creation\n",
    "gdf[[\"codi_districte\", \"codi_seccio_censal\", \"SECCIO_CENSAL\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8a38227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values in SECCIO_CENSAL: 1068\n",
      "Total number of rows: 1068\n"
     ]
    }
   ],
   "source": [
    "# Verify uniqueness of SECCIO_CENSAL\n",
    "num_unique = gdf[\"SECCIO_CENSAL\"].nunique()\n",
    "print(f\"Number of unique values in SECCIO_CENSAL: {num_unique}\")\n",
    "print(f\"Total number of rows: {len(gdf)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79fa3cf",
   "metadata": {},
   "source": [
    "**Note:** This corresponds to the number of census sections in Barcelona (1068), confirming that our identifier field is correct and unique for each section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f947d79a",
   "metadata": {},
   "source": [
    "## 3. Load Weather Stations\n",
    "\n",
    "Load the weather stations for which we have complete data for 2023 and 2024. \n",
    "\n",
    "**Note:** We exclude the Zoo weather station (X2) for our simplified version of the map, since it was dismantled in October 2024.\n",
    "\n",
    "The three stations used are:\n",
    "- **D5**: Located at coordinates (2.12379, 41.41864)\n",
    "- **X4**: Located at coordinates (2.16775, 41.38390)\n",
    "- **X8**: Located at coordinates (2.10540, 41.37919)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99c52a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D5</td>\n",
       "      <td>41.41864</td>\n",
       "      <td>2.12379</td>\n",
       "      <td>POINT (2.12379 41.41864)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>X4</td>\n",
       "      <td>41.38390</td>\n",
       "      <td>2.16775</td>\n",
       "      <td>POINT (2.16775 41.3839)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>X8</td>\n",
       "      <td>41.37919</td>\n",
       "      <td>2.10540</td>\n",
       "      <td>POINT (2.1054 41.37919)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  name       lat      lon                  geometry\n",
       "0   D5  41.41864  2.12379  POINT (2.12379 41.41864)\n",
       "1   X4  41.38390  2.16775   POINT (2.16775 41.3839)\n",
       "2   X8  41.37919  2.10540   POINT (2.1054 41.37919)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create GeoDataFrame with weather station locations\n",
    "stations = gpd.GeoDataFrame(\n",
    "    {\n",
    "        \"name\": [\"D5\", \"X4\", \"X8\"],\n",
    "        \"lat\": [41.41864, 41.38390, 41.37919],\n",
    "        \"lon\": [2.12379, 2.16775, 2.10540],\n",
    "    },\n",
    "    geometry=gpd.points_from_xy(\n",
    "        [2.12379, 2.16775, 2.10540],  # longitude (x)\n",
    "        [41.41864, 41.38390, 41.37919],  # latitude (y)\n",
    "    ),\n",
    "    crs=\"EPSG:4326\"  # WGS84 coordinate system\n",
    ")\n",
    "\n",
    "stations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d780d739",
   "metadata": {},
   "source": [
    "## 4. Assign Weather Stations to Census Sections\n",
    "\n",
    "For each census section, we find the nearest weather station using spatial joins. This process involves:\n",
    "\n",
    "1. **Reproject to UTM (EPSG:25831)**: Convert both datasets to a projected coordinate system (UTM Zone 31N) for accurate distance calculations in meters\n",
    "2. **Calculate centroids**: Compute the centroid of each census section polygon in the projected CRS\n",
    "3. **Find nearest station**: Use `sjoin_nearest` to find the closest weather station to each centroid\n",
    "4. **Merge results**: Add the assigned station name and distance to the original GeoDataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1684aeec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of census sections assigned to stations: 1068\n",
      "\n",
      "Distribution of stations:\n",
      "WEATHER_STATION\n",
      "X4    625\n",
      "D5    338\n",
      "X8    105\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distance statistics (meters):\n",
      "count    1068.000000\n",
      "mean     2988.696968\n",
      "std      1492.095398\n",
      "min       121.156567\n",
      "25%      1914.985512\n",
      "50%      2611.524009\n",
      "75%      4095.279686\n",
      "max      7152.228289\n",
      "Name: dist_m, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Reproject to UTM for accurate distance calculations\n",
    "gdf_utm = gdf.to_crs(epsg=25831)  # UTM Zone 31N (Spain)\n",
    "stations_utm = stations.to_crs(epsg=25831)\n",
    "\n",
    "# Step 2: Calculate centroids in projected CRS (more accurate than WGS84)\n",
    "gdf_utm[\"centroid\"] = gdf_utm.geometry.centroid\n",
    "centroids = gdf_utm.set_geometry(\"centroid\")\n",
    "\n",
    "# Step 3: Find nearest weather station to each census section centroid\n",
    "nearest = gpd.sjoin_nearest(\n",
    "    centroids,\n",
    "    stations_utm[[\"name\", \"geometry\"]],\n",
    "    how=\"left\",\n",
    "    distance_col=\"dist_m\"  # Distance in meters\n",
    ")\n",
    "\n",
    "# Step 4: Merge results back to original GeoDataFrame\n",
    "# Merge on index since sjoin_nearest preserves the index from centroids\n",
    "gdf = gdf.merge(\n",
    "    nearest[[\"name\", \"dist_m\"]],\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    how=\"left\"\n",
    ")\n",
    "gdf = gdf.rename(columns={\"name\": \"WEATHER_STATION\"})\n",
    "\n",
    "# Display results\n",
    "print(f\"Number of census sections assigned to stations: {gdf['WEATHER_STATION'].notna().sum()}\")\n",
    "print(f\"\\nDistribution of stations:\")\n",
    "print(gdf[\"WEATHER_STATION\"].value_counts())\n",
    "print(f\"\\nDistance statistics (meters):\")\n",
    "print(gdf[\"dist_m\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9001b7",
   "metadata": {},
   "source": [
    "## 5. Preview Results\n",
    "\n",
    "Preview the final GeoDataFrame with assigned weather stations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f5cd816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SECCIO_CENSAL</th>\n",
       "      <th>nom_districte</th>\n",
       "      <th>nom_barri</th>\n",
       "      <th>WEATHER_STATION</th>\n",
       "      <th>dist_m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>08019301001</td>\n",
       "      <td>Ciutat Vella</td>\n",
       "      <td>el Raval</td>\n",
       "      <td>X4</td>\n",
       "      <td>1325.662820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>08019301002</td>\n",
       "      <td>Ciutat Vella</td>\n",
       "      <td>el Raval</td>\n",
       "      <td>X4</td>\n",
       "      <td>839.842507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>08019301003</td>\n",
       "      <td>Ciutat Vella</td>\n",
       "      <td>el Raval</td>\n",
       "      <td>X4</td>\n",
       "      <td>970.927625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>08019301004</td>\n",
       "      <td>Ciutat Vella</td>\n",
       "      <td>el Raval</td>\n",
       "      <td>X4</td>\n",
       "      <td>840.155800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>08019301005</td>\n",
       "      <td>Ciutat Vella</td>\n",
       "      <td>el Raval</td>\n",
       "      <td>X4</td>\n",
       "      <td>701.768850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>08019301006</td>\n",
       "      <td>Ciutat Vella</td>\n",
       "      <td>el Raval</td>\n",
       "      <td>X4</td>\n",
       "      <td>544.606912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>08019301007</td>\n",
       "      <td>Ciutat Vella</td>\n",
       "      <td>el Raval</td>\n",
       "      <td>X4</td>\n",
       "      <td>560.631260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>08019301008</td>\n",
       "      <td>Ciutat Vella</td>\n",
       "      <td>el Raval</td>\n",
       "      <td>X4</td>\n",
       "      <td>675.929543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>08019301009</td>\n",
       "      <td>Ciutat Vella</td>\n",
       "      <td>el Raval</td>\n",
       "      <td>X4</td>\n",
       "      <td>726.465578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>08019301010</td>\n",
       "      <td>Ciutat Vella</td>\n",
       "      <td>el Raval</td>\n",
       "      <td>X4</td>\n",
       "      <td>563.659021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SECCIO_CENSAL nom_districte nom_barri WEATHER_STATION       dist_m\n",
       "0   08019301001  Ciutat Vella  el Raval              X4  1325.662820\n",
       "1   08019301002  Ciutat Vella  el Raval              X4   839.842507\n",
       "2   08019301003  Ciutat Vella  el Raval              X4   970.927625\n",
       "3   08019301004  Ciutat Vella  el Raval              X4   840.155800\n",
       "4   08019301005  Ciutat Vella  el Raval              X4   701.768850\n",
       "5   08019301006  Ciutat Vella  el Raval              X4   544.606912\n",
       "6   08019301007  Ciutat Vella  el Raval              X4   560.631260\n",
       "7   08019301008  Ciutat Vella  el Raval              X4   675.929543\n",
       "8   08019301009  Ciutat Vella  el Raval              X4   726.465578\n",
       "9   08019301010  Ciutat Vella  el Raval              X4   563.659021"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview the final GeoDataFrame\n",
    "gdf[[\"SECCIO_CENSAL\", \"nom_districte\", \"nom_barri\", \"WEATHER_STATION\", \"dist_m\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f80c62",
   "metadata": {},
   "source": [
    "## 6. Merge Daily Weather Data with Census Sections\n",
    "\n",
    "Load the cleaned weather data and merge it with census sections. The weather data is in long format (one row per station-date-variable), so we need to pivot it to wide format (one row per station-date with columns for each variable)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290f9128",
   "metadata": {},
   "source": [
    "### Date Range Filtering\n",
    "\n",
    "For simplicity, we filter the weather data to the period from **2023-01-04 to 2024-12-31**. This represents the temporal overlap between all our data sources:\n",
    "- **Weather data**: Available from 2021-01-01 to 2025-11-16\n",
    "- **Consumption data**: Available from 2023-01-04 onwards\n",
    "- **Leak incidents**: Available from 2023 onwards\n",
    "\n",
    "By focusing on this common period, we ensure all data sources are available for analysis while maintaining a substantial time range for our vulnerability score calculations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c07e4395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather data shape (after filtering): (48021, 9)\n",
      "\n",
      "Stations in weather data: ['D5', 'X4', 'X8']\n",
      "\n",
      "Date range: 2023-01-04 00:00:00 to 2024-12-31 00:00:00\n",
      "\n",
      "Number of variables: 22\n",
      "\n",
      "Sample of weather data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CODI_ESTACIO</th>\n",
       "      <th>DATA_LECTURA</th>\n",
       "      <th>CODI_VARIABLE</th>\n",
       "      <th>NOM_VARIABLE</th>\n",
       "      <th>VALOR</th>\n",
       "      <th>UNITAT</th>\n",
       "      <th>HORA _TU</th>\n",
       "      <th>VALOR_NUM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54232</th>\n",
       "      <td>D51000012304</td>\n",
       "      <td>D5</td>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>1.000</td>\n",
       "      <td>Temperatura mitjana diària</td>\n",
       "      <td>11,7</td>\n",
       "      <td>°C</td>\n",
       "      <td>NA</td>\n",
       "      <td>11.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54233</th>\n",
       "      <td>D51001012304</td>\n",
       "      <td>D5</td>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>1.001</td>\n",
       "      <td>Temperatura màxima diària + hora</td>\n",
       "      <td>17,7</td>\n",
       "      <td>°C</td>\n",
       "      <td>14:14:00</td>\n",
       "      <td>17.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54234</th>\n",
       "      <td>D51002012304</td>\n",
       "      <td>D5</td>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>1.002</td>\n",
       "      <td>Temperatura mínima diària + hora</td>\n",
       "      <td>9,1</td>\n",
       "      <td>°C</td>\n",
       "      <td>06:06:00</td>\n",
       "      <td>9.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54235</th>\n",
       "      <td>D51003012304</td>\n",
       "      <td>D5</td>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>1.003</td>\n",
       "      <td>Temperatura mitjana diària clàssica</td>\n",
       "      <td>13,4</td>\n",
       "      <td>°C</td>\n",
       "      <td>NA</td>\n",
       "      <td>13.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54236</th>\n",
       "      <td>D51004012304</td>\n",
       "      <td>D5</td>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>1.004</td>\n",
       "      <td>Amplitud tèrmica diària</td>\n",
       "      <td>8,6</td>\n",
       "      <td>°C</td>\n",
       "      <td>NA</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID CODI_ESTACIO DATA_LECTURA  CODI_VARIABLE  \\\n",
       "54232  D51000012304           D5   2023-01-04          1.000   \n",
       "54233  D51001012304           D5   2023-01-04          1.001   \n",
       "54234  D51002012304           D5   2023-01-04          1.002   \n",
       "54235  D51003012304           D5   2023-01-04          1.003   \n",
       "54236  D51004012304           D5   2023-01-04          1.004   \n",
       "\n",
       "                              NOM_VARIABLE VALOR UNITAT  HORA _TU  VALOR_NUM  \n",
       "54232           Temperatura mitjana diària  11,7     °C        NA       11.7  \n",
       "54233     Temperatura màxima diària + hora  17,7     °C  14:14:00       17.7  \n",
       "54234     Temperatura mínima diària + hora   9,1     °C  06:06:00        9.1  \n",
       "54235  Temperatura mitjana diària clàssica  13,4     °C        NA       13.4  \n",
       "54236              Amplitud tèrmica diària   8,6     °C        NA        8.6  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load weather data and filter to only the stations we use (D5, X4, X8)\n",
    "weather = pd.read_parquet(\"clean/weather_clean.parquet\")\n",
    "\n",
    "# Filter to only the stations assigned to census sections\n",
    "stations_to_keep = ['D5', 'X4', 'X8']\n",
    "weather = weather[weather['CODI_ESTACIO'].isin(stations_to_keep)].copy()\n",
    "\n",
    "# Drop NOM_ESTACIO since we already have CODI_ESTACIO\n",
    "if 'NOM_ESTACIO' in weather.columns:\n",
    "    weather = weather.drop(columns=['NOM_ESTACIO'])\n",
    "\n",
    "# Filter to date range: 2023-01-04 to 2024-12-31 (temporal overlap with consumption and leaks)\n",
    "date_start = pd.to_datetime('2023-01-04')\n",
    "date_end = pd.to_datetime('2024-12-31')\n",
    "weather = weather[\n",
    "    (pd.to_datetime(weather['DATA_LECTURA']) >= date_start) & \n",
    "    (pd.to_datetime(weather['DATA_LECTURA']) <= date_end)\n",
    "].copy()\n",
    "\n",
    "print(f\"Weather data shape (after filtering): {weather.shape}\")\n",
    "print(f\"\\nStations in weather data: {sorted(weather['CODI_ESTACIO'].unique())}\")\n",
    "print(f\"\\nDate range: {weather['DATA_LECTURA'].min()} to {weather['DATA_LECTURA'].max()}\")\n",
    "print(f\"\\nNumber of variables: {weather['NOM_VARIABLE'].nunique()}\")\n",
    "print(f\"\\nSample of weather data:\")\n",
    "weather.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ede05023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather daily shape: (2184, 24)\n",
      "Number of station-date combinations: 2184\n",
      "Number of weather variables: 24\n",
      "\n",
      "Columns: ['CODI_ESTACIO', 'DATA_LECTURA', 'Amplitud tèrmica diària', 'Direcció de la ratxa màx. diària de vent 10 m', 'Direcció mitjana diària del vent 10 m (m. 1)', 'Evapotranspiració de referència', 'Humitat relativa mitjana diària', 'Humitat relativa màxima diària + data', 'Humitat relativa mínima diària + data', 'Irradiació solar global diària']...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CODI_ESTACIO</th>\n",
       "      <th>DATA_LECTURA</th>\n",
       "      <th>Amplitud tèrmica diària</th>\n",
       "      <th>Direcció de la ratxa màx. diària de vent 10 m</th>\n",
       "      <th>Direcció mitjana diària del vent 10 m (m. 1)</th>\n",
       "      <th>Evapotranspiració de referència</th>\n",
       "      <th>Humitat relativa mitjana diària</th>\n",
       "      <th>Humitat relativa màxima diària + data</th>\n",
       "      <th>Humitat relativa mínima diària + data</th>\n",
       "      <th>Irradiació solar global diària</th>\n",
       "      <th>...</th>\n",
       "      <th>Precipitació màxima en 30 min (diària)+ hora</th>\n",
       "      <th>Pressió atmosfèrica mitjana diària</th>\n",
       "      <th>Pressió atmosfèrica màxima diària + hora</th>\n",
       "      <th>Pressió atmosfèrica mínima diària + hora</th>\n",
       "      <th>Ratxa màxima diària del vent 10 m + hora</th>\n",
       "      <th>Temperatura mitjana diària</th>\n",
       "      <th>Temperatura mitjana diària clàssica</th>\n",
       "      <th>Temperatura màxima diària + hora</th>\n",
       "      <th>Temperatura mínima diària + hora</th>\n",
       "      <th>Velocitat mitjana diària del vent 10 m (esc.)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D5</td>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>8.6</td>\n",
       "      <td>338.0</td>\n",
       "      <td>335.0</td>\n",
       "      <td>1.09</td>\n",
       "      <td>66.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>8.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>983.1</td>\n",
       "      <td>984.7</td>\n",
       "      <td>982.3</td>\n",
       "      <td>7.6</td>\n",
       "      <td>11.7</td>\n",
       "      <td>13.4</td>\n",
       "      <td>17.7</td>\n",
       "      <td>9.1</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D5</td>\n",
       "      <td>2023-01-05</td>\n",
       "      <td>7.6</td>\n",
       "      <td>304.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>1.18</td>\n",
       "      <td>53.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>9.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>979.0</td>\n",
       "      <td>982.7</td>\n",
       "      <td>975.8</td>\n",
       "      <td>8.7</td>\n",
       "      <td>11.7</td>\n",
       "      <td>12.5</td>\n",
       "      <td>16.3</td>\n",
       "      <td>8.7</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D5</td>\n",
       "      <td>2023-01-06</td>\n",
       "      <td>7.7</td>\n",
       "      <td>310.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>1.02</td>\n",
       "      <td>59.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>9.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>975.5</td>\n",
       "      <td>976.9</td>\n",
       "      <td>974.7</td>\n",
       "      <td>10.4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.1</td>\n",
       "      <td>14.9</td>\n",
       "      <td>7.2</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D5</td>\n",
       "      <td>2023-01-07</td>\n",
       "      <td>6.1</td>\n",
       "      <td>295.0</td>\n",
       "      <td>267.0</td>\n",
       "      <td>1.13</td>\n",
       "      <td>67.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>9.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>971.3</td>\n",
       "      <td>974.7</td>\n",
       "      <td>968.7</td>\n",
       "      <td>14.7</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.3</td>\n",
       "      <td>13.3</td>\n",
       "      <td>7.2</td>\n",
       "      <td>5.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D5</td>\n",
       "      <td>2023-01-08</td>\n",
       "      <td>3.8</td>\n",
       "      <td>274.0</td>\n",
       "      <td>266.0</td>\n",
       "      <td>0.58</td>\n",
       "      <td>72.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>964.4</td>\n",
       "      <td>968.8</td>\n",
       "      <td>961.1</td>\n",
       "      <td>11.9</td>\n",
       "      <td>11.9</td>\n",
       "      <td>11.7</td>\n",
       "      <td>13.6</td>\n",
       "      <td>9.8</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  CODI_ESTACIO DATA_LECTURA  Amplitud tèrmica diària  \\\n",
       "0           D5   2023-01-04                      8.6   \n",
       "1           D5   2023-01-05                      7.6   \n",
       "2           D5   2023-01-06                      7.7   \n",
       "3           D5   2023-01-07                      6.1   \n",
       "4           D5   2023-01-08                      3.8   \n",
       "\n",
       "   Direcció de la ratxa màx. diària de vent 10 m  \\\n",
       "0                                          338.0   \n",
       "1                                          304.0   \n",
       "2                                          310.0   \n",
       "3                                          295.0   \n",
       "4                                          274.0   \n",
       "\n",
       "   Direcció mitjana diària del vent 10 m (m. 1)  \\\n",
       "0                                         335.0   \n",
       "1                                         263.0   \n",
       "2                                         284.0   \n",
       "3                                         267.0   \n",
       "4                                         266.0   \n",
       "\n",
       "   Evapotranspiració de referència  Humitat relativa mitjana diària  \\\n",
       "0                             1.09                             66.0   \n",
       "1                             1.18                             53.0   \n",
       "2                             1.02                             59.0   \n",
       "3                             1.13                             67.0   \n",
       "4                             0.58                             72.0   \n",
       "\n",
       "   Humitat relativa màxima diària + data  \\\n",
       "0                                   87.0   \n",
       "1                                   67.0   \n",
       "2                                   77.0   \n",
       "3                                  100.0   \n",
       "4                                   86.0   \n",
       "\n",
       "   Humitat relativa mínima diària + data  Irradiació solar global diària  ...  \\\n",
       "0                                   39.0                             8.9  ...   \n",
       "1                                   35.0                             9.2  ...   \n",
       "2                                   47.0                             9.2  ...   \n",
       "3                                   46.0                             9.6  ...   \n",
       "4                                   56.0                             3.2  ...   \n",
       "\n",
       "   Precipitació màxima en 30 min (diària)+ hora  \\\n",
       "0                                           0.0   \n",
       "1                                           0.0   \n",
       "2                                           0.0   \n",
       "3                                           0.0   \n",
       "4                                           0.1   \n",
       "\n",
       "   Pressió atmosfèrica mitjana diària  \\\n",
       "0                               983.1   \n",
       "1                               979.0   \n",
       "2                               975.5   \n",
       "3                               971.3   \n",
       "4                               964.4   \n",
       "\n",
       "   Pressió atmosfèrica màxima diària + hora  \\\n",
       "0                                     984.7   \n",
       "1                                     982.7   \n",
       "2                                     976.9   \n",
       "3                                     974.7   \n",
       "4                                     968.8   \n",
       "\n",
       "   Pressió atmosfèrica mínima diària + hora  \\\n",
       "0                                     982.3   \n",
       "1                                     975.8   \n",
       "2                                     974.7   \n",
       "3                                     968.7   \n",
       "4                                     961.1   \n",
       "\n",
       "   Ratxa màxima diària del vent 10 m + hora  Temperatura mitjana diària  \\\n",
       "0                                       7.6                        11.7   \n",
       "1                                       8.7                        11.7   \n",
       "2                                      10.4                        10.0   \n",
       "3                                      14.7                         9.5   \n",
       "4                                      11.9                        11.9   \n",
       "\n",
       "   Temperatura mitjana diària clàssica  Temperatura màxima diària + hora  \\\n",
       "0                                 13.4                              17.7   \n",
       "1                                 12.5                              16.3   \n",
       "2                                 11.1                              14.9   \n",
       "3                                 10.3                              13.3   \n",
       "4                                 11.7                              13.6   \n",
       "\n",
       "   Temperatura mínima diària + hora  \\\n",
       "0                               9.1   \n",
       "1                               8.7   \n",
       "2                               7.2   \n",
       "3                               7.2   \n",
       "4                               9.8   \n",
       "\n",
       "   Velocitat mitjana diària del vent 10 m (esc.)  \n",
       "0                                            3.5  \n",
       "1                                            3.5  \n",
       "2                                            3.9  \n",
       "3                                            5.8  \n",
       "4                                            4.5  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pivot weather data from long to wide format\n",
    "# Each row will be a unique combination of station (CODI_ESTACIO) and date (DATA_LECTURA)\n",
    "# Each variable (NOM_VARIABLE) becomes a column with its VALOR_NUM value\n",
    "# Note: No duplicate measurements - each station-date-variable has only one value\n",
    "\n",
    "weather_daily = weather.pivot_table(\n",
    "    index=['CODI_ESTACIO', 'DATA_LECTURA'],\n",
    "    columns='NOM_VARIABLE',\n",
    "    values='VALOR_NUM',\n",
    "    aggfunc='first'  # Since there are no duplicates, 'first' is sufficient\n",
    ").reset_index()\n",
    "\n",
    "# Flatten column names (remove multi-index if any)\n",
    "weather_daily.columns.name = None\n",
    "\n",
    "print(f\"Weather daily shape: {weather_daily.shape}\")\n",
    "print(f\"Number of station-date combinations: {len(weather_daily)}\")\n",
    "print(f\"Number of weather variables: {len(weather_daily.columns)}\")\n",
    "print(f\"\\nColumns: {list(weather_daily.columns[:10])}...\")  # Show first 10 columns\n",
    "weather_daily.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc3cadc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final GeoDataFrame shape: (777504, 37)\n",
      "\n",
      "Number of census sections: 1068\n",
      "Number of unique dates: 728\n",
      "Expected rows (sections × dates): 777,504\n",
      "Actual rows: 777,504\n",
      "Match: ✓\n",
      "\n",
      "Date range: 2023-01-04 00:00:00 to 2024-12-31 00:00:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SECCIO_CENSAL</th>\n",
       "      <th>codi_districte</th>\n",
       "      <th>nom_districte</th>\n",
       "      <th>codi_barri</th>\n",
       "      <th>nom_barri</th>\n",
       "      <th>codi_aeb</th>\n",
       "      <th>codi_seccio_censal</th>\n",
       "      <th>geometria_etrs89</th>\n",
       "      <th>DATA_LECTURA</th>\n",
       "      <th>CODI_ESTACIO</th>\n",
       "      <th>...</th>\n",
       "      <th>Temperatura mitjana diària</th>\n",
       "      <th>Temperatura mitjana diària clàssica</th>\n",
       "      <th>Temperatura màxima diària + hora</th>\n",
       "      <th>Temperatura mínima diària + hora</th>\n",
       "      <th>Velocitat mitjana diària del vent 10 m (esc.)</th>\n",
       "      <th>geometry</th>\n",
       "      <th>centroid</th>\n",
       "      <th>centroid_lat</th>\n",
       "      <th>centroid_lon</th>\n",
       "      <th>dist_m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>08019301001</td>\n",
       "      <td>01</td>\n",
       "      <td>Ciutat Vella</td>\n",
       "      <td>01</td>\n",
       "      <td>el Raval</td>\n",
       "      <td>001</td>\n",
       "      <td>001</td>\n",
       "      <td>POLYGON ((431076.9025 4581077.3095, 431058.164...</td>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>X4</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>16.8</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>POLYGON ((2.17575 41.37827, 2.17552 41.37865, ...</td>\n",
       "      <td>POINT (2.17722 41.37432)</td>\n",
       "      <td>41.374324</td>\n",
       "      <td>2.177219</td>\n",
       "      <td>1325.66282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>08019301001</td>\n",
       "      <td>01</td>\n",
       "      <td>Ciutat Vella</td>\n",
       "      <td>01</td>\n",
       "      <td>el Raval</td>\n",
       "      <td>001</td>\n",
       "      <td>001</td>\n",
       "      <td>POLYGON ((431076.9025 4581077.3095, 431058.164...</td>\n",
       "      <td>2023-01-05</td>\n",
       "      <td>X4</td>\n",
       "      <td>...</td>\n",
       "      <td>12.5</td>\n",
       "      <td>13.1</td>\n",
       "      <td>17.3</td>\n",
       "      <td>8.8</td>\n",
       "      <td>1.1</td>\n",
       "      <td>POLYGON ((2.17575 41.37827, 2.17552 41.37865, ...</td>\n",
       "      <td>POINT (2.17722 41.37432)</td>\n",
       "      <td>41.374324</td>\n",
       "      <td>2.177219</td>\n",
       "      <td>1325.66282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>08019301001</td>\n",
       "      <td>01</td>\n",
       "      <td>Ciutat Vella</td>\n",
       "      <td>01</td>\n",
       "      <td>el Raval</td>\n",
       "      <td>001</td>\n",
       "      <td>001</td>\n",
       "      <td>POLYGON ((431076.9025 4581077.3095, 431058.164...</td>\n",
       "      <td>2023-01-06</td>\n",
       "      <td>X4</td>\n",
       "      <td>...</td>\n",
       "      <td>11.8</td>\n",
       "      <td>11.9</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>0.9</td>\n",
       "      <td>POLYGON ((2.17575 41.37827, 2.17552 41.37865, ...</td>\n",
       "      <td>POINT (2.17722 41.37432)</td>\n",
       "      <td>41.374324</td>\n",
       "      <td>2.177219</td>\n",
       "      <td>1325.66282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>08019301001</td>\n",
       "      <td>01</td>\n",
       "      <td>Ciutat Vella</td>\n",
       "      <td>01</td>\n",
       "      <td>el Raval</td>\n",
       "      <td>001</td>\n",
       "      <td>001</td>\n",
       "      <td>POLYGON ((431076.9025 4581077.3095, 431058.164...</td>\n",
       "      <td>2023-01-07</td>\n",
       "      <td>X4</td>\n",
       "      <td>...</td>\n",
       "      <td>12.5</td>\n",
       "      <td>13.0</td>\n",
       "      <td>16.7</td>\n",
       "      <td>9.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>POLYGON ((2.17575 41.37827, 2.17552 41.37865, ...</td>\n",
       "      <td>POINT (2.17722 41.37432)</td>\n",
       "      <td>41.374324</td>\n",
       "      <td>2.177219</td>\n",
       "      <td>1325.66282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>08019301001</td>\n",
       "      <td>01</td>\n",
       "      <td>Ciutat Vella</td>\n",
       "      <td>01</td>\n",
       "      <td>el Raval</td>\n",
       "      <td>001</td>\n",
       "      <td>001</td>\n",
       "      <td>POLYGON ((431076.9025 4581077.3095, 431058.164...</td>\n",
       "      <td>2023-01-08</td>\n",
       "      <td>X4</td>\n",
       "      <td>...</td>\n",
       "      <td>14.2</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.2</td>\n",
       "      <td>11.7</td>\n",
       "      <td>1.8</td>\n",
       "      <td>POLYGON ((2.17575 41.37827, 2.17552 41.37865, ...</td>\n",
       "      <td>POINT (2.17722 41.37432)</td>\n",
       "      <td>41.374324</td>\n",
       "      <td>2.177219</td>\n",
       "      <td>1325.66282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>08019301001</td>\n",
       "      <td>01</td>\n",
       "      <td>Ciutat Vella</td>\n",
       "      <td>01</td>\n",
       "      <td>el Raval</td>\n",
       "      <td>001</td>\n",
       "      <td>001</td>\n",
       "      <td>POLYGON ((431076.9025 4581077.3095, 431058.164...</td>\n",
       "      <td>2023-01-09</td>\n",
       "      <td>X4</td>\n",
       "      <td>...</td>\n",
       "      <td>14.5</td>\n",
       "      <td>14.4</td>\n",
       "      <td>17.1</td>\n",
       "      <td>11.7</td>\n",
       "      <td>4.2</td>\n",
       "      <td>POLYGON ((2.17575 41.37827, 2.17552 41.37865, ...</td>\n",
       "      <td>POINT (2.17722 41.37432)</td>\n",
       "      <td>41.374324</td>\n",
       "      <td>2.177219</td>\n",
       "      <td>1325.66282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>08019301001</td>\n",
       "      <td>01</td>\n",
       "      <td>Ciutat Vella</td>\n",
       "      <td>01</td>\n",
       "      <td>el Raval</td>\n",
       "      <td>001</td>\n",
       "      <td>001</td>\n",
       "      <td>POLYGON ((431076.9025 4581077.3095, 431058.164...</td>\n",
       "      <td>2023-01-10</td>\n",
       "      <td>X4</td>\n",
       "      <td>...</td>\n",
       "      <td>12.2</td>\n",
       "      <td>12.6</td>\n",
       "      <td>15.7</td>\n",
       "      <td>9.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>POLYGON ((2.17575 41.37827, 2.17552 41.37865, ...</td>\n",
       "      <td>POINT (2.17722 41.37432)</td>\n",
       "      <td>41.374324</td>\n",
       "      <td>2.177219</td>\n",
       "      <td>1325.66282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>08019301001</td>\n",
       "      <td>01</td>\n",
       "      <td>Ciutat Vella</td>\n",
       "      <td>01</td>\n",
       "      <td>el Raval</td>\n",
       "      <td>001</td>\n",
       "      <td>001</td>\n",
       "      <td>POLYGON ((431076.9025 4581077.3095, 431058.164...</td>\n",
       "      <td>2023-01-11</td>\n",
       "      <td>X4</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>16.4</td>\n",
       "      <td>9.9</td>\n",
       "      <td>1.2</td>\n",
       "      <td>POLYGON ((2.17575 41.37827, 2.17552 41.37865, ...</td>\n",
       "      <td>POINT (2.17722 41.37432)</td>\n",
       "      <td>41.374324</td>\n",
       "      <td>2.177219</td>\n",
       "      <td>1325.66282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>08019301001</td>\n",
       "      <td>01</td>\n",
       "      <td>Ciutat Vella</td>\n",
       "      <td>01</td>\n",
       "      <td>el Raval</td>\n",
       "      <td>001</td>\n",
       "      <td>001</td>\n",
       "      <td>POLYGON ((431076.9025 4581077.3095, 431058.164...</td>\n",
       "      <td>2023-01-12</td>\n",
       "      <td>X4</td>\n",
       "      <td>...</td>\n",
       "      <td>12.2</td>\n",
       "      <td>12.8</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>POLYGON ((2.17575 41.37827, 2.17552 41.37865, ...</td>\n",
       "      <td>POINT (2.17722 41.37432)</td>\n",
       "      <td>41.374324</td>\n",
       "      <td>2.177219</td>\n",
       "      <td>1325.66282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>08019301001</td>\n",
       "      <td>01</td>\n",
       "      <td>Ciutat Vella</td>\n",
       "      <td>01</td>\n",
       "      <td>el Raval</td>\n",
       "      <td>001</td>\n",
       "      <td>001</td>\n",
       "      <td>POLYGON ((431076.9025 4581077.3095, 431058.164...</td>\n",
       "      <td>2023-01-13</td>\n",
       "      <td>X4</td>\n",
       "      <td>...</td>\n",
       "      <td>12.7</td>\n",
       "      <td>13.5</td>\n",
       "      <td>16.8</td>\n",
       "      <td>10.2</td>\n",
       "      <td>1.1</td>\n",
       "      <td>POLYGON ((2.17575 41.37827, 2.17552 41.37865, ...</td>\n",
       "      <td>POINT (2.17722 41.37432)</td>\n",
       "      <td>41.374324</td>\n",
       "      <td>2.177219</td>\n",
       "      <td>1325.66282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  SECCIO_CENSAL codi_districte nom_districte codi_barri nom_barri codi_aeb  \\\n",
       "0   08019301001             01  Ciutat Vella         01  el Raval      001   \n",
       "1   08019301001             01  Ciutat Vella         01  el Raval      001   \n",
       "2   08019301001             01  Ciutat Vella         01  el Raval      001   \n",
       "3   08019301001             01  Ciutat Vella         01  el Raval      001   \n",
       "4   08019301001             01  Ciutat Vella         01  el Raval      001   \n",
       "5   08019301001             01  Ciutat Vella         01  el Raval      001   \n",
       "6   08019301001             01  Ciutat Vella         01  el Raval      001   \n",
       "7   08019301001             01  Ciutat Vella         01  el Raval      001   \n",
       "8   08019301001             01  Ciutat Vella         01  el Raval      001   \n",
       "9   08019301001             01  Ciutat Vella         01  el Raval      001   \n",
       "\n",
       "  codi_seccio_censal                                   geometria_etrs89  \\\n",
       "0                001  POLYGON ((431076.9025 4581077.3095, 431058.164...   \n",
       "1                001  POLYGON ((431076.9025 4581077.3095, 431058.164...   \n",
       "2                001  POLYGON ((431076.9025 4581077.3095, 431058.164...   \n",
       "3                001  POLYGON ((431076.9025 4581077.3095, 431058.164...   \n",
       "4                001  POLYGON ((431076.9025 4581077.3095, 431058.164...   \n",
       "5                001  POLYGON ((431076.9025 4581077.3095, 431058.164...   \n",
       "6                001  POLYGON ((431076.9025 4581077.3095, 431058.164...   \n",
       "7                001  POLYGON ((431076.9025 4581077.3095, 431058.164...   \n",
       "8                001  POLYGON ((431076.9025 4581077.3095, 431058.164...   \n",
       "9                001  POLYGON ((431076.9025 4581077.3095, 431058.164...   \n",
       "\n",
       "  DATA_LECTURA CODI_ESTACIO  ...  Temperatura mitjana diària  \\\n",
       "0   2023-01-04           X4  ...                        13.0   \n",
       "1   2023-01-05           X4  ...                        12.5   \n",
       "2   2023-01-06           X4  ...                        11.8   \n",
       "3   2023-01-07           X4  ...                        12.5   \n",
       "4   2023-01-08           X4  ...                        14.2   \n",
       "5   2023-01-09           X4  ...                        14.5   \n",
       "6   2023-01-10           X4  ...                        12.2   \n",
       "7   2023-01-11           X4  ...                        13.0   \n",
       "8   2023-01-12           X4  ...                        12.2   \n",
       "9   2023-01-13           X4  ...                        12.7   \n",
       "\n",
       "   Temperatura mitjana diària clàssica  Temperatura màxima diària + hora  \\\n",
       "0                                 13.5                              16.8   \n",
       "1                                 13.1                              17.3   \n",
       "2                                 11.9                              16.0   \n",
       "3                                 13.0                              16.7   \n",
       "4                                 14.0                              16.2   \n",
       "5                                 14.4                              17.1   \n",
       "6                                 12.6                              15.7   \n",
       "7                                 13.2                              16.4   \n",
       "8                                 12.8                              16.0   \n",
       "9                                 13.5                              16.8   \n",
       "\n",
       "   Temperatura mínima diària + hora  \\\n",
       "0                              10.2   \n",
       "1                               8.8   \n",
       "2                               7.7   \n",
       "3                               9.3   \n",
       "4                              11.7   \n",
       "5                              11.7   \n",
       "6                               9.4   \n",
       "7                               9.9   \n",
       "8                               9.6   \n",
       "9                              10.2   \n",
       "\n",
       "   Velocitat mitjana diària del vent 10 m (esc.)  \\\n",
       "0                                            0.9   \n",
       "1                                            1.1   \n",
       "2                                            0.9   \n",
       "3                                            1.7   \n",
       "4                                            1.8   \n",
       "5                                            4.2   \n",
       "6                                            1.6   \n",
       "7                                            1.2   \n",
       "8                                            1.9   \n",
       "9                                            1.1   \n",
       "\n",
       "                                            geometry  \\\n",
       "0  POLYGON ((2.17575 41.37827, 2.17552 41.37865, ...   \n",
       "1  POLYGON ((2.17575 41.37827, 2.17552 41.37865, ...   \n",
       "2  POLYGON ((2.17575 41.37827, 2.17552 41.37865, ...   \n",
       "3  POLYGON ((2.17575 41.37827, 2.17552 41.37865, ...   \n",
       "4  POLYGON ((2.17575 41.37827, 2.17552 41.37865, ...   \n",
       "5  POLYGON ((2.17575 41.37827, 2.17552 41.37865, ...   \n",
       "6  POLYGON ((2.17575 41.37827, 2.17552 41.37865, ...   \n",
       "7  POLYGON ((2.17575 41.37827, 2.17552 41.37865, ...   \n",
       "8  POLYGON ((2.17575 41.37827, 2.17552 41.37865, ...   \n",
       "9  POLYGON ((2.17575 41.37827, 2.17552 41.37865, ...   \n",
       "\n",
       "                   centroid  centroid_lat  centroid_lon      dist_m  \n",
       "0  POINT (2.17722 41.37432)     41.374324      2.177219  1325.66282  \n",
       "1  POINT (2.17722 41.37432)     41.374324      2.177219  1325.66282  \n",
       "2  POINT (2.17722 41.37432)     41.374324      2.177219  1325.66282  \n",
       "3  POINT (2.17722 41.37432)     41.374324      2.177219  1325.66282  \n",
       "4  POINT (2.17722 41.37432)     41.374324      2.177219  1325.66282  \n",
       "5  POINT (2.17722 41.37432)     41.374324      2.177219  1325.66282  \n",
       "6  POINT (2.17722 41.37432)     41.374324      2.177219  1325.66282  \n",
       "7  POINT (2.17722 41.37432)     41.374324      2.177219  1325.66282  \n",
       "8  POINT (2.17722 41.37432)     41.374324      2.177219  1325.66282  \n",
       "9  POINT (2.17722 41.37432)     41.374324      2.177219  1325.66282  \n",
       "\n",
       "[10 rows x 37 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create cartesian product: one row per census section per date\n",
    "# This ensures we have complete daily structure (one row per census section per day)\n",
    "\n",
    "# Get unique dates from weather data\n",
    "date_start = pd.to_datetime('2023-01-04')\n",
    "date_end = pd.to_datetime('2024-12-31')\n",
    "date_range = pd.date_range(start=date_start, end=date_end, freq='D')\n",
    "dates_df = pd.DataFrame({'DATA_LECTURA': date_range})\n",
    "\n",
    "# Create cartesian product: census sections × dates\n",
    "# Select only necessary columns from gdf (excluding geometry temporarily)\n",
    "gdf_stations = gdf[['SECCIO_CENSAL', 'WEATHER_STATION'] + \n",
    "                   [col for col in gdf.columns \n",
    "                    if col not in ['SECCIO_CENSAL', 'WEATHER_STATION', 'geometry', 'centroid', 'centroid_lat', 'centroid_lon', 'dist_m']]].copy()\n",
    "\n",
    "# Create cartesian product using merge with dummy key\n",
    "gdf_stations['key'] = 1\n",
    "dates_df['key'] = 1\n",
    "gdf_daily = gdf_stations.merge(dates_df, on='key').drop(columns='key')\n",
    "\n",
    "# Now merge weather data by matching WEATHER_STATION with CODI_ESTACIO and matching dates\n",
    "gdf_daily = gdf_daily.merge(\n",
    "    weather_daily,\n",
    "    left_on=['WEATHER_STATION', 'DATA_LECTURA'],\n",
    "    right_on=['CODI_ESTACIO', 'DATA_LECTURA'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Drop WEATHER_STATION since it's the same as CODI_ESTACIO (which comes from weather data)\n",
    "gdf_daily = gdf_daily.drop(columns=['WEATHER_STATION'])\n",
    "\n",
    "# Merge back geometry and other spatial info from original gdf\n",
    "gdf_spatial = gdf[['SECCIO_CENSAL', 'geometry', 'centroid', 'centroid_lat', 'centroid_lon', 'dist_m']].copy()\n",
    "gdf_daily = gdf_daily.merge(gdf_spatial, on='SECCIO_CENSAL', how='left')\n",
    "\n",
    "# Convert back to GeoDataFrame\n",
    "gdf_daily = gpd.GeoDataFrame(gdf_daily, geometry='geometry', crs='EPSG:4326')\n",
    "\n",
    "# Verify structure\n",
    "expected_rows = len(gdf['SECCIO_CENSAL'].unique()) * len(date_range)\n",
    "actual_rows = len(gdf_daily)\n",
    "\n",
    "print(f\"Final GeoDataFrame shape: {gdf_daily.shape}\")\n",
    "print(f\"\\nNumber of census sections: {gdf_daily['SECCIO_CENSAL'].nunique()}\")\n",
    "print(f\"Number of unique dates: {gdf_daily['DATA_LECTURA'].nunique()}\")\n",
    "print(f\"Expected rows (sections × dates): {expected_rows:,}\")\n",
    "print(f\"Actual rows: {actual_rows:,}\")\n",
    "print(f\"Match: {'✓' if actual_rows == expected_rows else '✗'}\")\n",
    "print(f\"\\nDate range: {gdf_daily['DATA_LECTURA'].min()} to {gdf_daily['DATA_LECTURA'].max()}\")\n",
    "\n",
    "# Show sample\n",
    "gdf_daily.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c275e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 weather variable columns\n",
      "Sample weather columns: ['Amplitud tèrmica diària', 'Direcció de la ratxa màx. diària de vent 10 m', 'Direcció mitjana diària del vent 10 m (m. 1)', 'Evapotranspiració de referència', 'Humitat relativa mitjana diària', 'Humitat relativa màxima diària + data', 'Humitat relativa mínima diària + data', 'Irradiació solar global diària', 'Precipitació acumulada diària', 'Precipitació acumulada diària (8-8 h)']\n",
      "\n",
      "All weather columns:\n",
      "  - Amplitud tèrmica diària\n",
      "  - Direcció de la ratxa màx. diària de vent 10 m\n",
      "  - Direcció mitjana diària del vent 10 m (m. 1)\n",
      "  - Evapotranspiració de referència\n",
      "  - Humitat relativa mitjana diària\n",
      "  - Humitat relativa màxima diària + data\n",
      "  - Humitat relativa mínima diària + data\n",
      "  - Irradiació solar global diària\n",
      "  - Precipitació acumulada diària\n",
      "  - Precipitació acumulada diària (8-8 h)\n",
      "  - Precipitació màxima en 1 h (diària) + hora\n",
      "  - Precipitació màxima en 1 min (diària) + hora\n",
      "  - Precipitació màxima en 30 min (diària)+ hora\n",
      "  - Pressió atmosfèrica mitjana diària\n",
      "  - Pressió atmosfèrica màxima diària + hora\n",
      "  - Pressió atmosfèrica mínima diària + hora\n",
      "  - Ratxa màxima diària del vent 10 m + hora\n",
      "  - Temperatura mitjana diària\n",
      "  - Temperatura mitjana diària clàssica\n",
      "  - Temperatura màxima diària + hora\n",
      "  - Temperatura mínima diària + hora\n",
      "  - Velocitat mitjana diària del vent 10 m (esc.)\n"
     ]
    }
   ],
   "source": [
    "# Prepare weather columns list for vulnerability calculation\n",
    "# Get all weather variable columns (exclude non-weather columns)\n",
    "non_weather_cols = ['SECCIO_CENSAL', 'DATA_LECTURA', 'CODI_ESTACIO', 'geometry', 'centroid', \n",
    "                    'centroid_lat', 'centroid_lon', 'dist_m', 'ist', 'CONSUMO_TOTAL', 'NUM_FUITES',\n",
    "                    'codi_districte', 'nom_districte', 'codi_barri', 'nom_barri', 'codi_aeb', \n",
    "                    'codi_seccio_censal', 'NUM_CONTRACTS', 'CONSUMO_POR_CONTRATO', 'CONSUMO_PROMEDIO', \n",
    "                    'CONSUMO_MEDIANA', 'NUM_CONTRATOS_CON_FUITES', 'FUITES_MAX_POR_CONTRATO', \n",
    "                    'FUITES_PROMEDIO_POR_CONTRATO', 'DATA_LECTURA_dt', 'month']\n",
    "\n",
    "# Get all columns that are weather variables (from the pivot operation)\n",
    "# These are columns that aren't in the non-weather list\n",
    "weather_cols = [col for col in gdf_daily.columns if col not in non_weather_cols]\n",
    "\n",
    "# Filter to only columns that contain numeric weather data (exclude text/object columns)\n",
    "weather_cols = [col for col in weather_cols if gdf_daily[col].dtype in ['float64', 'int64', 'float32', 'int32']]\n",
    "\n",
    "print(f\"Found {len(weather_cols)} weather variable columns\")\n",
    "print(f\"Sample weather columns: {weather_cols[:10]}\")\n",
    "print(f\"\\nAll weather columns:\")\n",
    "for col in sorted(weather_cols):\n",
    "    print(f\"  - {col}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d26f0e7",
   "metadata": {},
   "source": [
    "### Current Structure of `gdf_daily`\n",
    "\n",
    "At this point, `gdf_daily` contains:\n",
    "\n",
    "**Rows (Observations):**\n",
    "- One row per **census section** per **date** (from 2023-01-04 to 2024-12-31)\n",
    "- Total rows = Number of census sections (1068) × Number of dates in the filtered period\n",
    "- Each row represents a unique combination of a census section and a date\n",
    "\n",
    "**Columns (Variables):**\n",
    "\n",
    "1. **Census Section Information:**\n",
    "   - `SECCIO_CENSAL`: Unique identifier (format: 080193 + district + section)\n",
    "   - `codi_districte`, `nom_districte`: District codes and names\n",
    "   - `codi_barri`, `nom_barri`: Neighborhood codes and names\n",
    "   - `codi_aeb`, `codi_seccio_censal`: Additional identifiers\n",
    "   - `geometry`: Polygon geometry of the census section\n",
    "   - `centroid`, `centroid_lat`, `centroid_lon`: Geographic centroids\n",
    "\n",
    "2. **Weather Station Assignment:**\n",
    "   - `CODI_ESTACIO`: Assigned weather station code (D5, X4, or X8)\n",
    "   - `dist_m`: Distance to nearest weather station (in meters)\n",
    "\n",
    "3. **Weather Variables (24 columns):**\n",
    "   - `DATA_LECTURA`: Date of the weather reading\n",
    "   - All 24 weather variables from the weather stations (temperature, precipitation, humidity, pressure, wind, etc.)\n",
    "\n",
    "**Next Steps:**\n",
    "We will now merge consumption, leak incidents, and socioeconomic data to enrich this dataset further.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff780c84",
   "metadata": {},
   "source": [
    "## 7. Merge Additional Data Sources\n",
    "\n",
    "Now we'll merge consumption, leak incidents, and socioeconomic data with the weather-enriched census sections. Each dataset needs to be aggregated appropriately to match the daily structure of `gdf_daily`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4594db73",
   "metadata": {},
   "source": [
    "### 7.1 Load and Merge Socioeconomic Data (IST)\n",
    "\n",
    "The socioeconomic data (IST - Índex socioeconòmic territorial) is a static factor that we will keep constant across years (for now) for each census section. We'll load it and merge directly by `SECCIO_CENSAL`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "545babc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Socioeconomic data shape: (1068, 4)\n",
      "Years: [2022]\n",
      "Number of unique census sections: 1068\n",
      "\n",
      "Sample socioeconomic data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>any</th>\n",
       "      <th>SECCIO_CENSAL</th>\n",
       "      <th>concepte</th>\n",
       "      <th>valor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022</td>\n",
       "      <td>08019301001</td>\n",
       "      <td>Índex socioeconòmic territorial</td>\n",
       "      <td>85.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022</td>\n",
       "      <td>08019301002</td>\n",
       "      <td>Índex socioeconòmic territorial</td>\n",
       "      <td>75.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022</td>\n",
       "      <td>08019301003</td>\n",
       "      <td>Índex socioeconòmic territorial</td>\n",
       "      <td>73.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022</td>\n",
       "      <td>08019301004</td>\n",
       "      <td>Índex socioeconòmic territorial</td>\n",
       "      <td>81.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022</td>\n",
       "      <td>08019301005</td>\n",
       "      <td>Índex socioeconòmic territorial</td>\n",
       "      <td>79.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    any SECCIO_CENSAL                         concepte  valor\n",
       "0  2022   08019301001  Índex socioeconòmic territorial   85.7\n",
       "1  2022   08019301002  Índex socioeconòmic territorial   75.8\n",
       "2  2022   08019301003  Índex socioeconòmic territorial   73.7\n",
       "3  2022   08019301004  Índex socioeconòmic territorial   81.8\n",
       "4  2022   08019301005  Índex socioeconòmic territorial   79.1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load socioeconomic data\n",
    "socio = pd.read_parquet(\"clean/socio_clean.parquet\")\n",
    "\n",
    "print(f\"Socioeconomic data shape: {socio.shape}\")\n",
    "print(f\"Years: {sorted(socio['any'].unique())}\")\n",
    "print(f\"Number of unique census sections: {socio['SECCIO_CENSAL'].nunique()}\")\n",
    "print(f\"\\nSample socioeconomic data:\")\n",
    "socio.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bc2206dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Socioeconomic IST shape: (1068, 2)\n",
      "Number of unique census sections: 1068\n",
      "\n",
      "IST statistics:\n",
      "count    1068.000000\n",
      "mean      108.917603\n",
      "std        14.578281\n",
      "min        51.700000\n",
      "25%       101.200000\n",
      "50%       111.100000\n",
      "75%       118.200000\n",
      "max       138.000000\n",
      "Name: ist, dtype: float64\n",
      "\n",
      "Sample IST data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SECCIO_CENSAL</th>\n",
       "      <th>ist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>08019301001</td>\n",
       "      <td>85.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>08019301002</td>\n",
       "      <td>75.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>08019301003</td>\n",
       "      <td>73.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>08019301004</td>\n",
       "      <td>81.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>08019301005</td>\n",
       "      <td>79.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SECCIO_CENSAL   ist\n",
       "0   08019301001  85.7\n",
       "1   08019301002  75.8\n",
       "2   08019301003  73.7\n",
       "3   08019301004  81.8\n",
       "4   08019301005  79.1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since IST is static (constant across years), we'll take one value per SECCIO_CENSAL\n",
    "# Filter to get the IST value (concepte = \"Índex socioeconòmic territorial\")\n",
    "socio_ist = socio[socio['concepte'] == 'Índex socioeconòmic territorial'].copy()\n",
    "\n",
    "# Select SECCIO_CENSAL and valor, rename valor to ist\n",
    "socio_ist = socio_ist[['SECCIO_CENSAL', 'valor']].copy()\n",
    "socio_ist = socio_ist.rename(columns={'valor': 'ist'})\n",
    "\n",
    "# If there are multiple years, take the most recent one (or first if all same)\n",
    "# Group by SECCIO_CENSAL and take the first value (they should all be the same anyway)\n",
    "socio_ist = socio_ist.groupby('SECCIO_CENSAL')['ist'].first().reset_index()\n",
    "\n",
    "print(f\"Socioeconomic IST shape: {socio_ist.shape}\")\n",
    "print(f\"Number of unique census sections: {socio_ist['SECCIO_CENSAL'].nunique()}\")\n",
    "print(f\"\\nIST statistics:\")\n",
    "print(socio_ist['ist'].describe())\n",
    "print(f\"\\nSample IST data:\")\n",
    "socio_ist.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ab8d4c",
   "metadata": {},
   "source": [
    "### 7.2 Load and Aggregate Consumption Data\n",
    "\n",
    "Consumption data is split across multiple parquet files. We'll load all files, aggregate by `SECCIO_CENSAL` and `FECHA` (date), and calculate daily consumption metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3d18cbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18 consumption files\n",
      "\n",
      "Total consumption records (after filtering): 5014350\n",
      "Date range: 2023-01-04 00:00:00 to 2024-12-31 00:00:00\n",
      "Number of unique census sections: 621\n",
      "\n",
      "Sample consumption data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POLIZA_SUMINISTRO</th>\n",
       "      <th>FECHA</th>\n",
       "      <th>CONSUMO_REAL</th>\n",
       "      <th>SECCIO_CENSAL</th>\n",
       "      <th>US_AIGUA_GEST</th>\n",
       "      <th>DATA_INST_COMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>VECWAVDUULZDSBOP</td>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>2070</td>\n",
       "      <td>08019303025</td>\n",
       "      <td>C</td>\n",
       "      <td>2016-04-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>VECWAVDUULZDSBOP</td>\n",
       "      <td>2023-01-05</td>\n",
       "      <td>1938</td>\n",
       "      <td>08019303025</td>\n",
       "      <td>C</td>\n",
       "      <td>2016-04-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>VECWAVDUULZDSBOP</td>\n",
       "      <td>2023-01-06</td>\n",
       "      <td>4</td>\n",
       "      <td>08019303025</td>\n",
       "      <td>C</td>\n",
       "      <td>2016-04-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>VECWAVDUULZDSBOP</td>\n",
       "      <td>2023-01-07</td>\n",
       "      <td>53</td>\n",
       "      <td>08019303025</td>\n",
       "      <td>C</td>\n",
       "      <td>2016-04-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>VECWAVDUULZDSBOP</td>\n",
       "      <td>2023-01-08</td>\n",
       "      <td>7</td>\n",
       "      <td>08019303025</td>\n",
       "      <td>C</td>\n",
       "      <td>2016-04-25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    POLIZA_SUMINISTRO      FECHA  CONSUMO_REAL SECCIO_CENSAL US_AIGUA_GEST  \\\n",
       "730  VECWAVDUULZDSBOP 2023-01-04          2070   08019303025             C   \n",
       "731  VECWAVDUULZDSBOP 2023-01-05          1938   08019303025             C   \n",
       "732  VECWAVDUULZDSBOP 2023-01-06             4   08019303025             C   \n",
       "733  VECWAVDUULZDSBOP 2023-01-07            53   08019303025             C   \n",
       "734  VECWAVDUULZDSBOP 2023-01-08             7   08019303025             C   \n",
       "\n",
       "    DATA_INST_COMP  \n",
       "730     2016-04-25  \n",
       "731     2016-04-25  \n",
       "732     2016-04-25  \n",
       "733     2016-04-25  \n",
       "734     2016-04-25  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "# Load all consumption parquet files\n",
    "consum_files = glob.glob(\"clean/split_consum_bcn/consum_clean_bcn_part_*.parquet\")\n",
    "consum_files.sort()  # Ensure consistent order\n",
    "\n",
    "print(f\"Found {len(consum_files)} consumption files\")\n",
    "\n",
    "# Load and concatenate all consumption files\n",
    "consum_list = []\n",
    "for file in consum_files:\n",
    "    df = pd.read_parquet(file)\n",
    "    consum_list.append(df)\n",
    "\n",
    "consum = pd.concat(consum_list, ignore_index=True)\n",
    "\n",
    "# Filter to start at 2023-01-04 (temporal overlap with weather and leaks)\n",
    "date_start = pd.to_datetime('2023-01-04')\n",
    "consum = consum[pd.to_datetime(consum['FECHA']) >= date_start].copy()\n",
    "\n",
    "print(f\"\\nTotal consumption records (after filtering): {len(consum)}\")\n",
    "print(f\"Date range: {consum['FECHA'].min()} to {consum['FECHA'].max()}\")\n",
    "print(f\"Number of unique census sections: {consum['SECCIO_CENSAL'].nunique()}\")\n",
    "print(f\"\\nSample consumption data:\")\n",
    "consum.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "672fa5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consumption daily shape: (450195, 7)\n",
      "Date range: 2023-01-04 00:00:00 to 2024-12-31 00:00:00\n",
      "\n",
      "Consumption metrics summary:\n",
      "  Total consumption range: 0 - 466910 L/day\n",
      "  Contracts per section range: 1 - 256\n",
      "  Consumption per contract range: 0.00 - 55678.00 L/day/contract\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SECCIO_CENSAL</th>\n",
       "      <th>DATA_LECTURA</th>\n",
       "      <th>CONSUMO_TOTAL</th>\n",
       "      <th>NUM_CONTRACTS</th>\n",
       "      <th>CONSUMO_PROMEDIO</th>\n",
       "      <th>CONSUMO_MEDIANA</th>\n",
       "      <th>CONSUMO_POR_CONTRATO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>08019301001</td>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>4948</td>\n",
       "      <td>15</td>\n",
       "      <td>329.866667</td>\n",
       "      <td>330.0</td>\n",
       "      <td>329.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>08019301001</td>\n",
       "      <td>2023-01-05</td>\n",
       "      <td>5259</td>\n",
       "      <td>15</td>\n",
       "      <td>350.600000</td>\n",
       "      <td>338.0</td>\n",
       "      <td>350.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>08019301001</td>\n",
       "      <td>2023-01-06</td>\n",
       "      <td>5006</td>\n",
       "      <td>15</td>\n",
       "      <td>333.733333</td>\n",
       "      <td>313.0</td>\n",
       "      <td>333.733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>08019301001</td>\n",
       "      <td>2023-01-07</td>\n",
       "      <td>6301</td>\n",
       "      <td>15</td>\n",
       "      <td>420.066667</td>\n",
       "      <td>416.0</td>\n",
       "      <td>420.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>08019301001</td>\n",
       "      <td>2023-01-08</td>\n",
       "      <td>5428</td>\n",
       "      <td>15</td>\n",
       "      <td>361.866667</td>\n",
       "      <td>380.0</td>\n",
       "      <td>361.866667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SECCIO_CENSAL DATA_LECTURA  CONSUMO_TOTAL  NUM_CONTRACTS  CONSUMO_PROMEDIO  \\\n",
       "0   08019301001   2023-01-04           4948             15        329.866667   \n",
       "1   08019301001   2023-01-05           5259             15        350.600000   \n",
       "2   08019301001   2023-01-06           5006             15        333.733333   \n",
       "3   08019301001   2023-01-07           6301             15        420.066667   \n",
       "4   08019301001   2023-01-08           5428             15        361.866667   \n",
       "\n",
       "   CONSUMO_MEDIANA  CONSUMO_POR_CONTRATO  \n",
       "0            330.0            329.866667  \n",
       "1            338.0            350.600000  \n",
       "2            313.0            333.733333  \n",
       "3            416.0            420.066667  \n",
       "4            380.0            361.866667  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregate consumption with multiple metrics to capture different aspects:\n",
    "# 1. Total consumption (demand level)\n",
    "# 2. Number of contracts/meters (infrastructure density)\n",
    "# 3. Average consumption per contract (intensity per meter)\n",
    "# 4. Median consumption per contract (robust to outliers)\n",
    "consum_daily = consum.groupby(['SECCIO_CENSAL', 'FECHA'], as_index=False).agg(\n",
    "    CONSUMO_TOTAL=('CONSUMO_REAL', 'sum'),  # Total consumption (demand)\n",
    "    NUM_CONTRACTS=('POLIZA_SUMINISTRO', 'nunique'),  # Number of unique contracts/meters\n",
    "    CONSUMO_PROMEDIO=('CONSUMO_REAL', 'mean'),  # Average consumption per contract\n",
    "    CONSUMO_MEDIANA=('CONSUMO_REAL', 'median'),  # Median consumption per contract (robust)\n",
    ")\n",
    "\n",
    "# Calculate consumption per contract (intensity metric)\n",
    "consum_daily['CONSUMO_POR_CONTRATO'] = consum_daily['CONSUMO_TOTAL'] / consum_daily['NUM_CONTRACTS'].replace(0, np.nan)\n",
    "\n",
    "# Rename FECHA for consistency\n",
    "consum_daily = consum_daily.rename(columns={'FECHA': 'DATA_LECTURA'})\n",
    "\n",
    "print(f\"Consumption daily shape: {consum_daily.shape}\")\n",
    "print(f\"Date range: {consum_daily['DATA_LECTURA'].min()} to {consum_daily['DATA_LECTURA'].max()}\")\n",
    "print(f\"\\nConsumption metrics summary:\")\n",
    "print(f\"  Total consumption range: {consum_daily['CONSUMO_TOTAL'].min():.0f} - {consum_daily['CONSUMO_TOTAL'].max():.0f} L/day\")\n",
    "print(f\"  Contracts per section range: {consum_daily['NUM_CONTRACTS'].min():.0f} - {consum_daily['NUM_CONTRACTS'].max():.0f}\")\n",
    "print(f\"  Consumption per contract range: {consum_daily['CONSUMO_POR_CONTRATO'].min():.2f} - {consum_daily['CONSUMO_POR_CONTRATO'].max():.2f} L/day/contract\")\n",
    "consum_daily.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895fb219",
   "metadata": {},
   "source": [
    "### 7.3 Load and Aggregate Leak Incidents Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c421ff32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leak incidents shape (after filtering): (1243, 5)\n",
      "Date range: 2023-01-04 to 2024-12-30\n",
      "Number of unique census sections: 428\n",
      "\n",
      "Sample leak data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POLISSA_SUBM</th>\n",
       "      <th>CREATED_MENSAJE</th>\n",
       "      <th>CODIGO_MENSAJE</th>\n",
       "      <th>US_AIGUA_SUBM</th>\n",
       "      <th>SECCIO_CENSAL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KWHZ5UG2ZKENUFC2</td>\n",
       "      <td>2023-12-03</td>\n",
       "      <td>FUITA</td>\n",
       "      <td>DOMÈSTIC</td>\n",
       "      <td>08019305059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GVXPU34GVXQUIWFK</td>\n",
       "      <td>2023-08-10</td>\n",
       "      <td>FUITA</td>\n",
       "      <td>DOMÈSTIC</td>\n",
       "      <td>08019310139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GVXPU34GVXQUIWFK</td>\n",
       "      <td>2023-06-10</td>\n",
       "      <td>FUITA</td>\n",
       "      <td>DOMÈSTIC</td>\n",
       "      <td>08019310139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I7GGTJ6C6FMR5ARW</td>\n",
       "      <td>2024-09-06</td>\n",
       "      <td>FUITA</td>\n",
       "      <td>DOMÈSTIC</td>\n",
       "      <td>08019302087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I7GGTJ6C6FMR5ARW</td>\n",
       "      <td>2024-11-13</td>\n",
       "      <td>FUITA</td>\n",
       "      <td>DOMÈSTIC</td>\n",
       "      <td>08019302087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       POLISSA_SUBM CREATED_MENSAJE CODIGO_MENSAJE US_AIGUA_SUBM SECCIO_CENSAL\n",
       "0  KWHZ5UG2ZKENUFC2      2023-12-03          FUITA      DOMÈSTIC   08019305059\n",
       "1  GVXPU34GVXQUIWFK      2023-08-10          FUITA      DOMÈSTIC   08019310139\n",
       "2  GVXPU34GVXQUIWFK      2023-06-10          FUITA      DOMÈSTIC   08019310139\n",
       "3  I7GGTJ6C6FMR5ARW      2024-09-06          FUITA      DOMÈSTIC   08019302087\n",
       "4  I7GGTJ6C6FMR5ARW      2024-11-13          FUITA      DOMÈSTIC   08019302087"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load leak incidents data\n",
    "leaks = pd.read_parquet(\"clean/fuites_clean_bcn.parquet\")\n",
    "\n",
    "# Filter to date range: 2023-01-04 to 2024-12-31 (temporal overlap with weather and consumption)\n",
    "date_start = pd.to_datetime('2023-01-04')\n",
    "date_end = pd.to_datetime('2024-12-31')\n",
    "leaks = leaks[\n",
    "    (pd.to_datetime(leaks['CREATED_MENSAJE']) >= date_start) & \n",
    "    (pd.to_datetime(leaks['CREATED_MENSAJE']) <= date_end)\n",
    "].copy()\n",
    "\n",
    "print(f\"Leak incidents shape (after filtering): {leaks.shape}\")\n",
    "print(f\"Date range: {leaks['CREATED_MENSAJE'].min()} to {leaks['CREATED_MENSAJE'].max()}\")\n",
    "print(f\"Number of unique census sections: {leaks['SECCIO_CENSAL'].nunique()}\")\n",
    "\n",
    "# Note: Not all dates will have leaks - this is normal\n",
    "# When merged with gdf_daily, days without leaks will have NUM_FUITES = 0\n",
    "\n",
    "print(f\"\\nSample leak data:\")\n",
    "leaks.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0dd3fca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leaks daily shape: (1235, 6)\n",
      "Date range: 2023-01-04 00:00:00 to 2024-12-30 00:00:00\n",
      "\n",
      "Leak metrics summary:\n",
      "  Total leaks per day range: 1 - 2\n",
      "  Contracts with leaks per day range: 1 - 2\n",
      "  Max leaks per contract range: 1 - 1\n",
      "  Average leaks per contract range: 1.00 - 1.00\n",
      "\n",
      "Example: Days with multiple leaks from different contracts:\n",
      "    SECCIO_CENSAL DATA_LECTURA  NUM_FUITES  NUM_CONTRATOS_CON_FUITES  \\\n",
      "44    08019301020   2024-05-07           2                         2   \n",
      "55    08019301021   2023-11-08           2                         2   \n",
      "56    08019301021   2024-01-10           2                         2   \n",
      "465   08019303025   2024-12-18           2                         2   \n",
      "685   08019305025   2024-07-18           2                         2   \n",
      "\n",
      "     FUITES_MAX_POR_CONTRATO  FUITES_PROMEDIO_POR_CONTRATO  \n",
      "44                         1                           1.0  \n",
      "55                         1                           1.0  \n",
      "56                         1                           1.0  \n",
      "465                        1                           1.0  \n",
      "685                        1                           1.0  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SECCIO_CENSAL</th>\n",
       "      <th>DATA_LECTURA</th>\n",
       "      <th>NUM_FUITES</th>\n",
       "      <th>NUM_CONTRATOS_CON_FUITES</th>\n",
       "      <th>FUITES_MAX_POR_CONTRATO</th>\n",
       "      <th>FUITES_PROMEDIO_POR_CONTRATO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>08019301002</td>\n",
       "      <td>2024-06-19</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>08019301002</td>\n",
       "      <td>2024-08-16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>08019301004</td>\n",
       "      <td>2023-03-22</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>08019301004</td>\n",
       "      <td>2024-03-04</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>08019301004</td>\n",
       "      <td>2024-06-25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SECCIO_CENSAL DATA_LECTURA  NUM_FUITES  NUM_CONTRATOS_CON_FUITES  \\\n",
       "0   08019301002   2024-06-19           1                         1   \n",
       "1   08019301002   2024-08-16           1                         1   \n",
       "2   08019301004   2023-03-22           1                         1   \n",
       "3   08019301004   2024-03-04           1                         1   \n",
       "4   08019301004   2024-06-25           1                         1   \n",
       "\n",
       "   FUITES_MAX_POR_CONTRATO  FUITES_PROMEDIO_POR_CONTRATO  \n",
       "0                        1                           1.0  \n",
       "1                        1                           1.0  \n",
       "2                        1                           1.0  \n",
       "3                        1                           1.0  \n",
       "4                        1                           1.0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert CREATED_MENSAJE to datetime if needed\n",
    "leaks['CREATED_MENSAJE'] = pd.to_datetime(leaks['CREATED_MENSAJE'])\n",
    "\n",
    "# Aggregate leak incidents with multiple metrics to capture different aspects:\n",
    "# 1. Total number of leak incidents (NUM_FUITES) - overall leak count\n",
    "# 2. Number of unique contracts with leaks (NUM_CONTRATOS_CON_FUITES) - leak density\n",
    "# 3. Maximum leaks from a single contract (FUITES_MAX_POR_CONTRATO) - indicates severity\n",
    "# 4. Average leaks per contract (FUITES_PROMEDIO_POR_CONTRATO) - intensity metric\n",
    "\n",
    "# First, count leaks per contract per day to get contract-level metrics\n",
    "leaks_by_contract = leaks.groupby(['SECCIO_CENSAL', 'CREATED_MENSAJE', 'POLISSA_SUBM'], as_index=False).agg(\n",
    "    LEAKS_PER_CONTRACT=('POLISSA_SUBM', 'count')  # Count leaks per contract per day\n",
    ")\n",
    "\n",
    "# Then aggregate to census section level with multiple metrics\n",
    "leaks_daily = leaks.groupby(['SECCIO_CENSAL', 'CREATED_MENSAJE'], as_index=False).agg(\n",
    "    NUM_FUITES=('POLISSA_SUBM', 'count'),  # Total leak incidents (rows)\n",
    "    NUM_CONTRATOS_CON_FUITES=('POLISSA_SUBM', 'nunique'),  # Number of unique contracts with leaks\n",
    ")\n",
    "\n",
    "# Merge with contract-level metrics to get max leaks per contract\n",
    "max_leaks_per_contract = leaks_by_contract.groupby(['SECCIO_CENSAL', 'CREATED_MENSAJE'], as_index=False).agg(\n",
    "    FUITES_MAX_POR_CONTRATO=('LEAKS_PER_CONTRACT', 'max')  # Maximum leaks from any single contract\n",
    ")\n",
    "\n",
    "leaks_daily = leaks_daily.merge(max_leaks_per_contract, on=['SECCIO_CENSAL', 'CREATED_MENSAJE'], how='left')\n",
    "\n",
    "# Calculate average leaks per contract (intensity metric)\n",
    "leaks_daily['FUITES_PROMEDIO_POR_CONTRATO'] = leaks_daily['NUM_FUITES'] / leaks_daily['NUM_CONTRATOS_CON_FUITES'].replace(0, np.nan)\n",
    "\n",
    "# Fill NaN values for days with no leaks\n",
    "leaks_daily['NUM_CONTRATOS_CON_FUITES'] = leaks_daily['NUM_CONTRATOS_CON_FUITES'].fillna(0).astype(int)\n",
    "leaks_daily['FUITES_MAX_POR_CONTRATO'] = leaks_daily['FUITES_MAX_POR_CONTRATO'].fillna(0).astype(int)\n",
    "leaks_daily['FUITES_PROMEDIO_POR_CONTRATO'] = leaks_daily['FUITES_PROMEDIO_POR_CONTRATO'].fillna(0)\n",
    "\n",
    "# Rename for consistency\n",
    "leaks_daily = leaks_daily.rename(columns={'CREATED_MENSAJE': 'DATA_LECTURA'})\n",
    "\n",
    "print(f\"Leaks daily shape: {leaks_daily.shape}\")\n",
    "print(f\"Date range: {leaks_daily['DATA_LECTURA'].min()} to {leaks_daily['DATA_LECTURA'].max()}\")\n",
    "print(f\"\\nLeak metrics summary:\")\n",
    "print(f\"  Total leaks per day range: {leaks_daily['NUM_FUITES'].min():.0f} - {leaks_daily['NUM_FUITES'].max():.0f}\")\n",
    "print(f\"  Contracts with leaks per day range: {leaks_daily['NUM_CONTRATOS_CON_FUITES'].min():.0f} - {leaks_daily['NUM_CONTRATOS_CON_FUITES'].max():.0f}\")\n",
    "print(f\"  Max leaks per contract range: {leaks_daily['FUITES_MAX_POR_CONTRATO'].min():.0f} - {leaks_daily['FUITES_MAX_POR_CONTRATO'].max():.0f}\")\n",
    "print(f\"  Average leaks per contract range: {leaks_daily['FUITES_PROMEDIO_POR_CONTRATO'].min():.2f} - {leaks_daily['FUITES_PROMEDIO_POR_CONTRATO'].max():.2f}\")\n",
    "print(f\"\\nExample: Days with multiple leaks from different contracts:\")\n",
    "print(leaks_daily[leaks_daily['NUM_FUITES'] > 1].head())\n",
    "leaks_daily.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9a0912",
   "metadata": {},
   "source": [
    "### 7.4 Merge All Data Sources\n",
    "\n",
    "Now we'll merge consumption, leaks, and socioeconomic data with the weather-enriched `gdf_daily` GeoDataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7dadfb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After IST merge: (777504, 38)\n",
      "IST records matched: 777504\n",
      "\n",
      "After consumption merge: (777504, 43)\n",
      "Consumption records matched: 450195\n",
      "Consumption metrics available: CONSUMO_TOTAL, NUM_CONTRACTS, CONSUMO_POR_CONTRATO, CONSUMO_PROMEDIO, CONSUMO_MEDIANA\n",
      "\n",
      "After leaks merge: (777504, 47)\n",
      "Days with leaks: 1235\n",
      "Leak metrics initialized: NUM_FUITES, NUM_CONTRATOS_CON_FUITES, FUITES_MAX_POR_CONTRATO, FUITES_PROMEDIO_POR_CONTRATO\n",
      "\n",
      "Final columns: 47\n",
      "\n",
      "Sample of final merged data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SECCIO_CENSAL</th>\n",
       "      <th>codi_districte</th>\n",
       "      <th>nom_districte</th>\n",
       "      <th>codi_barri</th>\n",
       "      <th>nom_barri</th>\n",
       "      <th>codi_aeb</th>\n",
       "      <th>codi_seccio_censal</th>\n",
       "      <th>geometria_etrs89</th>\n",
       "      <th>DATA_LECTURA</th>\n",
       "      <th>CODI_ESTACIO</th>\n",
       "      <th>...</th>\n",
       "      <th>ist</th>\n",
       "      <th>CONSUMO_TOTAL</th>\n",
       "      <th>NUM_CONTRACTS</th>\n",
       "      <th>CONSUMO_PROMEDIO</th>\n",
       "      <th>CONSUMO_MEDIANA</th>\n",
       "      <th>CONSUMO_POR_CONTRATO</th>\n",
       "      <th>NUM_FUITES</th>\n",
       "      <th>NUM_CONTRATOS_CON_FUITES</th>\n",
       "      <th>FUITES_MAX_POR_CONTRATO</th>\n",
       "      <th>FUITES_PROMEDIO_POR_CONTRATO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>08019301001</td>\n",
       "      <td>01</td>\n",
       "      <td>Ciutat Vella</td>\n",
       "      <td>01</td>\n",
       "      <td>el Raval</td>\n",
       "      <td>001</td>\n",
       "      <td>001</td>\n",
       "      <td>POLYGON ((431076.9025 4581077.3095, 431058.164...</td>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>X4</td>\n",
       "      <td>...</td>\n",
       "      <td>85.7</td>\n",
       "      <td>4948.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>329.866667</td>\n",
       "      <td>330.0</td>\n",
       "      <td>329.866667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>08019301001</td>\n",
       "      <td>01</td>\n",
       "      <td>Ciutat Vella</td>\n",
       "      <td>01</td>\n",
       "      <td>el Raval</td>\n",
       "      <td>001</td>\n",
       "      <td>001</td>\n",
       "      <td>POLYGON ((431076.9025 4581077.3095, 431058.164...</td>\n",
       "      <td>2023-01-05</td>\n",
       "      <td>X4</td>\n",
       "      <td>...</td>\n",
       "      <td>85.7</td>\n",
       "      <td>5259.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>350.600000</td>\n",
       "      <td>338.0</td>\n",
       "      <td>350.600000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>08019301001</td>\n",
       "      <td>01</td>\n",
       "      <td>Ciutat Vella</td>\n",
       "      <td>01</td>\n",
       "      <td>el Raval</td>\n",
       "      <td>001</td>\n",
       "      <td>001</td>\n",
       "      <td>POLYGON ((431076.9025 4581077.3095, 431058.164...</td>\n",
       "      <td>2023-01-06</td>\n",
       "      <td>X4</td>\n",
       "      <td>...</td>\n",
       "      <td>85.7</td>\n",
       "      <td>5006.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>333.733333</td>\n",
       "      <td>313.0</td>\n",
       "      <td>333.733333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>08019301001</td>\n",
       "      <td>01</td>\n",
       "      <td>Ciutat Vella</td>\n",
       "      <td>01</td>\n",
       "      <td>el Raval</td>\n",
       "      <td>001</td>\n",
       "      <td>001</td>\n",
       "      <td>POLYGON ((431076.9025 4581077.3095, 431058.164...</td>\n",
       "      <td>2023-01-07</td>\n",
       "      <td>X4</td>\n",
       "      <td>...</td>\n",
       "      <td>85.7</td>\n",
       "      <td>6301.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>420.066667</td>\n",
       "      <td>416.0</td>\n",
       "      <td>420.066667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>08019301001</td>\n",
       "      <td>01</td>\n",
       "      <td>Ciutat Vella</td>\n",
       "      <td>01</td>\n",
       "      <td>el Raval</td>\n",
       "      <td>001</td>\n",
       "      <td>001</td>\n",
       "      <td>POLYGON ((431076.9025 4581077.3095, 431058.164...</td>\n",
       "      <td>2023-01-08</td>\n",
       "      <td>X4</td>\n",
       "      <td>...</td>\n",
       "      <td>85.7</td>\n",
       "      <td>5428.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>361.866667</td>\n",
       "      <td>380.0</td>\n",
       "      <td>361.866667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>08019301001</td>\n",
       "      <td>01</td>\n",
       "      <td>Ciutat Vella</td>\n",
       "      <td>01</td>\n",
       "      <td>el Raval</td>\n",
       "      <td>001</td>\n",
       "      <td>001</td>\n",
       "      <td>POLYGON ((431076.9025 4581077.3095, 431058.164...</td>\n",
       "      <td>2023-01-09</td>\n",
       "      <td>X4</td>\n",
       "      <td>...</td>\n",
       "      <td>85.7</td>\n",
       "      <td>4970.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>331.333333</td>\n",
       "      <td>368.0</td>\n",
       "      <td>331.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>08019301001</td>\n",
       "      <td>01</td>\n",
       "      <td>Ciutat Vella</td>\n",
       "      <td>01</td>\n",
       "      <td>el Raval</td>\n",
       "      <td>001</td>\n",
       "      <td>001</td>\n",
       "      <td>POLYGON ((431076.9025 4581077.3095, 431058.164...</td>\n",
       "      <td>2023-01-10</td>\n",
       "      <td>X4</td>\n",
       "      <td>...</td>\n",
       "      <td>85.7</td>\n",
       "      <td>4487.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>299.133333</td>\n",
       "      <td>330.0</td>\n",
       "      <td>299.133333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>08019301001</td>\n",
       "      <td>01</td>\n",
       "      <td>Ciutat Vella</td>\n",
       "      <td>01</td>\n",
       "      <td>el Raval</td>\n",
       "      <td>001</td>\n",
       "      <td>001</td>\n",
       "      <td>POLYGON ((431076.9025 4581077.3095, 431058.164...</td>\n",
       "      <td>2023-01-11</td>\n",
       "      <td>X4</td>\n",
       "      <td>...</td>\n",
       "      <td>85.7</td>\n",
       "      <td>4972.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>331.466667</td>\n",
       "      <td>292.0</td>\n",
       "      <td>331.466667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>08019301001</td>\n",
       "      <td>01</td>\n",
       "      <td>Ciutat Vella</td>\n",
       "      <td>01</td>\n",
       "      <td>el Raval</td>\n",
       "      <td>001</td>\n",
       "      <td>001</td>\n",
       "      <td>POLYGON ((431076.9025 4581077.3095, 431058.164...</td>\n",
       "      <td>2023-01-12</td>\n",
       "      <td>X4</td>\n",
       "      <td>...</td>\n",
       "      <td>85.7</td>\n",
       "      <td>5323.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>354.866667</td>\n",
       "      <td>348.0</td>\n",
       "      <td>354.866667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>08019301001</td>\n",
       "      <td>01</td>\n",
       "      <td>Ciutat Vella</td>\n",
       "      <td>01</td>\n",
       "      <td>el Raval</td>\n",
       "      <td>001</td>\n",
       "      <td>001</td>\n",
       "      <td>POLYGON ((431076.9025 4581077.3095, 431058.164...</td>\n",
       "      <td>2023-01-13</td>\n",
       "      <td>X4</td>\n",
       "      <td>...</td>\n",
       "      <td>85.7</td>\n",
       "      <td>4747.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>316.466667</td>\n",
       "      <td>250.0</td>\n",
       "      <td>316.466667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  SECCIO_CENSAL codi_districte nom_districte codi_barri nom_barri codi_aeb  \\\n",
       "0   08019301001             01  Ciutat Vella         01  el Raval      001   \n",
       "1   08019301001             01  Ciutat Vella         01  el Raval      001   \n",
       "2   08019301001             01  Ciutat Vella         01  el Raval      001   \n",
       "3   08019301001             01  Ciutat Vella         01  el Raval      001   \n",
       "4   08019301001             01  Ciutat Vella         01  el Raval      001   \n",
       "5   08019301001             01  Ciutat Vella         01  el Raval      001   \n",
       "6   08019301001             01  Ciutat Vella         01  el Raval      001   \n",
       "7   08019301001             01  Ciutat Vella         01  el Raval      001   \n",
       "8   08019301001             01  Ciutat Vella         01  el Raval      001   \n",
       "9   08019301001             01  Ciutat Vella         01  el Raval      001   \n",
       "\n",
       "  codi_seccio_censal                                   geometria_etrs89  \\\n",
       "0                001  POLYGON ((431076.9025 4581077.3095, 431058.164...   \n",
       "1                001  POLYGON ((431076.9025 4581077.3095, 431058.164...   \n",
       "2                001  POLYGON ((431076.9025 4581077.3095, 431058.164...   \n",
       "3                001  POLYGON ((431076.9025 4581077.3095, 431058.164...   \n",
       "4                001  POLYGON ((431076.9025 4581077.3095, 431058.164...   \n",
       "5                001  POLYGON ((431076.9025 4581077.3095, 431058.164...   \n",
       "6                001  POLYGON ((431076.9025 4581077.3095, 431058.164...   \n",
       "7                001  POLYGON ((431076.9025 4581077.3095, 431058.164...   \n",
       "8                001  POLYGON ((431076.9025 4581077.3095, 431058.164...   \n",
       "9                001  POLYGON ((431076.9025 4581077.3095, 431058.164...   \n",
       "\n",
       "  DATA_LECTURA CODI_ESTACIO  ...   ist  CONSUMO_TOTAL  NUM_CONTRACTS  \\\n",
       "0   2023-01-04           X4  ...  85.7         4948.0           15.0   \n",
       "1   2023-01-05           X4  ...  85.7         5259.0           15.0   \n",
       "2   2023-01-06           X4  ...  85.7         5006.0           15.0   \n",
       "3   2023-01-07           X4  ...  85.7         6301.0           15.0   \n",
       "4   2023-01-08           X4  ...  85.7         5428.0           15.0   \n",
       "5   2023-01-09           X4  ...  85.7         4970.0           15.0   \n",
       "6   2023-01-10           X4  ...  85.7         4487.0           15.0   \n",
       "7   2023-01-11           X4  ...  85.7         4972.0           15.0   \n",
       "8   2023-01-12           X4  ...  85.7         5323.0           15.0   \n",
       "9   2023-01-13           X4  ...  85.7         4747.0           15.0   \n",
       "\n",
       "   CONSUMO_PROMEDIO  CONSUMO_MEDIANA  CONSUMO_POR_CONTRATO  NUM_FUITES  \\\n",
       "0        329.866667            330.0            329.866667           0   \n",
       "1        350.600000            338.0            350.600000           0   \n",
       "2        333.733333            313.0            333.733333           0   \n",
       "3        420.066667            416.0            420.066667           0   \n",
       "4        361.866667            380.0            361.866667           0   \n",
       "5        331.333333            368.0            331.333333           0   \n",
       "6        299.133333            330.0            299.133333           0   \n",
       "7        331.466667            292.0            331.466667           0   \n",
       "8        354.866667            348.0            354.866667           0   \n",
       "9        316.466667            250.0            316.466667           0   \n",
       "\n",
       "   NUM_CONTRATOS_CON_FUITES  FUITES_MAX_POR_CONTRATO  \\\n",
       "0                         0                        0   \n",
       "1                         0                        0   \n",
       "2                         0                        0   \n",
       "3                         0                        0   \n",
       "4                         0                        0   \n",
       "5                         0                        0   \n",
       "6                         0                        0   \n",
       "7                         0                        0   \n",
       "8                         0                        0   \n",
       "9                         0                        0   \n",
       "\n",
       "   FUITES_PROMEDIO_POR_CONTRATO  \n",
       "0                           0.0  \n",
       "1                           0.0  \n",
       "2                           0.0  \n",
       "3                           0.0  \n",
       "4                           0.0  \n",
       "5                           0.0  \n",
       "6                           0.0  \n",
       "7                           0.0  \n",
       "8                           0.0  \n",
       "9                           0.0  \n",
       "\n",
       "[10 rows x 47 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Merge socioeconomic data (IST) - static factor\n",
    "# Merge on SECCIO_CENSAL only (no date needed since it's constant)\n",
    "gdf_daily = gdf_daily.merge(\n",
    "    socio_ist,\n",
    "    on='SECCIO_CENSAL',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"After IST merge: {gdf_daily.shape}\")\n",
    "print(f\"IST records matched: {gdf_daily['ist'].notna().sum()}\")\n",
    "\n",
    "# Step 2: Merge consumption data\n",
    "# Merge on SECCIO_CENSAL and DATA_LECTURA (date)\n",
    "gdf_daily = gdf_daily.merge(\n",
    "    consum_daily,\n",
    "    on=['SECCIO_CENSAL', 'DATA_LECTURA'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Note: Consumption metrics may be NaN for dates without consumption data\n",
    "# This is expected and will be handled in the vulnerability calculation\n",
    "# We keep NaN values (not filling with 0) since missing consumption data is meaningful\n",
    "\n",
    "print(f\"\\nAfter consumption merge: {gdf_daily.shape}\")\n",
    "print(f\"Consumption records matched: {gdf_daily['CONSUMO_TOTAL'].notna().sum()}\")\n",
    "consum_metrics = ['CONSUMO_TOTAL', 'NUM_CONTRACTS', 'CONSUMO_POR_CONTRATO', 'CONSUMO_PROMEDIO', 'CONSUMO_MEDIANA']\n",
    "consum_metrics_found = [m for m in consum_metrics if m in gdf_daily.columns]\n",
    "print(f\"Consumption metrics available: {', '.join(consum_metrics_found)}\")\n",
    "\n",
    "# Step 3: Merge leak incidents data\n",
    "# Merge on SECCIO_CENSAL and DATA_LECTURA (date)\n",
    "gdf_daily = gdf_daily.merge(\n",
    "    leaks_daily,\n",
    "    on=['SECCIO_CENSAL', 'DATA_LECTURA'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill NaN with 0 for all leak metrics (no leaks = 0)\n",
    "# Using all aggregation metrics from section 7.3\n",
    "leak_metrics = ['NUM_FUITES', 'NUM_CONTRATOS_CON_FUITES', 'FUITES_MAX_POR_CONTRATO', 'FUITES_PROMEDIO_POR_CONTRATO']\n",
    "for metric in leak_metrics:\n",
    "    if metric in gdf_daily.columns:\n",
    "        if metric in ['NUM_FUITES', 'NUM_CONTRATOS_CON_FUITES', 'FUITES_MAX_POR_CONTRATO']:\n",
    "            gdf_daily[metric] = gdf_daily[metric].fillna(0).astype(int)\n",
    "        else:  # FUITES_PROMEDIO_POR_CONTRATO (float)\n",
    "            gdf_daily[metric] = gdf_daily[metric].fillna(0.0)\n",
    "\n",
    "print(f\"\\nAfter leaks merge: {gdf_daily.shape}\")\n",
    "print(f\"Days with leaks: {(gdf_daily['NUM_FUITES'] > 0).sum()}\")\n",
    "print(f\"Leak metrics initialized: {', '.join([m for m in leak_metrics if m in gdf_daily.columns])}\")\n",
    "\n",
    "print(f\"\\nFinal columns: {len(gdf_daily.columns)}\")\n",
    "print(f\"\\nSample of final merged data:\")\n",
    "gdf_daily.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de64564a",
   "metadata": {},
   "source": [
    "## 8. Verify Dataset Structure\n",
    "\n",
    "Let's verify that `gdf_daily` has the expected structure:\n",
    "- 1068 census sections\n",
    "- One row per census section per day from 2023-01-04 to 2024-12-31\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "58303e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dataset Structure Verification ===\n",
      "\n",
      "Expected rows: 777,504\n",
      "Actual rows: 777,504\n",
      "Match: ✓\n",
      "\n",
      "Expected census sections: 1068\n",
      "Actual census sections: 1068\n",
      "Match: ✓\n",
      "\n",
      "Expected days: 728\n",
      "Actual unique dates: 728\n",
      "Match: ✓\n",
      "\n",
      "Expected date range: 2023-01-04 to 2024-12-31\n",
      "Actual date range: 2023-01-04 to 2024-12-31\n",
      "Match: ✓\n",
      "\n",
      "Duplicate rows (SECCIO_CENSAL + DATA_LECTURA): 0\n",
      "No duplicates: ✓\n",
      "\n",
      "✓ Dataset structure is correct: one row per census section per day\n",
      "\n",
      "Sample: First few dates for one census section:\n",
      "  SECCIO_CENSAL DATA_LECTURA\n",
      "0   08019301001   2023-01-04\n",
      "1   08019301001   2023-01-05\n",
      "2   08019301001   2023-01-06\n",
      "3   08019301001   2023-01-07\n",
      "4   08019301001   2023-01-08\n",
      "5   08019301001   2023-01-09\n",
      "6   08019301001   2023-01-10\n",
      "7   08019301001   2023-01-11\n",
      "8   08019301001   2023-01-12\n",
      "9   08019301001   2023-01-13\n"
     ]
    }
   ],
   "source": [
    "# Calculate expected number of rows\n",
    "date_start = pd.to_datetime('2023-01-04')\n",
    "date_end = pd.to_datetime('2024-12-31')\n",
    "expected_days = (date_end - date_start).days + 1  # +1 to include both start and end dates\n",
    "expected_census_sections = 1068\n",
    "expected_rows = expected_census_sections * expected_days\n",
    "\n",
    "# Actual values\n",
    "actual_rows = len(gdf_daily)\n",
    "actual_census_sections = gdf_daily['SECCIO_CENSAL'].nunique()\n",
    "actual_dates = gdf_daily['DATA_LECTURA'].nunique()\n",
    "actual_date_range = (gdf_daily['DATA_LECTURA'].min(), gdf_daily['DATA_LECTURA'].max())\n",
    "\n",
    "# Check for duplicates (should be 0)\n",
    "duplicates = gdf_daily.duplicated(subset=['SECCIO_CENSAL', 'DATA_LECTURA']).sum()\n",
    "\n",
    "print(\"=== Dataset Structure Verification ===\\n\")\n",
    "print(f\"Expected rows: {expected_rows:,}\")\n",
    "print(f\"Actual rows: {actual_rows:,}\")\n",
    "print(f\"Match: {'✓' if actual_rows == expected_rows else '✗'}\\n\")\n",
    "\n",
    "print(f\"Expected census sections: {expected_census_sections}\")\n",
    "print(f\"Actual census sections: {actual_census_sections}\")\n",
    "print(f\"Match: {'✓' if actual_census_sections == expected_census_sections else '✗'}\\n\")\n",
    "\n",
    "print(f\"Expected days: {expected_days}\")\n",
    "print(f\"Actual unique dates: {actual_dates}\")\n",
    "print(f\"Match: {'✓' if actual_dates == expected_days else '✗'}\\n\")\n",
    "\n",
    "print(f\"Expected date range: {date_start.date()} to {date_end.date()}\")\n",
    "print(f\"Actual date range: {actual_date_range[0].date()} to {actual_date_range[1].date()}\")\n",
    "print(f\"Match: {'✓' if actual_date_range[0].date() == date_start.date() and actual_date_range[1].date() == date_end.date() else '✗'}\\n\")\n",
    "\n",
    "print(f\"Duplicate rows (SECCIO_CENSAL + DATA_LECTURA): {duplicates}\")\n",
    "print(f\"No duplicates: {'✓' if duplicates == 0 else '✗'}\\n\")\n",
    "\n",
    "# Check if we have exactly one row per census section per date\n",
    "if actual_rows == expected_rows and duplicates == 0:\n",
    "    print(\"✓ Dataset structure is correct: one row per census section per day\")\n",
    "else:\n",
    "    print(\"✗ Dataset structure needs review\")\n",
    "    \n",
    "# Show sample of date distribution\n",
    "print(f\"\\nSample: First few dates for one census section:\")\n",
    "sample_seccio = gdf_daily['SECCIO_CENSAL'].iloc[0]\n",
    "print(gdf_daily[gdf_daily['SECCIO_CENSAL'] == sample_seccio][['SECCIO_CENSAL', 'DATA_LECTURA']].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e936df",
   "metadata": {},
   "source": [
    "## 9. Load Vulnerability Score Configuration\n",
    "\n",
    "Load the data-driven recommendations from `correlationAnalysis.ipynb` to use recommended weather variables and weights for vulnerability score calculation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a94bfce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded vulnerability score configuration from correlationAnalysis.ipynb\n",
      "  Heatwave variables: 6\n",
      "  Rainfall variables: 5\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Load vulnerability score configuration from correlationAnalysis.ipynb\n",
    "try:\n",
    "    with open('vulnerability_score_config.json', 'r') as f:\n",
    "        vulnerability_score_config = json.load(f)\n",
    "    \n",
    "    print(\"✓ Loaded vulnerability score configuration from correlationAnalysis.ipynb\")\n",
    "    if vulnerability_score_config.get('heatwave'):\n",
    "        print(f\"  Heatwave variables: {len(vulnerability_score_config['heatwave']['variables'])}\")\n",
    "    if vulnerability_score_config.get('rainfall'):\n",
    "        print(f\"  Rainfall variables: {len(vulnerability_score_config['rainfall']['variables'])}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"⚠ Warning: vulnerability_score_config.json not found.\")\n",
    "    print(\"  Please run correlationAnalysis.ipynb first to generate the configuration file.\")\n",
    "    print(\"  Falling back to default hardcoded approach.\")\n",
    "    vulnerability_score_config = None\n",
    "\n",
    "# Helper function to map correlationAnalysis variable names to actual weather column names\n",
    "def map_variable_to_column(var_name, weather_cols):\n",
    "    \"\"\"Map correlationAnalysis variable names to actual weather column names.\"\"\"\n",
    "    mapping = {\n",
    "        'temp_max': 'Temperatura màxima diària + hora',\n",
    "        'temp_avg': 'Temperatura mitjana diària',\n",
    "        'temp_min': 'Temperatura mínima diària + hora',\n",
    "        'temp_avg_classic': 'Temperatura mitjana diària clàssica',\n",
    "        'temp_amplitude': 'Amplitud tèrmica diària',\n",
    "        'solar_global': 'Irradiació solar global diària',\n",
    "        'evapotranspiration': 'Evapotranspiració de referència',\n",
    "        'humidity_avg': 'Humitat relativa mitjana diària',\n",
    "        'humidity_max': 'Humitat relativa màxima diària + data',\n",
    "        'humidity_min': 'Humitat relativa mínima diària + data',\n",
    "        'rain_daily': 'Precipitació acumulada diària',\n",
    "        'rain_8h': 'Precipitació acumulada diària (8-8 h)',\n",
    "        'rain_max_1h': 'Precipitació màxima en 1 h (diària) + hora',\n",
    "        'rain_max_30min': 'Precipitació màxima en 30 min (diària)+ hora',\n",
    "        'rain_max_1min': 'Precipitació màxima en 1 min (diària) + hora',\n",
    "    }\n",
    "    \n",
    "    if var_name in mapping:\n",
    "        target = mapping[var_name]\n",
    "        # Find matching column (case-insensitive, partial match)\n",
    "        for col in weather_cols:\n",
    "            if target.lower() in col.lower() or col.lower() in target.lower():\n",
    "                return col\n",
    "    return None\n",
    "\n",
    "# Helper function to normalize weather variables to 0-1 scale\n",
    "def normalize_weather_variable(series, var_type, method='percentile'):\n",
    "    \"\"\"\n",
    "    Normalize weather variable to 0-1 scale based on type.\n",
    "    \n",
    "    Parameters:\n",
    "    - series: pandas Series with weather values\n",
    "    - var_type: string indicating variable type (e.g., 'temp_max', 'rain_daily')\n",
    "    - method: 'percentile' (for temperature) or 'max' (for precipitation)\n",
    "    \"\"\"\n",
    "    if series.isna().all() or len(series.dropna()) == 0:\n",
    "        return pd.Series(0, index=series.index)\n",
    "    \n",
    "    # For temperature variables: use percentile-based normalization (more robust to outliers)\n",
    "    if 'temp' in var_type:\n",
    "        q95 = series.quantile(0.95)\n",
    "        q05 = series.quantile(0.05)\n",
    "        if q95 > q05:\n",
    "            normalized = (series - q05) / (q95 - q05)\n",
    "            return np.clip(normalized, 0, 1)\n",
    "        else:\n",
    "            # Fallback to min-max if percentiles are equal\n",
    "            min_val = series.min()\n",
    "            max_val = series.max()\n",
    "            if max_val > min_val:\n",
    "                normalized = (series - min_val) / (max_val - min_val)\n",
    "                return np.clip(normalized, 0, 1)\n",
    "    \n",
    "    # For precipitation variables: normalize based on positive values\n",
    "    elif 'rain' in var_type:\n",
    "        max_val = series.max()\n",
    "        if max_val > 0:\n",
    "            normalized = series / max_val\n",
    "            return np.clip(normalized, 0, 1)\n",
    "    \n",
    "    # For solar/evapotranspiration: standard min-max\n",
    "    elif var_type in ['solar_global', 'evapotranspiration']:\n",
    "        min_val = series.min()\n",
    "        max_val = series.max()\n",
    "        if max_val > min_val:\n",
    "            normalized = (series - min_val) / (max_val - min_val)\n",
    "            return np.clip(normalized, 0, 1)\n",
    "    \n",
    "    # For humidity: standard min-max\n",
    "    elif 'humidity' in var_type:\n",
    "        min_val = series.min()\n",
    "        max_val = series.max()\n",
    "        if max_val > min_val:\n",
    "            normalized = (series - min_val) / (max_val - min_val)\n",
    "            return np.clip(normalized, 0, 1)\n",
    "    \n",
    "    # Default: standard min-max normalization\n",
    "    min_val = series.min()\n",
    "    max_val = series.max()\n",
    "    if max_val > min_val:\n",
    "        normalized = (series - min_val) / (max_val - min_val)\n",
    "        return np.clip(normalized, 0, 1)\n",
    "    \n",
    "    return pd.Series(0, index=series.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ce8d71",
   "metadata": {},
   "source": [
    "### 9.1 Calculate Weather Vulnerability Using Recommended Variables\n",
    "\n",
    "Calculate weather vulnerability components using the data-driven recommendations from `correlationAnalysis.ipynb`. This replaces the previous hardcoded approach with evidence-based variable selection and weighting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3563e509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared weather_cols: 22 weather variable columns found\n",
      "================================================================================\n",
      "HEATWAVE WEATHER VULNERABILITY (Using Recommended Variables)\n",
      "================================================================================\n",
      "Recommended variables: ['temp_avg', 'temp_min', 'temp_avg_classic', 'temp_max', 'evapotranspiration', 'solar_global']\n",
      "Number of variables: 6\n",
      "\n",
      "  ✓ temp_avg                  (weight: 0.174) -> Temperatura mitjana diària\n",
      "  ✓ temp_min                  (weight: 0.173) -> Temperatura mínima diària + hora\n",
      "  ✓ temp_avg_classic          (weight: 0.172) -> Temperatura mitjana diària\n",
      "  ✓ temp_max                  (weight: 0.166) -> Temperatura màxima diària + hora\n",
      "  ✓ evapotranspiration        (weight: 0.161) -> Evapotranspiració de referència\n",
      "  ✓ solar_global              (weight: 0.155) -> Irradiació solar global diària\n",
      "\n",
      "  Used 6/6 variables (total weight: 1.001)\n",
      "  Heatwave weather vulnerability: min=0.001, max=0.981\n",
      "\n",
      "================================================================================\n",
      "RAINFALL WEATHER VULNERABILITY (Using Recommended Variables)\n",
      "================================================================================\n",
      "Recommended variables: ['rain_max_30min', 'rain_max_1h', 'rain_max_1min', 'rain_daily', 'rain_8h']\n",
      "Number of variables: 5\n",
      "\n",
      "  ✓ rain_max_30min            (weight: 0.246) -> Precipitació màxima en 30 min (diària)+ hora\n",
      "  ✓ rain_max_1h               (weight: 0.216) -> Precipitació màxima en 1 h (diària) + hora\n",
      "  ✓ rain_max_1min             (weight: 0.203) -> Precipitació màxima en 1 min (diària) + hora\n",
      "  ✓ rain_daily                (weight: 0.179) -> Precipitació acumulada diària\n",
      "  ✓ rain_8h                   (weight: 0.155) -> Precipitació acumulada diària\n",
      "\n",
      "  Used 5/5 variables (total weight: 0.999)\n",
      "  Rainfall weather vulnerability: min=0.000, max=0.787\n",
      "\n",
      "================================================================================\n",
      "✓ Weather vulnerability components calculated\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Calculate weather vulnerability using recommended variables and weights from correlationAnalysis.ipynb\n",
    "\n",
    "# Prepare weather columns list for vulnerability calculation\n",
    "# Get all weather variable columns (exclude non-weather columns)\n",
    "non_weather_cols = ['SECCIO_CENSAL', 'DATA_LECTURA', 'CODI_ESTACIO', 'geometry', 'centroid', \n",
    "                    'centroid_lat', 'centroid_lon', 'dist_m', 'ist', 'CONSUMO_TOTAL', 'NUM_FUITES',\n",
    "                    'codi_districte', 'nom_districte', 'codi_barri', 'nom_barri', 'codi_aeb', \n",
    "                    'codi_seccio_censal', 'NUM_CONTRACTS', 'CONSUMO_POR_CONTRATO', 'CONSUMO_PROMEDIO', \n",
    "                    'CONSUMO_MEDIANA', 'NUM_CONTRATOS_CON_FUITES', 'FUITES_MAX_POR_CONTRATO', \n",
    "                    'FUITES_PROMEDIO_POR_CONTRATO', 'DATA_LECTURA_dt', 'month', 'consum_baseline',\n",
    "                    'leak_freq_mean', 'leak_contracts_mean', 'leak_max_mean', 'leak_avg_mean',\n",
    "                    'consum_surge', 'contracts_surge', 'consum_intensity_surge', 'is_rainfall_event']\n",
    "\n",
    "# Get all columns that are weather variables (from the pivot operation)\n",
    "# These are columns that aren't in the non-weather list\n",
    "weather_cols = [col for col in gdf_daily.columns if col not in non_weather_cols]\n",
    "\n",
    "# Filter to only columns that contain numeric weather data (exclude text/object columns)\n",
    "weather_cols = [col for col in weather_cols if gdf_daily[col].dtype in ['float64', 'int64', 'float32', 'int32']]\n",
    "\n",
    "print(f\"Prepared weather_cols: {len(weather_cols)} weather variable columns found\")\n",
    "\n",
    "# HEATWAVE WEATHER VULNERABILITY\n",
    "if vulnerability_score_config and vulnerability_score_config.get('heatwave'):\n",
    "    heatwave_config = vulnerability_score_config['heatwave']\n",
    "    heatwave_weights = heatwave_config['weights']\n",
    "    heatwave_vars = heatwave_config['variables']\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"HEATWAVE WEATHER VULNERABILITY (Using Recommended Variables)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Recommended variables: {heatwave_vars}\")\n",
    "    print(f\"Number of variables: {len(heatwave_vars)}\\n\")\n",
    "    \n",
    "    gdf_daily['vuln_weather_heatwave'] = 0\n",
    "    total_weight = 0\n",
    "    vars_used = []\n",
    "    \n",
    "    for var, weight in sorted(heatwave_weights.items(), key=lambda x: x[1], reverse=True):\n",
    "        var_col = map_variable_to_column(var, weather_cols)\n",
    "        if var_col and var_col in gdf_daily.columns:\n",
    "            # Normalize the variable to 0-1 scale\n",
    "            var_normalized = normalize_weather_variable(gdf_daily[var_col], var)\n",
    "            gdf_daily['vuln_weather_heatwave'] += weight * var_normalized\n",
    "            total_weight += weight\n",
    "            vars_used.append(var)\n",
    "            print(f\"  ✓ {var:25s} (weight: {weight:.3f}) -> {var_col}\")\n",
    "        else:\n",
    "            print(f\"  ✗ {var:25s} -> Column not found in dataset\")\n",
    "    \n",
    "    # Normalize to ensure weights sum to 1\n",
    "    if total_weight > 0:\n",
    "        gdf_daily['vuln_weather_heatwave'] = gdf_daily['vuln_weather_heatwave'] / total_weight\n",
    "        print(f\"\\n  Used {len(vars_used)}/{len(heatwave_vars)} variables (total weight: {total_weight:.3f})\")\n",
    "        print(f\"  Heatwave weather vulnerability: min={gdf_daily['vuln_weather_heatwave'].min():.3f}, max={gdf_daily['vuln_weather_heatwave'].max():.3f}\")\n",
    "    else:\n",
    "        print(\"\\n  ⚠ No variables found, using fallback\")\n",
    "        gdf_daily['vuln_weather_heatwave'] = 0\n",
    "else:\n",
    "    # Fallback to original approach if config not available\n",
    "    print(\"⚠ Using fallback: Single temperature variable approach\")\n",
    "    temp_cols = [col for col in weather_cols if 'temperatura' in col.lower() or 'tèrmica' in col.lower()]\n",
    "    if temp_cols:\n",
    "        if any('màxima' in col.lower() for col in temp_cols):\n",
    "            temp_col = [col for col in temp_cols if 'màxima' in col.lower()][0]\n",
    "        else:\n",
    "            temp_col = temp_cols[0]\n",
    "        temp_seasonal = gdf_daily.groupby(['SECCIO_CENSAL', 'month'])[temp_col].transform('mean')\n",
    "        temp_anomaly = gdf_daily[temp_col] - temp_seasonal\n",
    "        temp_anomaly_max = temp_anomaly.abs().max()\n",
    "        if temp_anomaly_max > 0:\n",
    "            gdf_daily['vuln_weather_heatwave'] = (temp_anomaly / temp_anomaly_max).abs()\n",
    "            gdf_daily['vuln_weather_heatwave'] = (gdf_daily['vuln_weather_heatwave'] - gdf_daily['vuln_weather_heatwave'].min()) / (gdf_daily['vuln_weather_heatwave'].max() - gdf_daily['vuln_weather_heatwave'].min() + 1e-10)\n",
    "        else:\n",
    "            gdf_daily['vuln_weather_heatwave'] = 0\n",
    "    else:\n",
    "        gdf_daily['vuln_weather_heatwave'] = 0\n",
    "\n",
    "# RAINFALL WEATHER VULNERABILITY\n",
    "if vulnerability_score_config and vulnerability_score_config.get('rainfall'):\n",
    "    rainfall_config = vulnerability_score_config['rainfall']\n",
    "    rainfall_weights = rainfall_config['weights']\n",
    "    rainfall_vars = rainfall_config['variables']\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RAINFALL WEATHER VULNERABILITY (Using Recommended Variables)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Recommended variables: {rainfall_vars}\")\n",
    "    print(f\"Number of variables: {len(rainfall_vars)}\\n\")\n",
    "    \n",
    "    gdf_daily['vuln_weather_rainfall'] = 0\n",
    "    total_weight = 0\n",
    "    vars_used = []\n",
    "    \n",
    "    for var, weight in sorted(rainfall_weights.items(), key=lambda x: x[1], reverse=True):\n",
    "        var_col = map_variable_to_column(var, weather_cols)\n",
    "        if var_col and var_col in gdf_daily.columns:\n",
    "            # Normalize the variable to 0-1 scale\n",
    "            var_normalized = normalize_weather_variable(gdf_daily[var_col], var)\n",
    "            gdf_daily['vuln_weather_rainfall'] += weight * var_normalized\n",
    "            total_weight += weight\n",
    "            vars_used.append(var)\n",
    "            print(f\"  ✓ {var:25s} (weight: {weight:.3f}) -> {var_col}\")\n",
    "        else:\n",
    "            print(f\"  ✗ {var:25s} -> Column not found in dataset\")\n",
    "    \n",
    "    # Normalize to ensure weights sum to 1\n",
    "    if total_weight > 0:\n",
    "        gdf_daily['vuln_weather_rainfall'] = gdf_daily['vuln_weather_rainfall'] / total_weight\n",
    "        print(f\"\\n  Used {len(vars_used)}/{len(rainfall_vars)} variables (total weight: {total_weight:.3f})\")\n",
    "        print(f\"  Rainfall weather vulnerability: min={gdf_daily['vuln_weather_rainfall'].min():.3f}, max={gdf_daily['vuln_weather_rainfall'].max():.3f}\")\n",
    "    else:\n",
    "        print(\"\\n  ⚠ No variables found, using fallback\")\n",
    "        gdf_daily['vuln_weather_rainfall'] = 0\n",
    "else:\n",
    "    # Fallback to original approach if config not available\n",
    "    print(\"\\n⚠ Using fallback: Single precipitation variable approach\")\n",
    "    precip_cols = [col for col in weather_cols if 'precipitació' in col.lower() or 'pluja' in col.lower()]\n",
    "    if precip_cols:\n",
    "        precip_col = precip_cols[0]\n",
    "        precip_seasonal = gdf_daily.groupby(['SECCIO_CENSAL', 'month'])[precip_col].transform('mean')\n",
    "        precip_anomaly = gdf_daily[precip_col] - precip_seasonal\n",
    "        precip_anomaly_max = precip_anomaly.max()\n",
    "        if precip_anomaly_max > 0:\n",
    "            gdf_daily['vuln_weather_rainfall'] = np.maximum(0, precip_anomaly) / precip_anomaly_max\n",
    "            gdf_daily['vuln_weather_rainfall'] = (gdf_daily['vuln_weather_rainfall'] - gdf_daily['vuln_weather_rainfall'].min()) / (gdf_daily['vuln_weather_rainfall'].max() - gdf_daily['vuln_weather_rainfall'].min() + 1e-10)\n",
    "        else:\n",
    "            gdf_daily['vuln_weather_rainfall'] = 0\n",
    "    else:\n",
    "        gdf_daily['vuln_weather_rainfall'] = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ Weather vulnerability components calculated\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "191b5943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "INFRASTRUCTURE & SOCIOECONOMIC VULNERABILITY\n",
      "================================================================================\n",
      "Using comprehensive aggregation metrics from sections 7.2 and 7.3\n",
      "================================================================================\n",
      "\n",
      "Infrastructure Vulnerability (using comprehensive metrics):\n",
      "  Baseline leak frequency (composite): min=0.000, max=1.000\n",
      "  Heatwave (consumption surge - composite): min=0.045, max=0.847\n",
      "  Heatwave infrastructure: min=0.186, max=0.714\n",
      "  Rainfall (leak response - composite): min=0.000, max=1.000\n",
      "  Rainfall infrastructure: min=0.254, max=0.897\n",
      "\n",
      "Socioeconomic Vulnerability (from IST):\n",
      "  IST range: 51.7 to 138.0\n",
      "  Vulnerability: min=0.000, max=1.000\n",
      "  (Lower IST → Higher vulnerability)\n",
      "\n",
      "================================================================================\n",
      "✓ Infrastructure and socioeconomic vulnerability components calculated\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Calculate infrastructure and socioeconomic vulnerability components\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"INFRASTRUCTURE & SOCIOECONOMIC VULNERABILITY\")\n",
    "print(\"=\"*80)\n",
    "print(\"Using comprehensive aggregation metrics from sections 7.2 and 7.3\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. INFRASTRUCTURE VULNERABILITY\n",
    "# Create separate infrastructure vulnerability components for heatwave and rainfall\n",
    "# to better capture how infrastructure responds to different weather conditions\n",
    "# Using all aggregation metrics: consumption (density, intensity) and leaks (density, severity, intensity)\n",
    "\n",
    "# === COMMON BASELINE METRICS (Using Multiple Leak Metrics) ===\n",
    "# Historical leak baseline vulnerability - composite metric using all leak aggregation metrics\n",
    "# Fill missing leak metrics with 0 for baseline calculation\n",
    "leak_cols = ['NUM_FUITES', 'NUM_CONTRATOS_CON_FUITES', 'FUITES_MAX_POR_CONTRATO', 'FUITES_PROMEDIO_POR_CONTRATO']\n",
    "for col in leak_cols:\n",
    "    if col not in gdf_daily.columns:\n",
    "        gdf_daily[col] = 0.0\n",
    "    gdf_daily[col] = gdf_daily[col].fillna(0.0)\n",
    "\n",
    "# Calculate baseline leak metrics (historical averages per census section)\n",
    "leak_baselines = gdf_daily.groupby('SECCIO_CENSAL', as_index=False).agg({\n",
    "    'NUM_FUITES': 'mean',                    # Average total leaks\n",
    "    'NUM_CONTRATOS_CON_FUITES': 'mean',      # Average contracts with leaks\n",
    "    'FUITES_MAX_POR_CONTRATO': 'mean',       # Average max severity\n",
    "    'FUITES_PROMEDIO_POR_CONTRATO': 'mean'   # Average intensity\n",
    "})\n",
    "leak_baselines.columns = ['SECCIO_CENSAL', 'leak_freq_mean', 'leak_contracts_mean', 'leak_max_mean', 'leak_avg_mean']\n",
    "gdf_daily = gdf_daily.merge(leak_baselines, on='SECCIO_CENSAL', how='left')\n",
    "\n",
    "# Normalize each baseline leak metric to 0-1 scale\n",
    "for metric, col_name in [('leak_freq_mean', 'vuln_infra_leak_freq'),\n",
    "                          ('leak_contracts_mean', 'vuln_infra_leak_contracts'),\n",
    "                          ('leak_max_mean', 'vuln_infra_leak_max'),\n",
    "                          ('leak_avg_mean', 'vuln_infra_leak_avg')]:\n",
    "    if gdf_daily[metric].max() > gdf_daily[metric].min():\n",
    "        gdf_daily[col_name] = (\n",
    "            (gdf_daily[metric] - gdf_daily[metric].min()) / \n",
    "            (gdf_daily[metric].max() - gdf_daily[metric].min())\n",
    "        )\n",
    "    else:\n",
    "        gdf_daily[col_name] = 0.0\n",
    "\n",
    "# Combined baseline leak vulnerability (weighted combination of all metrics)\n",
    "# 40% frequency, 30% density, 20% severity, 10% intensity\n",
    "gdf_daily['vuln_infra_leaks_baseline'] = (\n",
    "    0.40 * gdf_daily['vuln_infra_leak_freq'] +\n",
    "    0.30 * gdf_daily['vuln_infra_leak_contracts'] +\n",
    "    0.20 * gdf_daily['vuln_infra_leak_max'] +\n",
    "    0.10 * gdf_daily['vuln_infra_leak_avg']\n",
    ")\n",
    "\n",
    "# Historical consumption baseline (for calculating consumption surge during heatwaves)\n",
    "# Keep NaN values for missing consumption data - sections without consumption data will have NaN vulnerability scores\n",
    "consum_cols = ['CONSUMO_TOTAL', 'NUM_CONTRACTS', 'CONSUMO_POR_CONTRATO']\n",
    "for col in consum_cols:\n",
    "    if col not in gdf_daily.columns:\n",
    "        gdf_daily[col] = np.nan\n",
    "    # Do NOT fill NaN - missing consumption data is meaningful and should propagate to vulnerability scores\n",
    "\n",
    "gdf_daily['DATA_LECTURA_dt'] = pd.to_datetime(gdf_daily['DATA_LECTURA'])\n",
    "gdf_daily['month'] = gdf_daily['DATA_LECTURA_dt'].dt.month\n",
    "\n",
    "# Calculate monthly baseline for all consumption metrics\n",
    "gdf_daily['consum_baseline'] = gdf_daily.groupby(['SECCIO_CENSAL', 'month'])['CONSUMO_TOTAL'].transform('mean')\n",
    "gdf_daily['contracts_baseline'] = gdf_daily.groupby(['SECCIO_CENSAL', 'month'])['NUM_CONTRACTS'].transform('mean')\n",
    "gdf_daily['consum_per_contract_baseline'] = gdf_daily.groupby(['SECCIO_CENSAL', 'month'])['CONSUMO_POR_CONTRATO'].transform('mean')\n",
    "\n",
    "# === HEATWAVE INFRASTRUCTURE VULNERABILITY ===\n",
    "# For heatwaves: infrastructure stress comes from consumption surge + baseline leak frequency\n",
    "# Using multiple consumption metrics: total surge, contract density, and consumption intensity\n",
    "if 'CONSUMO_TOTAL' in gdf_daily.columns:\n",
    "    # 1. Total consumption surge (demand level)\n",
    "    gdf_daily['consum_surge'] = gdf_daily['CONSUMO_TOTAL'] - gdf_daily['consum_baseline']\n",
    "    # Keep NaN - do not fill with 0 (missing consumption data should propagate)\n",
    "    \n",
    "    # 2. Contract density surge (more contracts = more infrastructure to maintain)\n",
    "    gdf_daily['contracts_surge'] = gdf_daily['NUM_CONTRACTS'] - gdf_daily['contracts_baseline'].replace(0, np.nan)\n",
    "    # Keep NaN - do not fill with 0 (missing consumption data should propagate)\n",
    "    \n",
    "    # 3. Consumption intensity surge (higher per-contract usage = more stress)\n",
    "    gdf_daily['consum_intensity_surge'] = gdf_daily['CONSUMO_POR_CONTRATO'] - gdf_daily['consum_per_contract_baseline']\n",
    "    # Keep NaN - do not fill with 0 (missing consumption data should propagate)\n",
    "    \n",
    "    # Normalize each consumption surge metric to 0-1 scale\n",
    "    # Note: NaN values will remain NaN (sections without consumption data)\n",
    "    for metric, col_name in [('consum_surge', 'vuln_infra_consum_surge'),\n",
    "                              ('contracts_surge', 'vuln_infra_contracts_surge'),\n",
    "                              ('consum_intensity_surge', 'vuln_infra_consum_intensity_surge')]:\n",
    "        # Only normalize non-NaN values\n",
    "        metric_valid = gdf_daily[metric].dropna()\n",
    "        if len(metric_valid) > 0:\n",
    "            metric_max = metric_valid.max()\n",
    "            metric_min = metric_valid.min()\n",
    "            if metric_max > metric_min:\n",
    "                # Normalize to 0-1, where higher surge = higher vulnerability\n",
    "                # NaN values remain NaN\n",
    "                gdf_daily[col_name] = (gdf_daily[metric] - metric_min) / (metric_max - metric_min)\n",
    "                gdf_daily[col_name] = np.clip(gdf_daily[col_name], 0, 1)\n",
    "            else:\n",
    "                # All values are the same (or all NaN) - set to 0 for non-NaN, keep NaN for NaN\n",
    "                gdf_daily[col_name] = gdf_daily[metric].where(pd.isna(gdf_daily[metric]), 0.0)\n",
    "        else:\n",
    "            # All values are NaN - keep as NaN\n",
    "            gdf_daily[col_name] = gdf_daily[metric]\n",
    "    \n",
    "    # Combined consumption surge vulnerability (weighted combination)\n",
    "    # 50% total surge, 30% density, 20% intensity\n",
    "    # If any component is NaN, the result will be NaN (sections without consumption data)\n",
    "    gdf_daily['vuln_infra_consumption_surge'] = (\n",
    "        0.50 * gdf_daily['vuln_infra_consum_surge'] +\n",
    "        0.30 * gdf_daily['vuln_infra_contracts_surge'] +\n",
    "        0.20 * gdf_daily['vuln_infra_consum_intensity_surge']\n",
    "    )\n",
    "    \n",
    "    # Heatwave infrastructure: 60% consumption surge + 40% baseline leak frequency\n",
    "    # If consumption surge is NaN, the result will be NaN (sections without consumption data)\n",
    "    gdf_daily['vuln_infrastructure_heatwave'] = (\n",
    "        0.6 * gdf_daily['vuln_infra_consumption_surge'] + \n",
    "        0.4 * gdf_daily['vuln_infra_leaks_baseline']\n",
    "    )\n",
    "else:\n",
    "    gdf_daily['vuln_infra_consumption_surge'] = 0.0\n",
    "    gdf_daily['vuln_infrastructure_heatwave'] = gdf_daily['vuln_infra_leaks_baseline']\n",
    "\n",
    "# === RAINFALL INFRASTRUCTURE VULNERABILITY ===\n",
    "# For rainfall: infrastructure stress comes from leak response to rain + baseline leak frequency\n",
    "# Using multiple leak metrics: total response, contract density response, severity response, intensity response\n",
    "\n",
    "# Get rainfall indicator (using daily precipitation if available)\n",
    "rainfall_cols = [col for col in gdf_daily.columns if 'precipitació' in col.lower() or 'pluja' in col.lower() or 'rain' in col.lower()]\n",
    "if rainfall_cols:\n",
    "    rain_col = rainfall_cols[0]\n",
    "    gdf_daily['is_rainfall_event'] = (gdf_daily[rain_col] > 5.0).astype(int)\n",
    "    \n",
    "    # Calculate leak response metrics: compare rainy days vs non-rainy days for each metric\n",
    "    leak_metrics_response = {}\n",
    "    for metric in ['NUM_FUITES', 'NUM_CONTRATOS_CON_FUITES', 'FUITES_MAX_POR_CONTRATO', 'FUITES_PROMEDIO_POR_CONTRATO']:\n",
    "        leak_on_rain = gdf_daily[gdf_daily['is_rainfall_event'] == 1].groupby('SECCIO_CENSAL')[metric].mean()\n",
    "        leak_on_no_rain = gdf_daily[gdf_daily['is_rainfall_event'] == 0].groupby('SECCIO_CENSAL')[metric].mean()\n",
    "        leak_response = (leak_on_rain - leak_on_no_rain).fillna(0)\n",
    "        leak_metrics_response[metric] = leak_response.reset_index()\n",
    "        leak_metrics_response[metric].columns = ['SECCIO_CENSAL', f'leak_response_{metric}']\n",
    "        gdf_daily = gdf_daily.merge(leak_metrics_response[metric], on='SECCIO_CENSAL', how='left')\n",
    "    \n",
    "    # Normalize each leak response metric to 0-1 scale\n",
    "    response_cols = {\n",
    "        'leak_response_NUM_FUITES': 'vuln_infra_leak_response_freq',\n",
    "        'leak_response_NUM_CONTRATOS_CON_FUITES': 'vuln_infra_leak_response_contracts',\n",
    "        'leak_response_FUITES_MAX_POR_CONTRATO': 'vuln_infra_leak_response_max',\n",
    "        'leak_response_FUITES_PROMEDIO_POR_CONTRATO': 'vuln_infra_leak_response_avg'\n",
    "    }\n",
    "    \n",
    "    for response_col, vuln_col in response_cols.items():\n",
    "        if response_col in gdf_daily.columns:\n",
    "            if gdf_daily[response_col].max() > gdf_daily[response_col].min():\n",
    "                gdf_daily[vuln_col] = (\n",
    "                    (gdf_daily[response_col] - gdf_daily[response_col].min()) / \n",
    "                    (gdf_daily[response_col].max() - gdf_daily[response_col].min())\n",
    "                )\n",
    "            else:\n",
    "                gdf_daily[vuln_col] = 0.0\n",
    "        else:\n",
    "            gdf_daily[vuln_col] = 0.0\n",
    "    \n",
    "    # Combined leak response vulnerability (weighted combination)\n",
    "    # 40% frequency response, 30% density response, 20% severity response, 10% intensity response\n",
    "    gdf_daily['vuln_infra_leak_response'] = (\n",
    "        0.40 * gdf_daily.get('vuln_infra_leak_response_freq', 0) +\n",
    "        0.30 * gdf_daily.get('vuln_infra_leak_response_contracts', 0) +\n",
    "        0.20 * gdf_daily.get('vuln_infra_leak_response_max', 0) +\n",
    "        0.10 * gdf_daily.get('vuln_infra_leak_response_avg', 0)\n",
    "    )\n",
    "    \n",
    "    # Rainfall infrastructure: 60% leak response to rainfall + 40% baseline leak frequency\n",
    "    gdf_daily['vuln_infrastructure_rainfall'] = (\n",
    "        0.6 * gdf_daily['vuln_infra_leak_response'] + \n",
    "        0.4 * gdf_daily['vuln_infra_leaks_baseline']\n",
    "    )\n",
    "else:\n",
    "    gdf_daily['vuln_infra_leak_response'] = 0.0\n",
    "    gdf_daily['vuln_infrastructure_rainfall'] = gdf_daily['vuln_infra_leaks_baseline']\n",
    "\n",
    "# Keep a general infrastructure vulnerability for backwards compatibility (average of both)\n",
    "gdf_daily['vuln_infrastructure'] = 0.5 * gdf_daily['vuln_infrastructure_heatwave'] + 0.5 * gdf_daily['vuln_infrastructure_rainfall']\n",
    "\n",
    "print(\"\\nInfrastructure Vulnerability (using comprehensive metrics):\")\n",
    "print(f\"  Baseline leak frequency (composite): min={gdf_daily['vuln_infra_leaks_baseline'].min():.3f}, max={gdf_daily['vuln_infra_leaks_baseline'].max():.3f}\")\n",
    "print(f\"  Heatwave (consumption surge - composite): min={gdf_daily['vuln_infra_consumption_surge'].min():.3f}, max={gdf_daily['vuln_infra_consumption_surge'].max():.3f}\")\n",
    "print(f\"  Heatwave infrastructure: min={gdf_daily['vuln_infrastructure_heatwave'].min():.3f}, max={gdf_daily['vuln_infrastructure_heatwave'].max():.3f}\")\n",
    "if 'vuln_infra_leak_response' in gdf_daily.columns:\n",
    "    print(f\"  Rainfall (leak response - composite): min={gdf_daily['vuln_infra_leak_response'].min():.3f}, max={gdf_daily['vuln_infra_leak_response'].max():.3f}\")\n",
    "print(f\"  Rainfall infrastructure: min={gdf_daily['vuln_infrastructure_rainfall'].min():.3f}, max={gdf_daily['vuln_infrastructure_rainfall'].max():.3f}\")\n",
    "\n",
    "# 2. SOCIOECONOMIC VULNERABILITY\n",
    "# Based on IST (Índex socioeconòmic territorial)\n",
    "# Lower IST = higher vulnerability (inverse relationship)\n",
    "# IST typically ranges from ~50 to ~140, where lower values indicate more vulnerable areas\n",
    "\n",
    "if 'ist' in gdf_daily.columns and gdf_daily['ist'].notna().any():\n",
    "    # Normalize IST to vulnerability (inverse: lower IST = higher vulnerability)\n",
    "    ist_min = gdf_daily['ist'].min()\n",
    "    ist_max = gdf_daily['ist'].max()\n",
    "    \n",
    "    if ist_max > ist_min:\n",
    "        # Inverse normalization: (max - value) / (max - min)\n",
    "        # This makes lower IST values (more vulnerable) map to higher vulnerability scores\n",
    "        gdf_daily['vuln_socioeconomic'] = (ist_max - gdf_daily['ist']) / (ist_max - ist_min)\n",
    "    else:\n",
    "        gdf_daily['vuln_socioeconomic'] = 0.0\n",
    "    \n",
    "    print(\"\\nSocioeconomic Vulnerability (from IST):\")\n",
    "    print(f\"  IST range: {ist_min:.1f} to {ist_max:.1f}\")\n",
    "    print(f\"  Vulnerability: min={gdf_daily['vuln_socioeconomic'].min():.3f}, max={gdf_daily['vuln_socioeconomic'].max():.3f}\")\n",
    "    print(f\"  (Lower IST → Higher vulnerability)\")\n",
    "else:\n",
    "    print(\"\\n⚠ IST not available, setting socioeconomic vulnerability to 0\")\n",
    "    gdf_daily['vuln_socioeconomic'] = 0.0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ Infrastructure and socioeconomic vulnerability components calculated\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3c287f",
   "metadata": {},
   "source": [
    "### 9.3 Calculate Final Vulnerability Scores\n",
    "\n",
    "Combine weather, infrastructure, and socioeconomic components to create final vulnerability scores for heatwave and rainfall.\n",
    "\n",
    "**Weighting scheme:**\n",
    "- **Weather vulnerability**: 60% (primary driver, varies daily)\n",
    "- **Infrastructure vulnerability**: 25% (static factor, reflects historical infrastructure condition)\n",
    "- **Socioeconomic vulnerability**: 15% (static factor, reflects social vulnerability)\n",
    "\n",
    "This weighting emphasizes weather conditions while accounting for infrastructure and social factors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b7f647e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FINAL VULNERABILITY SCORE CALCULATION\n",
      "================================================================================\n",
      "\n",
      "Component weights:\n",
      "  Weather:        60%\n",
      "  Infrastructure: 25%\n",
      "  Socioeconomic:  15%\n",
      "  Total:          100%\n",
      "\n",
      "================================================================================\n",
      "FINAL VULNERABILITY SCORES\n",
      "================================================================================\n",
      "\n",
      "Heatwave Vulnerability Score:\n",
      "  Min: 0.0688\n",
      "  Max: 0.8147\n",
      "  Mean: 0.4030\n",
      "  Median: 0.3904\n",
      "\n",
      "Rainfall Vulnerability Score:\n",
      "  Min: 0.0644\n",
      "  Max: 0.7294\n",
      "  Mean: 0.1302\n",
      "  Median: 0.1181\n",
      "\n",
      "Sample of final vulnerability scores (first 10 rows):\n",
      "  SECCIO_CENSAL DATA_LECTURA  VULNERABILITY_SCORE_HEATWAVE  \\\n",
      "0   08019301001   2023-01-04                      0.279945   \n",
      "1   08019301001   2023-01-05                      0.270259   \n",
      "2   08019301001   2023-01-06                      0.249357   \n",
      "3   08019301001   2023-01-07                      0.273828   \n",
      "4   08019301001   2023-01-08                      0.275974   \n",
      "5   08019301001   2023-01-09                      0.316919   \n",
      "6   08019301001   2023-01-10                      0.263755   \n",
      "7   08019301001   2023-01-11                      0.264968   \n",
      "8   08019301001   2023-01-12                      0.267180   \n",
      "9   08019301001   2023-01-13                      0.275746   \n",
      "\n",
      "   VULNERABILITY_SCORE_RAINFALL  \n",
      "0                      0.154284  \n",
      "1                      0.154284  \n",
      "2                      0.154284  \n",
      "3                      0.154284  \n",
      "4                      0.158669  \n",
      "5                      0.154284  \n",
      "6                      0.154284  \n",
      "7                      0.154284  \n",
      "8                      0.154284  \n",
      "9                      0.154284  \n",
      "\n",
      "================================================================================\n",
      "✓ Final vulnerability scores calculated successfully\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Combine all vulnerability components into final scores\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FINAL VULNERABILITY SCORE CALCULATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define weights for combining components\n",
    "WEIGHT_WEATHER = 0.60         # 60% - weather conditions (varies daily)\n",
    "WEIGHT_INFRASTRUCTURE = 0.25  # 25% - infrastructure factors (varies daily for heatwave/rainfall)\n",
    "WEIGHT_SOCIOECONOMIC = 0.15   # 15% - socioeconomic factors (static)\n",
    "\n",
    "print(f\"\\nComponent weights:\")\n",
    "print(f\"  Weather:        {WEIGHT_WEATHER*100:.0f}%\")\n",
    "print(f\"  Infrastructure: {WEIGHT_INFRASTRUCTURE*100:.0f}%\")\n",
    "print(f\"  Socioeconomic:  {WEIGHT_SOCIOECONOMIC*100:.0f}%\")\n",
    "print(f\"  Total:          {(WEIGHT_WEATHER + WEIGHT_INFRASTRUCTURE + WEIGHT_SOCIOECONOMIC)*100:.0f}%\")\n",
    "\n",
    "# Ensure all components exist and are normalized (0-1 scale)\n",
    "# Weather components should already be normalized from previous steps\n",
    "if 'vuln_weather_heatwave' not in gdf_daily.columns:\n",
    "    print(\"\\n⚠ vuln_weather_heatwave not found, setting to 0\")\n",
    "    gdf_daily['vuln_weather_heatwave'] = 0.0\n",
    "\n",
    "if 'vuln_weather_rainfall' not in gdf_daily.columns:\n",
    "    print(\"⚠ vuln_weather_rainfall not found, setting to 0\")\n",
    "    gdf_daily['vuln_weather_rainfall'] = 0.0\n",
    "\n",
    "if 'vuln_infrastructure_heatwave' not in gdf_daily.columns:\n",
    "    print(\"⚠ vuln_infrastructure_heatwave not found, setting to 0\")\n",
    "    gdf_daily['vuln_infrastructure_heatwave'] = 0.0\n",
    "\n",
    "if 'vuln_infrastructure_rainfall' not in gdf_daily.columns:\n",
    "    print(\"⚠ vuln_infrastructure_rainfall not found, setting to 0\")\n",
    "    gdf_daily['vuln_infrastructure_rainfall'] = 0.0\n",
    "\n",
    "if 'vuln_socioeconomic' not in gdf_daily.columns:\n",
    "    print(\"⚠ vuln_socioeconomic not found, setting to 0\")\n",
    "    gdf_daily['vuln_socioeconomic'] = 0.0\n",
    "\n",
    "# Fill NaN values with 0 for weather and socioeconomic (these should always have values)\n",
    "# Do NOT fill infrastructure NaN - missing consumption data should result in NaN vulnerability scores\n",
    "gdf_daily['vuln_weather_heatwave'] = gdf_daily['vuln_weather_heatwave'].fillna(0.0)\n",
    "gdf_daily['vuln_weather_rainfall'] = gdf_daily['vuln_weather_rainfall'].fillna(0.0)\n",
    "# Keep NaN for infrastructure components - sections without consumption data will have NaN infrastructure vulnerability\n",
    "# gdf_daily['vuln_infrastructure_heatwave'] = gdf_daily['vuln_infrastructure_heatwave'].fillna(0.0)  # REMOVED: preserve NaN\n",
    "# gdf_daily['vuln_infrastructure_rainfall'] = gdf_daily['vuln_infrastructure_rainfall'].fillna(0.0)  # REMOVED: preserve NaN\n",
    "gdf_daily['vuln_socioeconomic'] = gdf_daily['vuln_socioeconomic'].fillna(0.0)\n",
    "\n",
    "# Calculate final vulnerability scores\n",
    "# HEATWAVE VULNERABILITY SCORE\n",
    "gdf_daily['VULNERABILITY_SCORE_HEATWAVE'] = (\n",
    "    WEIGHT_WEATHER * gdf_daily['vuln_weather_heatwave'] +\n",
    "    WEIGHT_INFRASTRUCTURE * gdf_daily['vuln_infrastructure_heatwave'] +\n",
    "    WEIGHT_SOCIOECONOMIC * gdf_daily['vuln_socioeconomic']\n",
    ")\n",
    "\n",
    "# RAINFALL VULNERABILITY SCORE\n",
    "gdf_daily['VULNERABILITY_SCORE_RAINFALL'] = (\n",
    "    WEIGHT_WEATHER * gdf_daily['vuln_weather_rainfall'] +\n",
    "    WEIGHT_INFRASTRUCTURE * gdf_daily['vuln_infrastructure_rainfall'] +\n",
    "    WEIGHT_SOCIOECONOMIC * gdf_daily['vuln_socioeconomic']\n",
    ")\n",
    "\n",
    "# Ensure scores are in 0-1 range (should already be, but clip to be safe)\n",
    "gdf_daily['VULNERABILITY_SCORE_HEATWAVE'] = np.clip(gdf_daily['VULNERABILITY_SCORE_HEATWAVE'], 0, 1)\n",
    "gdf_daily['VULNERABILITY_SCORE_RAINFALL'] = np.clip(gdf_daily['VULNERABILITY_SCORE_RAINFALL'], 0, 1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL VULNERABILITY SCORES\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nHeatwave Vulnerability Score:\")\n",
    "print(f\"  Min: {gdf_daily['VULNERABILITY_SCORE_HEATWAVE'].min():.4f}\")\n",
    "print(f\"  Max: {gdf_daily['VULNERABILITY_SCORE_HEATWAVE'].max():.4f}\")\n",
    "print(f\"  Mean: {gdf_daily['VULNERABILITY_SCORE_HEATWAVE'].mean():.4f}\")\n",
    "print(f\"  Median: {gdf_daily['VULNERABILITY_SCORE_HEATWAVE'].median():.4f}\")\n",
    "\n",
    "print(f\"\\nRainfall Vulnerability Score:\")\n",
    "print(f\"  Min: {gdf_daily['VULNERABILITY_SCORE_RAINFALL'].min():.4f}\")\n",
    "print(f\"  Max: {gdf_daily['VULNERABILITY_SCORE_RAINFALL'].max():.4f}\")\n",
    "print(f\"  Mean: {gdf_daily['VULNERABILITY_SCORE_RAINFALL'].mean():.4f}\")\n",
    "print(f\"  Median: {gdf_daily['VULNERABILITY_SCORE_RAINFALL'].median():.4f}\")\n",
    "\n",
    "# Show sample of final scores\n",
    "print(f\"\\nSample of final vulnerability scores (first 10 rows):\")\n",
    "sample_cols = ['SECCIO_CENSAL', 'DATA_LECTURA', 'VULNERABILITY_SCORE_HEATWAVE', 'VULNERABILITY_SCORE_RAINFALL']\n",
    "print(gdf_daily[sample_cols].head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ Final vulnerability scores calculated successfully\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600a459e",
   "metadata": {},
   "source": [
    "## 10. Save Results\n",
    "\n",
    "Save the final dataset with vulnerability scores for use in mapping and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e25134a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAVING RESULTS\n",
      "================================================================================\n",
      "\n",
      "Saving dataset to: clean/vulnerability_daily.parquet\n",
      "Dataset shape: (777504, 86) (rows, columns)\n",
      "\n",
      "Key columns being saved (10/10):\n",
      "  ✓ SECCIO_CENSAL\n",
      "  ✓ DATA_LECTURA\n",
      "  ✓ geometry\n",
      "  ✓ VULNERABILITY_SCORE_HEATWAVE\n",
      "  ✓ VULNERABILITY_SCORE_RAINFALL\n",
      "  ✓ vuln_weather_heatwave\n",
      "  ✓ vuln_weather_rainfall\n",
      "  ✓ vuln_infrastructure_heatwave\n",
      "  ✓ vuln_infrastructure_rainfall\n",
      "  ✓ vuln_socioeconomic\n",
      "\n",
      "✓ Successfully saved dataset to: clean/vulnerability_daily.parquet\n",
      "  File size: 36.54 MB\n",
      "\n",
      "================================================================================\n",
      "DATASET SUMMARY\n",
      "================================================================================\n",
      "Total rows: 777,504 (census sections × days)\n",
      "Total columns: 86\n",
      "Date range: 2023-01-04 to 2024-12-31\n",
      "Census sections: 1,068\n",
      "Unique dates: 728\n",
      "\n",
      "Final Vulnerability Scores:\n",
      "  Heatwave: min=0.0688, max=0.8147, mean=0.4030\n",
      "  Rainfall: min=0.0644, max=0.7294, mean=0.1302\n",
      "\n",
      "Geometry information:\n",
      "  Geometry column: ✓ present\n",
      "  Geometry types: Polygon, MultiPolygon\n",
      "  CRS: EPSG:4326\n",
      "\n",
      "================================================================================\n",
      "✓ Save complete\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Save the dataset with vulnerability scores\n",
    "print(\"=\"*80)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ensure output directory exists\n",
    "import os\n",
    "output_dir = \"clean\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save as parquet (recommended for large datasets with geometry)\n",
    "# GeoPandas to_parquet() automatically handles geometry column\n",
    "output_file = os.path.join(output_dir, \"vulnerability_daily.parquet\")\n",
    "\n",
    "print(f\"\\nSaving dataset to: {output_file}\")\n",
    "print(f\"Dataset shape: {gdf_daily.shape} (rows, columns)\")\n",
    "\n",
    "# Identify key columns to save (all columns will be saved, but let's list important ones)\n",
    "key_columns = [\n",
    "    'SECCIO_CENSAL', 'DATA_LECTURA', 'geometry',\n",
    "    'VULNERABILITY_SCORE_HEATWAVE', 'VULNERABILITY_SCORE_RAINFALL',\n",
    "    'vuln_weather_heatwave', 'vuln_weather_rainfall',\n",
    "    'vuln_infrastructure_heatwave', 'vuln_infrastructure_rainfall',\n",
    "    'vuln_socioeconomic'\n",
    "]\n",
    "\n",
    "# Check which key columns exist\n",
    "existing_key_cols = [col for col in key_columns if col in gdf_daily.columns]\n",
    "missing_key_cols = [col for col in key_columns if col not in gdf_daily.columns]\n",
    "\n",
    "print(f\"\\nKey columns being saved ({len(existing_key_cols)}/{len(key_columns)}):\")\n",
    "for col in existing_key_cols[:10]:  # Show first 10\n",
    "    print(f\"  ✓ {col}\")\n",
    "if len(existing_key_cols) > 10:\n",
    "    print(f\"  ... and {len(existing_key_cols) - 10} more columns\")\n",
    "\n",
    "if missing_key_cols:\n",
    "    print(f\"\\n⚠ Missing key columns: {missing_key_cols}\")\n",
    "\n",
    "# Save the GeoDataFrame\n",
    "# GeoPandas to_parquet() preserves geometry automatically\n",
    "try:\n",
    "    gdf_daily.to_parquet(output_file, index=False)\n",
    "    print(f\"\\n✓ Successfully saved dataset to: {output_file}\")\n",
    "    \n",
    "    # Verify the file was created and get its size\n",
    "    file_size = os.path.getsize(output_file) / (1024 * 1024)  # Size in MB\n",
    "    print(f\"  File size: {file_size:.2f} MB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Error saving dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total rows: {len(gdf_daily):,} (census sections × days)\")\n",
    "print(f\"Total columns: {len(gdf_daily.columns)}\")\n",
    "print(f\"Date range: {gdf_daily['DATA_LECTURA'].min().date()} to {gdf_daily['DATA_LECTURA'].max().date()}\")\n",
    "print(f\"Census sections: {gdf_daily['SECCIO_CENSAL'].nunique():,}\")\n",
    "print(f\"Unique dates: {gdf_daily['DATA_LECTURA'].nunique()}\")\n",
    "\n",
    "print(f\"\\nFinal Vulnerability Scores:\")\n",
    "print(f\"  Heatwave: min={gdf_daily['VULNERABILITY_SCORE_HEATWAVE'].min():.4f}, \"\n",
    "      f\"max={gdf_daily['VULNERABILITY_SCORE_HEATWAVE'].max():.4f}, \"\n",
    "      f\"mean={gdf_daily['VULNERABILITY_SCORE_HEATWAVE'].mean():.4f}\")\n",
    "print(f\"  Rainfall: min={gdf_daily['VULNERABILITY_SCORE_RAINFALL'].min():.4f}, \"\n",
    "      f\"max={gdf_daily['VULNERABILITY_SCORE_RAINFALL'].max():.4f}, \"\n",
    "      f\"mean={gdf_daily['VULNERABILITY_SCORE_RAINFALL'].mean():.4f}\")\n",
    "\n",
    "# Check geometry is preserved\n",
    "if 'geometry' in gdf_daily.columns:\n",
    "    geom_type = gdf_daily.geometry.geom_type.unique()\n",
    "    print(f\"\\nGeometry information:\")\n",
    "    print(f\"  Geometry column: ✓ present\")\n",
    "    print(f\"  Geometry types: {', '.join(geom_type)}\")\n",
    "    print(f\"  CRS: {gdf_daily.crs}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ Save complete\")\n",
    "print(\"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adb",
   "language": "python",
   "name": "adb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
